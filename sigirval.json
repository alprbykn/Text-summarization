[{"doc": "development of image retrieval, especially of fine-grained image retrieval is more or less impeded by the problem of missing labeled data due to increasing annotation costs\nas zero-shot learning (zsl) realizes image classification of certain classes which have no labeled training samples, it has been drawing much attention in recent years [5]\nbut task of zsl is difficult because it lacks labeled training samples of some classes, called unseen classes, to directly train classifiers\njust given labeled training samples of seen classes, zsl aims to achieve unseen classes recognition by building relationship between unseen classes and seen classes.\nnowadays, there are mainly two popular methods for zsl: probability reasoning based on attribute prediction [4] and feature projection among different embeddings [6]\nthe first method usually predicts attribute probability to calculate class maximum likelihood while the second method mainly bridges different embedding spaces to exploit space projection and feature mapping for zsl\nhowever, existing methods have several flaws.\non the one hand, there is inherent error accumulation in probabilistic reasoning\non the other hand, most embedding methods apply complex deep network to realize space projection, which is widely believed to have little interpretability of projection process and takes a lot of time to train the network\ndifferent from existing two types of methods, we propose a new type of method for zsl: sample construction\nour proposed method, imagination based sample construction (ibsc), is based on human associative cognition process to directly construct samples of unseen classes.\nthus, unseen class recognition can be realized via learning from the constructed samples.\nhuman can visualize unseen objects through referring some already known objects and assembling their visual components based on imagination [7]\na human, who never see a tiger before but has seen some cats yet, can speculate the species at the first sight of a real tiger if he knows the description of tiger or attribute relationship between tiger and cat\nby mimicking human associative cognition process, we construct samples of unseen classes from samples of seen classes in feature space, based on a relationship between feature and attribute\nour proposed method is schematically displayed in figure 1\nas shown in figure 1, each attribute is related to specific dimensions of image feature.\nfor example, samples which don\u2019t have attribute \u201cpaws\u201d are different from samples with \u201cpaws\u201d in certain feature dimensions\nbased on attribute-feature relation, an image feature can be reconstructed from other samples to express different attributes\nif use feature dimension related to \u201cpaws\u201d to replace original feature dimension of samples without \u201cpaws\u201d, the reconstructed samples will have a new characteristic.\ngenerally, it is reasonable to choose seen classes with large similarity to unseen classes as reference basis when constructing target samples of unseen classes\nafter samples are constructed through splicing different samples of seen classes, the constructed samples of higher quality need to be picked out\nhence, we adopt the idea about dissimilarity representation [3] to measure representativeness of the constructed samples\nalthough our method is designed to classify images of unseen classes, no more new classifiers need constructing, as zsl has been simplified into a traditional supervised classification problem where most existing classifiers can be used\nwe experiment on four benchmark datasets.\ncompared with state-of-the-art approaches, comprehensive results demonstrate the superiority of our proposed method\nfurthermore, our work can be viewed as a baseline for future sample construction works for zs", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Zero-shot learning (ZSL) which aims to recognize unseen classes with no labeled training sample, efficiently tackles the problem of missing labeled data in image retrieval\nNowadays there are mainly two types of popular methods for ZSL to recognize images of unseen classes: probabilistic reasoning and feature projection\nDifferent from these existing types of methods, we propose a new method: sample construction to deal with the problem of ZSL\nOur proposed method, called Imagination Based Sample Construction (IBSC), innovatively constructs image samples of target classes in feature space by mimicking human associative cognition process\nBased on an association between attribute and feature, target samples are constructed from different parts of various samples\nFurthermore, dissimilarity representation is employed to select high-quality constructed samples which are used as labeled data to train a specific classifier for those unseen classes\nIn this way, zero-shot learning is turned into a supervised learning problem\nAs far as we know, it is the first work to construct samples for ZSL thus, our work is viewed as a baseline for future sample construction methods\nExperiments on four benchmark datasets show the superiority of our proposed metho"},
{"doc": "a technology-assisted review (tar) aims at locating all relevant documents in a collection (\u201ctotal recall\u201d) while minimizing manual reviewing effort\ntar has been successfully applied in a variety of high-recall tasks such as conducting systematic reviews in evidence-based medicine [8], electronic discovery in the legal proceedings [1], creating test collections for information retrieval (ir) evaluation [10]\no\u2019mara-eves [8] provides a detailed survey of machine learning methods used in tar\nactive learning techniques, which iteratively improve the accuracy of the predictions through interaction with reviewers, achieve state-of-the-art performance\nin particular, cormack and grossman [1, 2] have proposed the baseline model implementation (bmi), a continuous active learning (cal) algorithm, which has been evaluated in a number of high-recall tasks as the best performing algorithm [5, 7]\nbmi identifies an initial set of documents to be reviewed by experts to be used as an initial training set for learning a logistic regression model.\nthe logistic regression algorithm predicts the relevance of the remaining of the documents\na set of top-scored documents is returned to assessors for labeling\nthe labeled documents are added back to the initial training set and the model is being retrained\nwhile cal algorithms have demonstrated their ability to efficiently find relevant documents in a collection [1, 6], recall typically reaches a plateau of 80%-90% after reviewing and labeling 30%-40% of the collection [7]\nfinding the last few relevant documents requires reviewing almost the entire collection\nthe goal of this work is to efficiently retrieve these last few relevant documents\nour hypothesis is that asking direct questions to reviewers will allow an algorithm to discover the missing documents faster than when requesting relevance feedback on documents through continuous active learning.\nhence, we propose a sequential bayesian search [11] based method (sbstar), which locates the missing relevant documents efficiently by directly querying reviewers about significant pieces of information expected to appear, or not, in the relevant documents\nour framework applies cal up to a certain level of effort, in terms of documents reviewed\nthen it switches to sbstar to directly ask questions to reviewers\n sbstar first identifies a pool of questions to be asked.\nin this work we focus on questions about the expected presence of an entity in the missing relevant documents\nhence, entities found in the corpus constitute the pool of available questions\nsbstar then constructs a prior belief over document relevance on the basis of the ranking model trained by cal\nthen, it applies generalized binary search (gbs) over entities to find the entity that dichotomizes the probability mass of document relevance.\nafter each question is being answered by the reviewer a posterior belief is obtained to be used for the selection of the next question\nthe main contribution of this paper is two-fold\n(1) a method to construct a set of questions to be asked to the reviewers in terms of entities contained in the documents of the collection\n (2) a novel interactive method, which directly queries reviewers about the expected presence of an entity in relevant documents, and updates the prior belief on document relevance at every round of interaction\nto the best of our knowledge this is the first work that attempts to ask explicit questions to reviewers for the purpose of achieving total recall that goes beyond document relevance feedback\nthe evaluation results show that our approach can significantly reduce human effort, while achieve high recal", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The goal of a technology-assisted review is to achieve high recall with low human effort\n Continuous active learning algorithms have demonstrated good performance in locating the majority of relevant documents in a collection, however their performance is reaching a plateau when 80%-90% of them has been found\nFinding the last few relevant documents typically requires exhaustively reviewing the collection\nIn this paper,we propose a novel method to identify these last few, but significant, documents efficiently\nOur method makes the hypothesis that entities carry vital information in documents, and that reviewers can answer questions about the presence or absence of an entity in the missing relevance documents.\nBased on this we devise a sequential Bayesian search method that selects the optimal sequence of questions to ask.\nThe experimental results show that our proposed method can greatly improve performance requiring less reviewing effor"},
{"doc": "multivariate time series data are ubiquitous in our everyday life ranging from the prices in stock markets, the traffic flows on highways, the outputs of solar power plants, the temperatures across different cities, just to name a few\nin such applications, users are often interested in the forecasting of the new trends or potential hazardous events based on historical observations on time series signals\nfor instance, a better route plan could be devised based on the predicted traffic jam patterns a few hours ahead, and a larger profit could be made with the forecasting of the near-future stock market\nmultivariate time series forecasting often faces a major research challenge, that is, how to capture and leverage the dynamics dependencies among multiple variables\nspecifically, real-world applications often entail a mixture of short-term and long-term repeating patterns, as shown in figure 1 which plots the hourly occupancy rate of a freeway\napparently, there are two repeating patterns, daily and weekly\nthe former portraits the morning peaks vs. evening peaks, while the latter reflects the workday and weekend patterns\na successful time series forecasting model should be capture both kinds of recurring patterns for accurate predictions\nas another example, consider the task of predicting the output of a solar energy farm based on the measured solar radiation by massive sensors over different locations\nthe long-term patterns reflect the difference between days vs. nights, summer vs. winter, etc., and the shortterm patterns reflect the effects of cloud movements, wind direction changes, etc\nagain, without taking both kinds of recurrent patterns into account, accurate time series forecasting is not possible\nhowever, traditional approaches such as the large body of work in autoregressive methods [2, 12, 22, 33, 36] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically\naddressing such limitations of existing methods in time series forecasting is the main focus of this paper, for which we propose a novel framework that takes advantages of recent developments in deep learning research\ndeep neural networks have been intensively studied in related domains, and made extraordinary impacts on the solutions of a broad range of problems\nthe recurrent neural networks (rnn) models [9], for example, have become most popular in recent natural language processing (nlp) research\ntwo variants of rnn in particular, namely the long short term memory (lstm) [15] and the gated recurrent unit (gru) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other nlp tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19]\nin the field of computer vision, as another example, convolution neural network (cnn) models [19, 21] have shown outstanding performance by successfully extracting local and shift-invariant features (called \"shapelets\" sometimes) at various granularity levels from input images\ndeep neural networks have received an increasing amount of attention in time series analysis\na substantial portion of the previous work has been focusing on time series classification, i.e., the task of automated assignment of class labels to time series input\nfor instance, rnn architectures have been studied for extracting informative patterns from health-care sequential data [5, 23] and classifying the data with respect diagnostic categories\nrnn has been applied to mobile data, for classifying the input sequences with respect to actions or activities [13]\ncnn models have been used in action/activity recognition [13, 20, 32], for the extraction of shift-invariant local patterns from input sequences as the features of classification models\ndeep neural networks have been studied for time series forecasting [8, 27, 34, 37], i.e., the task of using observed time series in the past to predict the unknown time series in a look-ahead horizon \u2013 the larger the horizon, the harder the problem\nefforts in this direction range from the early work using naive rnn models [7] and the hybrid models [16, 35, 36] combining the use of arima [3] and multilayer perceptron (mlp), to the recent combination of vanilla rnn and dynamic boltzmann machines in time series forecasting [8]\nin this paper, we propose a deep learning framework designed for the multivariate time series forecasting, namely long and short term time-series network (lstnet), as illustrated in figure 2\nit leverages the strengths of both the convolutional layer to discover the local dependency patterns among multi-dimensional input variables and the recurrent layer to capture complex long-term dependencies\na novel recurrent structure, namely recurrent-skip, is designed for capturing very long-term dependence patterns and making the optimization easier as it utilizes the periodic property of the input time series signals\nfinally, the lstnet incorporates a traditional autoregressive linear model in parallel to the non-linear neural network part, which makes the non-linear deep learning model more robust for the time series with violate scale changing\nin the experiment on the real world seasonal time series datasets, our model consistently outperforms the traditional linear models and gru recurrent neural network\nthe rest of this paper is organized as follows\nsection 2 outlines the related background, including representative auto-regressive methods and gaussian process models\nsection 3 describe our proposed lstnet\nsection 4 reports the evaluation results of our model in comparison with strong baselines on real-world datasets\nfinally, we conclude our findings in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation\nTemporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail\nIn this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge\nLSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends\nFurthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model\nIn our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods\nAll the data and experiment codes are available onlin"},
{"doc": "recent advances in hin research [7] allow us to embed a hin into a multi-dimensional vector space [6]\nwe present a method that facilitates these embeddings to be used as features in an ltr framework for the problem of contextual suggestion\nin this problem domain, we have contextual queries and user profiles that contain preference rankings of objects of interest\nin our approach, we model users and objects in the same hin and define meta-paths specific to the problem domain, then generate graph embeddings by randomly sampling the hin conditioned on the meta-paths\nonce users and their interests are projected into the resulting embedding space, we derive features that rank the objects of interest according to a contextual query\nmeta-paths enable us to capture human intuition by introducing a set of semantic constrains on the hin\nfor example, if user u1 tags a relevant document d1 using word w, then another user u2 that has used w to tag a different document d1 might also find d1 relevant\nthis information is inherently contained in the graph but hidden\na domain expert can utilize meta-paths to express her knowledge about this latent structure\nthe generation of the statistical representation of the graph is then guided by conditioning the sampling of the graph along nodes that follow the meta-path\nin this work we show how meta-paths can be utilized in an information retrieval setting.\nby representing users and the objects of interest (i.e. documents) in the same vector space we can make use of the distances between objects as features in a ranking function\nthe ranking function is then learned from a small sample of relevance judged documents (in our case the relevance judgments from previous years of the treccs task)\nonce trained, the ranking function can be utilized to re-rank a set of retrieved documents, thereby incorporating the latent information of the hin\nto demonstrate a concrete application of the retrieval framework we have selected the trip recommendation problem of the treccs task [3]\n the treccs dataset provides user profiles composed of a set of objects ranked by relevancy to the user along with the search intent (e.g. planning a trip)\n the goal is to return a ranked list of attractions that might be interesting to the user\nas the objects of interest are represented by text documents, we investigate how document-based nodes can be broken up into fine-grained node types to improve the retrieval performance significantly\nfurther, we experimentally show that we can reduce sparsity in the graph by limiting the number of nodes prior to training, which results in significant performance improvements\nto summarize, the contributions of this paper are to\n(1) define a general ir framework for the ranking of heterogeneous objects based on hin embeddings\n(2) identify node types and graph topology specific to the treccs task.\n(3) specify meta-paths to encode domain knowledge in the embedding space\n(4) show how finegrained modeling of document-based nodes can improve performance\n(5) compare how different feature selection methods can reduce graph size and sparsity, while improving performanc", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion\nOur method represents users, documents and other context-related documents as heterogeneous objects in a HIN\nUsing meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space\nThis allows inferences of user interest on unseen objects based on distance in the embedding space\nThese object distances are then incorporated as features in a well-established learning to rank (LTR) framework.\nWe make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS tas"},
{"doc": "patent retrieval (pr) is the pillar of almost all patent analysis tasks\npr is a challenging task as patents are multi-page, multi-modal, multi-language, semi-structured, and metadata rich documents\non another hand, patent queries can be a complete multi-page patent application\nthese unique features make traditional ir methods used for web or ad hoc search inappropriate or at least of limited applicability to pr [12].\npr methods are either keyword-based [7, 9, 13] or semantic-based [3, 6, 8].\nbecause neither methods has acceptable performance, few interactive methods were proposed to better discriminate relevant vs. irrelevant terms based on user feedback [4]\nin this paper, we present a novel interactive framework for pr based on distributed representations of concepts and entities identified in patents text.\noffline, we jointly learn the embeddings of words, concepts, patent documents, and patent classes in the same semantic space\nwe then use the learned embeddings to generate multiple vector-based representations of the topic patent query and its prior art candidates\ngiven a topic patent, we find its prior art through two steps:\n1) candidate generation through keyword search, favoring recall,\nand 2) candidate reranking through an ensemble of semantic similarities computed from the vector representations, favoring precision\nempirical evaluation of this automated retrieval scheme on the clef-ip 2010 dataset shows its efficacy over keyword search where we get 4.6% improvement in recall@100\nwe also propose an effective query reformulation and term weighting mechanism based on interactive relevance feedback.\nwe model term weighting as a supervised feature selection problem where term weights are assigned based on how good each term is at discriminating the relevant vs. irrelevant candidates obtained from user feedback\n our interaction mechanism is more practical and realistic than the one proposed by golestan far et al. [4]\nwe ask the user to annotate hits in the top n results as relevant/irrelevant, while in [4] the user is restricted to annotate only relevant candidates which might appear very deep in the candidates list\nwe simulate this interactive term weighting mechanism to demonstrate its effectiveness over the best performer in the clef-ip 2010 competition; patatras [5]\n simulation results show that we can outperform patatras with only 1 annotated candidate regardless of whether it is relevant or not\nit is worth mentioning that similar results have been presented in golestan far et al. [4], but with restricting the user to annotate 1 relevant candidate which again might require the user to navigate through several candidate", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We present a novel interactive framework for patent retrieval leveraging distributed representations of concepts and entities extracted from the patents text\nWe propose a simple and practical interactive relevance feedback mechanism where the user is asked to annotate relevant/irrelevant results from the top n hits\nWe then utilize this feedback for query reformulation and term weighting where weights are assigned based on how good each term is at discriminating the relevant vs. irrelevant candidates\nFirst, we demonstrate the efficacy of the distributed representations on the CLEF-IP 2010 dataset where we achieve significant improvement of 4.6% in recall over the keyword search baseline\nSecond, we simulate interactivity to demonstrate the efficacy of our interactive term weighting mechanism\nSimulation results show that we can achieve significant improvement in recall from one interaction iteration outperforming previous semantic and interactive patent retrieval method"},
{"doc": "among various recommendation techniques, the most successful approach is collaborative filtering (cf) [4]; it recommends items to a user based on previous ratings of other users whose tastes are similar to the target user\n however, this in turn implies that the performance of cf will suffer without a sufficient amount of ratings previously given by users, which is common in reality\nto compensate for the sparsity of the user\u2013item rating data, side information related to users and items, such as user social network [8], user review documents [11\u201314], and item affinity network [9] has been actively leveraged\n in this work, we specifically focus on user review-aware recommendation\nuser reviews are particularly useful for alleviating the sparsity of user ratings, because the reviews not only embody a user\u2019s intention behind the ratings, but also contain conspicuous item properties\nthat is to say, if reviews are fully exploited, we can build recommender systems even with few ratings provided, which naturally alleviates the sparsity of user\u2013item rating data\nto extract meaningful features from review documents, deep learning-based approaches have been recently proposed [1, 13]\nmore specifically, convolutional neural network (cnn)-based recommendation methods have gained attention [3, 11, 14] thanks to the capability of cnns to capture general contextual features from documents\ndeepconn [14] adopts two cnns, where one of them models users through reviews written by the users, while the other models items through reviews written for the items.\nbuilding upon deepconn, seo et al. propose d-attn [11] that further adopts the dual local and global attention mechanism on the cnns, which endow the recommender systems with interpretability regarding the reviews that are used for modeling users and items\ndespite their state-of-the-art performance, they are limited in that users and items are modeled by the reviews consisting of raw words.\nhowever, each user has different tendency in writing a review and thus words contain an inherent ambiguity, which makes it hard to precisely understand the user\u2019s intent\nas a concrete example, let\u2019s assume that two different users provided reviews that contain the following identical sentence: \u201c... i like the laptop... \u201d\nwhereas a tolerant user would use the word \u201clike\u201d to describe an adequate laptop, a critical user would not use it unless he is completely satisfied with the laptop\n however, the previous reviewaware methods simply aggregate all the associated reviews and feed them to cnns expecting the cnns to automatically extract meaningful features for modeling users and items, which does not suffice for precisely modeling the users and items\nthis phenomenon compounds when users provided only a few reviews, i.e., cold-start [10], which is common in reality\nmoreover, as the existing approaches model each user/item by the concatenation of all the words from every associated review, the size of input for cnns becomes considerably large, which makes the above approaches practically not feasible in the real-world applications\nin this paper, to overcome the above limitations of the existing methods, we propose a novel sentiment guided review-aware recommendation method, called sentirec.\nthe core idea is to leverage the overall sentiments of reviews that are represented as ratings that accompany the reviews.\nin our previous example, if we have a prior knowledge that the tolerant user gave a 3-star rating to the laptop while the critical user gave a 5-star rating, we will be able to more accurately understand the review, which in turn enables us to better model users and items\nour proposed method consists of two steps.\nin the first step, instead of representing a review by the concatenation of its constituent raw words as in the previous methods, we encode each review into a fixed-size review vector that is guided to embody the sentiment information of the review\nmore precisely, we regard a rating that accompanies a review as a summarization of the overall sentiment of a user on an item, and train a cnn that is designed to predict the rating given the review as input, after which a fixed-size vector for the review is obtained by taking the output of the last hidden layer\nthe second step resembles the training process of deepconn and d-attn, but is distinguished in that users/items in sentirec are represented by the concatenation of their associated fixed-size review vectors, rather than raw words\nthe advantages of sentirec compared with the previous methods are\n1) we obtain more accurate representations for reviews by incorporating users\u2019 overall sentiments on items into reviews, which removes the possible ambiguity contained in the reviews.\nthis in turn results in a better understanding of the reviews, and leads to more accurate representations for users and items resulting in an improved recommendation accuracy\n moreover, 2) we drastically reduce the size of the input, which gives us scalability in terms of the training time and the memory usage\nour experiments show that sentirec outperforms the state-of-the-art baselines, while being considerably more efficient\nmoreover, we perform a qualitative evaluation on the review vectors trained by sentirec to ascertain that the overall sentiments are indeed encoded in the vector", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Existing review-aware recommendation methods represent users (or items) through the concatenation of the reviews written by (or for) them, and depend entirely on convolutional neural networks (CNNs) to extract meaningful features for modeling users (or items)\nHowever, understanding reviews based only on the raw words of reviews is challenging because of the inherent ambiguity contained in them originated from the users\u2019 different tendency in writing\n Moreover, it is inefficient in time and memory to model users/items by the concatenation of their associated reviews owing to considerably large inputs to CNNs\nIn this work, we present a scalable reviewaware recommendation method, called SentiRec, that is guided to incorporate the sentiments of reviews when modeling the users and the items\nSentiRec is a two-step approach composed of the first step that includes the encoding of each review into a fixed-size review vector that is trained to embody the sentiment of the review, followed by the second step that generates recommendations based on the vector-encoded reviews\nThrough our experiments, we show that SentiRec not only outperforms the existing reviewaware methods, but also drastically reduces the training time and the memory usage.\nWe also conduct a qualitative evaluation on the vector-encoded reviews trained by SentiRec to demonstrate that the overall sentiments are indeed encoded therei"},
{"doc": "community-acquired pneumonia (cap) [13] refers to the lungs of patients infected when they are not in hospital\nit has long been a major cause of morbidity and death, especially for children\nas reported by the studies [12, 15], pneumonia is one of the top ranked diseases responsible for the deaths of children both in usa and china\ncuring cap largely requires an early administration of appropriate antibiotics [9]\nunfortunately, the issue of the abuse of antibiotics is very prevalent, especially in developing countries such as china [7], which seriously endangers human health\nalleviating the above issue needs an accurate detection of pathogenic microorganism [13].\npathogenic microorganism is a family of microorganisms which will cause human diseases\nif the pathogenic microorganism of cap can be precisely identified, clinicians are able to prescribe optimal antibiotics\nconventional gold-standard detection methods are mainly etiology based, including culture based assays, polymerase-chain-reaction (pcr), etc\nhowever, many of them need specialized equipment and reagents, and are labor and time intensive [4, 17], which limit their application only in major hospitals.\nthus, there is an urgent need to develop intelligent and cost-effective methodologies to detect pathogenic microorganism of cap using data which is easier to be acquired\nrecent progress in wide collection of electronic health records (ehrs) [8] applies the methodologies from artificial intelligence community to cap\nhowever, existing studies in this regard are somewhat limited and mainly aim at 1) predicting whether suspected patients have pneumonia [16] or 2) further judging the risk of patients with pneumonia [3]\nmost of them have ignored to investigate the power of patient easy-to-acquire data from ehrs for automatically detecting pathogenic microorganism of cap.\nin fact, it plays a great role in treating cap children\nin this paper, we formulate a new problem of utilizing pneumonia patients multiple medical features from ehrs to identify their pathogenic microorganisms\nto our best knowledge, none of previous studies has investigated this problem\nthe studied features include time-varying body temperature and some carefully selected clinical measurements which are easy to be acquired, such as white blood cell count from routine blood test (see table 1 for details)\nconsequently, the central challenge is how to effectively fuse the above multiple types of features and construct an effective model for the problem\nto address the challenge, we develop a patient attention based recurrent neural network (pa-rnn), which is capable of modeling sequential body temperatures and fusing multiple types of patient features\nto be specific, pa-rnn first exploits the power of recurrent neural network (rnn) to obtain a sequence of body temperature representations for different time steps\nmeantime it constructs patient basic features which are carefully selected from ehrs\nafterwards, inspired by attention mechanism [1], pa-rnn provides a patient feature based attention to determine the importance of each time-varying temperature representation and further gains an integrated representation for a whole body temperature sequence\nfinally, the model fuses the integrated representation with the representation of patient basic features for pathogenic microorganism detection\nin a nutshell, the major novelty of pa-rnn is that most previous studies which utilize rnn to model ehrs [2, 5, 10, 11, 14] focus on predicting targets at the next time step based on current hidden states of rnn\nhowever, we obtain an integrated representation of body temperatures sequence through a novel patient feature based attention computation to all hidden states of rnn.\nwe conduct comprehensive experiments on a real world dataset from a major hospital in china, indicating the benefit of fusing multiple types of features from ehrs for the studied problem, and demonstrating the effectiveness of pa-rnn over several alternative method", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Community-acquired pneumonia (CAP) is a major death cause for children, requiring an early administration of appropriate antibiotics to cure it\nTo achieve this, accurate detection of pathogenic microorganism is crucial, especially for reducing the abuse of antibiotics\nConventional gold standard detection methods are mainly etiology based, incurring high cost and labor intensity\nAlthough recently electronic health records (EHRs) become prevalent and widely used, their power for automatically determining pathogenic microorganism has not been investigated\nIn this paper, we formulate a new problem for automatically detecting pathogenic microorganism of CAP by considering patient biomedical features from EHRs, including time-varying body temperatures and common laboratory measurements\nWe further develop a Patient Attention based Recurrent Neural Network (PA-RNN) model to fuse different patient features for detection\nWe conduct experiments on a real dataset, demonstrating utilizing electronic health records yields promising performance and PA-RNN outperforms several alternative"},
{"doc": "in recent years, recommender systems have played a significant part in the multimedia field\nhowever, within most of these web services, the number of users and the number of images/videos are dramatically growing, making the multimedia recommendation more challenging than ever before\nthe dominating web multimedia content requires modern recommender systems, in particular, those based on collaborative filtering (cf), to sift through massive multimedia content for users in a highly dynamic environment\ncollaborative filtering methods group people with similar interests and make recommendations on this basis.\nin the context of multimedia recommendation, item indicates different kinds of multimedia content\nmost cf methods rely on items star ratings, which provide explicit feedback [5, 9]\nhowever, when used in the multimedia field, traditional cf methods have two shortcomings.\nfirst, cf methods failed to focus on the multimedia content itself, which is the most important factor when users choose images or videos\nas content information of items is available, content-aware methods have been introduced\nsuch incorporation of content information usually leads to better recommendation performance [1, 10, 12].\nsecond, explicit ratings are not always available in many applications\nmore often, interaction data such as \"like\" of photos, or \"view\" of movies are more convenient to collect\nsuch data are based on implicit feedback\nto combine the multimedia content with the cf method and make the best use of implicit feedback, we propose a novel contentaware multimedia recommendation framework with graph autoencoder (graphcar)\nwe use two graph convolutional networks as the encoder to model latent factor of users and items, respectively\nafter that, we generate preference scores by using the inner product of two latent factor vectors\nwe evaluate graphcar extensively on two real-world datasets that represent a spectrum of different media: the amazon movies and tvs dataset and the vine dataset, with the former providing images features and the latter providing videos features\nthrough these experiments, we observe that graphcar is superior to competing methods of the best configuration, ranging from cf-based methods to content-based methods\nin summary, the main contributions of our work are\nwe propose a novel content-aware multimedia recommendation model with graph autoencoder (graphcar) to employ graph autoencoder in cf with implicit feedback\nto combine user-item interaction with user attributes and the multimedia content, we introduce two graph convolutional networks, both of which are neural networks that can be seamlessly incorporated into any neighborhood models with efficient end-to-end sgd training\nexperiments on two real datasets show that graphcar significantly outperforms state-of-the-art techniques of both cf and content-based method", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Precisely recommending relevant multimedia items from massive candidates to a large number of users is an indispensable yet difficult task on many platforms\nA promising way is to project users and items into a latent space and recommend items via the inner product of latent factor vectors\nHowever, previous studies paid little attention to the multimedia content itself and couldn\u2019t make the best use of preference data like implicit feedback.\nTo fill this gap, we propose a Content-aware Multimedia Recommendation Model with Graph Autoencoder (GraphCAR), combining informative multimedia content with user-item interaction.\nSpecifically, user-item interaction, user attributes and multimedia contents (e.g., images, videos, audios, etc.) are taken as input of the autoencoder to generate the item preference scores for each user\nThrough extensive experiments on two real-world multimedia Web services: Amazon and Vine, we show that GraphCAR significantly outperforms state-of-the-art techniques of both collaborative filtering and content-based method"},
{"doc": " recently, several deep learning models for information retrieval have been proposed [2, 4, 6, 8]\nthese models have demonstrated their potential to improve the effectiveness in ad-hoc search\nin general, a deep neural model is constructed to represent the content of the document and the query [4, 8], and/or their interactions or matching scores [2, 6].\nthe utilization of deep neural models is motivated by their ability to make high level and more abstract matching between the document and the query, thereby alleviating the vocabulary mismatch problem\nwe observe, however, that these models only use one level of final representation or matching score for any document-query pair\nfor example, in [8], several layers of convolutions are used to create more and more abstract representations for the document and the query, and the matching score only relies on the last layer of representation\nconvolution is an operation that aggregates lower-level features to produce more abstract features\na matching score at the highest level tends to reflect a conceptual matching\nin reality, user\u2019s queries may be of different nature\nsome queries such as \u201cron howard\u201d (a query in clueweb) asking for information about a celebrity would require a low level lexical matching rather than conceptual matching.\n we call them lexical queries\n on the other hand, a query like \u201clymphoma in dogs\u201d is intended to find document about corresponding concept(s), therefore a high level conceptual matching is preferred\nthese queries are called conceptual queries\nthese examples clearly show the need for matching document and query at different levels of abstraction.\ninspired by this intuition, in this paper we propose a multi-level abstraction convolutional model (macm), which integrates document-query matching at different levels of abstraction\nthis model is expected to have a better capability of coping with different types of user queries\nalthough neural ir models can focus either on document and query representation or on interactions between them, guo et al. [2] showed that the latter is more effective than the former\nbased on this observation, our model is built on document-query interactions rather than representations.\na critical problem in building deep neural models for ir is the requirement of a large amount of labeled training data, which is often unavailable\nthe idea of weak supervision by a traditional ir model is proposed recently [1] and shown to be effective\ninspired by this work, we employ the bm25 retrieval model [7] for weak supervision - the ranked documents retrieved by bm25 are used to train our deep neural model\nwe will see that this strategy is able to train our deep neural model, leading to superior effectiveness to bm25\nthe main contribution of our paper lies in a new neural model capable of coping with different types of queries by matching them with documents at different levels of abstraction\nthis idea can be easily adopted in other deep neural models, whether they are based on representations or interactions, use cnn or rnn\nour experiments on clueweb confirm that our approach can result in superior retrieval effectivenes", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Recent neural models for IR have produced good retrieval effectiveness compared with traditional models\nYet all of them assume that a single matching function should be used for all queries.\n In practice, user\u2019s queries may be of various nature which might require different levels of matching, from low level word matching to high level conceptual matching\nTo cope with this problem, we propose a multi-level abstraction convolutional model (MACM) that generates and aggregates several levels of matching scores\nWeak supervision is used to address the problem of large training data\nExperimental results demonstrated the effectiveness of our proposed MACM mode"},
{"doc": "most of information retrieval (ir) contributions follow a standard structure: analysis of the state of the art, description of the approach, and empirical evaluation over a certain data set\nthe large amount of available annotated collections allows to improve models by trial and error\nhowever, this methodology does not match with the standard scientific procedures: hypothesis statement, definition of an experiment guided by the specific hypothesis, and result analysis\nas a consequence, the ir community tends to produce solutions to a greater extent than knowledge\nthe slow progress in creating new knowledge in ir is at least partly because it is not easy to import scientific methodologies from other areas such as physics or human sciences into ir; unlike in other engineering areas, the unpredictability of human behavior makes it difficult to find general laws that describe precise phenomena\n(note that in this paper we focus on effectiveness rather than efficiency or scalability in which the user is not involved.)\nregarding social and psychological researches, the need for effectiveness in systems makes futile the production of general principles\nthis situation makes us wonder whether the current practices in ir research are on the \u201cright\" track toward discovery of new knowledge about ir.\nthe more general question here is:\nwhat are the best methodologies (if any) that researchers should follow to optimally advance the knowledge in ir research\n to address this question, in this paper, we first try to quantify the current methodological trends in ir research and categorize existing ir methodologies along two dimensions: (1) empirical vs. theoretical, and (2) top-down vs. bottom-up\n we then identify six desirable properties and anaylze these four types of methodologies accordingly.\nthe analysis indicates that none of the methodologies can satisfy all the desirable properties but they are complementary to each other\nfor example, theoretical methodologies give theoretical foundations, interpretability, and robustness across new scenarios, while empirical methodologies provide evidence about the relative effectiveness of approaches in particular realistic scenarios, as well as providing statistical significance when comparing systems to each other\nfurthermore, we categorized the 167 full papers published in the sigir 2016 and 2017 as well as ictir 2017 conferences into the proposed four categories, i.e., empirical bottom-up, empirical top-down, theoretical bottom-up and theoretical bottom-down, and found that, to certain extent, empirical bottom-up methods are the most dominating methodology in the ir field, indicating a strong bias toward empirical rather than theoretical work\n motivated by the analysis results, we propose a general methodology for ir research that aims to leverage the strengths of both theoretical and empirical methodologie", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The unpredictability of user behavior and the need for effectiveness make it difficult to define a suitable research methodology for Information Retrieval (IR).\nIn order to tackle this challenge, we categorize existing IR methodologies along two dimensions: (1) empirical vs. theoretical, and (2) top-down vs. bottom-up\nThe strengths and drawbacks of the resulting categories are characterized according to 6 desirable aspects\nThe analysis suggests that different methodologies are complementary and therefore, equally necessary.\nThe categorization of the 167 full papers published in the last SIGIR (2016 and 2017) and ICTIR (2017) conferences suggest that most of existing work is empirical bottom-up, suggesting lack of some desirable aspects\nWith the hope of improving IR research practice, we propose a general methodology for IR that integrates the strengths of existing research method"}]
