[{"doc": "neural ir models have received much attention due to their continuous text representations, soft-matching of terms, and sophisticated non-linear models\nhowever, the non-convexity and stochastic training of neural ir models raises questions about their consistency compared to heuristic and learning-to-rank models that use discrete representations and simpler methods of combining evidence\nconsistent behavior under slightly different conditions is essential to reproducible research and deployment in industry\nthis paper studies the stability of k-nrm, a recent state-of-the-art neural ranking model [10]\nk-nrm learns the word embeddings and ranking model from relevance signals\nits effectiveness is due to word embeddings tailored for search tasks and kernels that group matches into bins of different quality\nto better understand its stability, we compare the behavior of multiple trained models under similar conditions\n we find that although k-nrm produces similar accuracy across different trials, it also produces rather different document rankings for individual queries\nanalysis of weights learned for k-nrm kernel scores (soft-match features) revealed that weights from different trials match one of two patterns.\nthe word embeddings reflect these patterns\n trials whose kernel weights have the same pattern have similar word embeddings\n interestingly, the two patterns are equally effective\nthe difference in the ranking patterns from different k-nrm trials makes them a good fit for ensembles\naggregating scores from different trials enables an ensemble to promote documents that multiple trials agree are most likely to be relevant\nexperimental results show that simple k-nrm ensembles significantly boost its ranking accuracy and improve its generalization ability\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "This paper studies the consistency of the kernel-based neural ranking model (K-NRM), a recent state-of-the-art neural IR model, which is important for reproducible research and deployment in the industry\nWe find that K-NRM has low variance on relevance-based metrics across experimental trials\nIn spite of this low variance in overall performance, different trials produce different document rankings for individual queries\nThe main source of variance in our experiments was found to be different latent matching patterns captured by K-NRM\nIn the IR-customized word embeddings learned by K-NRM, the query-document word pairs follow two different matching patterns that are equally effective, but align word pairs differently in the embedding space\nThe different latent matching patterns enable a simple yet effective approach to construct ensemble rankers, which improve K-NRM\u2019s effectiveness and generalization abilities\n"}, {"doc": "among various recommendation techniques, the most successful approach is collaborative filtering (cf) [4]; it recommends items to a user based on previous ratings of other users whose tastes are similar to the target user\n however, this in turn implies that the performance of cf will suffer without a sufficient amount of ratings previously given by users, which is common in reality\nto compensate for the sparsity of the user\u2013item rating data, side information related to users and items, such as user social network [8], user review documents [11\u201314], and item affinity network [9] has been actively leveraged\n in this work, we specifically focus on user review-aware recommendation\nuser reviews are particularly useful for alleviating the sparsity of user ratings, because the reviews not only embody a user\u2019s intention behind the ratings, but also contain conspicuous item properties\nthat is to say, if reviews are fully exploited, we can build recommender systems even with few ratings provided, which naturally alleviates the sparsity of user\u2013item rating data\nto extract meaningful features from review documents, deep learning-based approaches have been recently proposed [1, 13]\nmore specifically, convolutional neural network (cnn)-based recommendation methods have gained attention [3, 11, 14] thanks to the capability of cnns to capture general contextual features from documents\ndeepconn [14] adopts two cnns, where one of them models users through reviews written by the users, while the other models items through reviews written for the items.\nbuilding upon deepconn, seo et al. propose d-attn [11] that further adopts the dual local and global attention mechanism on the cnns, which endow the recommender systems with interpretability regarding the reviews that are used for modeling users and items\ndespite their state-of-the-art performance, they are limited in that users and items are modeled by the reviews consisting of raw words.\nhowever, each user has different tendency in writing a review and thus words contain an inherent ambiguity, which makes it hard to precisely understand the user\u2019s intent\nas a concrete example, let\u2019s assume that two different users provided reviews that contain the following identical sentence: \u201c... i like the laptop... \u201d\nwhereas a tolerant user would use the word \u201clike\u201d to describe an adequate laptop, a critical user would not use it unless he is completely satisfied with the laptop\n however, the previous reviewaware methods simply aggregate all the associated reviews and feed them to cnns expecting the cnns to automatically extract meaningful features for modeling users and items, which does not suffice for precisely modeling the users and items\nthis phenomenon compounds when users provided only a few reviews, i.e., cold-start [10], which is common in reality\nmoreover, as the existing approaches model each user/item by the concatenation of all the words from every associated review, the size of input for cnns becomes considerably large, which makes the above approaches practically not feasible in the real-world applications\nin this paper, to overcome the above limitations of the existing methods, we propose a novel sentiment guided review-aware recommendation method, called sentirec.\nthe core idea is to leverage the overall sentiments of reviews that are represented as ratings that accompany the reviews.\nin our previous example, if we have a prior knowledge that the tolerant user gave a 3-star rating to the laptop while the critical user gave a 5-star rating, we will be able to more accurately understand the review, which in turn enables us to better model users and items\nour proposed method consists of two steps.\nin the first step, instead of representing a review by the concatenation of its constituent raw words as in the previous methods, we encode each review into a fixed-size review vector that is guided to embody the sentiment information of the review\nmore precisely, we regard a rating that accompanies a review as a summarization of the overall sentiment of a user on an item, and train a cnn that is designed to predict the rating given the review as input, after which a fixed-size vector for the review is obtained by taking the output of the last hidden layer\nthe second step resembles the training process of deepconn and d-attn, but is distinguished in that users/items in sentirec are represented by the concatenation of their associated fixed-size review vectors, rather than raw words\nthe advantages of sentirec compared with the previous methods are\n1) we obtain more accurate representations for reviews by incorporating users\u2019 overall sentiments on items into reviews, which removes the possible ambiguity contained in the reviews.\nthis in turn results in a better understanding of the reviews, and leads to more accurate representations for users and items resulting in an improved recommendation accuracy\n moreover, 2) we drastically reduce the size of the input, which gives us scalability in terms of the training time and the memory usage\nour experiments show that sentirec outperforms the state-of-the-art baselines, while being considerably more efficient\nmoreover, we perform a qualitative evaluation on the review vectors trained by sentirec to ascertain that the overall sentiments are indeed encoded in the vectors\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Existing review-aware recommendation methods represent users (or items) through the concatenation of the reviews written by (or for) them, and depend entirely on convolutional neural networks (CNNs) to extract meaningful features for modeling users (or items)\nHowever, understanding reviews based only on the raw words of reviews is challenging because of the inherent ambiguity contained in them originated from the users\u2019 different tendency in writing\n Moreover, it is inefficient in time and memory to model users/items by the concatenation of their associated reviews owing to considerably large inputs to CNNs\nIn this work, we present a scalable reviewaware recommendation method, called SentiRec, that is guided to incorporate the sentiments of reviews when modeling the users and the items\nSentiRec is a two-step approach composed of the first step that includes the encoding of each review into a fixed-size review vector that is trained to embody the sentiment of the review, followed by the second step that generates recommendations based on the vector-encoded reviews\nThrough our experiments, we show that SentiRec not only outperforms the existing reviewaware methods, but also drastically reduces the training time and the memory usage.\nWe also conduct a qualitative evaluation on the vector-encoded reviews trained by SentiRec to demonstrate that the overall sentiments are indeed encoded therein\n"}, {"doc": "community-acquired pneumonia (cap) [13] refers to the lungs of patients infected when they are not in hospital\nit has long been a major cause of morbidity and death, especially for children\nas reported by the studies [12, 15], pneumonia is one of the top ranked diseases responsible for the deaths of children both in usa and china\ncuring cap largely requires an early administration of appropriate antibiotics [9]\nunfortunately, the issue of the abuse of antibiotics is very prevalent, especially in developing countries such as china [7], which seriously endangers human health\nalleviating the above issue needs an accurate detection of pathogenic microorganism [13].\npathogenic microorganism is a family of microorganisms which will cause human diseases\nif the pathogenic microorganism of cap can be precisely identified, clinicians are able to prescribe optimal antibiotics\nconventional gold-standard detection methods are mainly etiology based, including culture based assays, polymerase-chain-reaction (pcr), etc\nhowever, many of them need specialized equipment and reagents, and are labor and time intensive [4, 17], which limit their application only in major hospitals.\nthus, there is an urgent need to develop intelligent and cost-effective methodologies to detect pathogenic microorganism of cap using data which is easier to be acquired\nrecent progress in wide collection of electronic health records (ehrs) [8] applies the methodologies from artificial intelligence community to cap\nhowever, existing studies in this regard are somewhat limited and mainly aim at 1) predicting whether suspected patients have pneumonia [16] or 2) further judging the risk of patients with pneumonia [3]\nmost of them have ignored to investigate the power of patient easy-to-acquire data from ehrs for automatically detecting pathogenic microorganism of cap.\nin fact, it plays a great role in treating cap children\nin this paper, we formulate a new problem of utilizing pneumonia patients multiple medical features from ehrs to identify their pathogenic microorganisms\nto our best knowledge, none of previous studies has investigated this problem\nthe studied features include time-varying body temperature and some carefully selected clinical measurements which are easy to be acquired, such as white blood cell count from routine blood test (see table 1 for details)\nconsequently, the central challenge is how to effectively fuse the above multiple types of features and construct an effective model for the problem\nto address the challenge, we develop a patient attention based recurrent neural network (pa-rnn), which is capable of modeling sequential body temperatures and fusing multiple types of patient features\nto be specific, pa-rnn first exploits the power of recurrent neural network (rnn) to obtain a sequence of body temperature representations for different time steps\nmeantime it constructs patient basic features which are carefully selected from ehrs\nafterwards, inspired by attention mechanism [1], pa-rnn provides a patient feature based attention to determine the importance of each time-varying temperature representation and further gains an integrated representation for a whole body temperature sequence\nfinally, the model fuses the integrated representation with the representation of patient basic features for pathogenic microorganism detection\nin a nutshell, the major novelty of pa-rnn is that most previous studies which utilize rnn to model ehrs [2, 5, 10, 11, 14] focus on predicting targets at the next time step based on current hidden states of rnn\nhowever, we obtain an integrated representation of body temperatures sequence through a novel patient feature based attention computation to all hidden states of rnn.\nwe conduct comprehensive experiments on a real world dataset from a major hospital in china, indicating the benefit of fusing multiple types of features from ehrs for the studied problem, and demonstrating the effectiveness of pa-rnn over several alternative methods\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Community-acquired pneumonia (CAP) is a major death cause for children, requiring an early administration of appropriate antibiotics to cure it\nTo achieve this, accurate detection of pathogenic microorganism is crucial, especially for reducing the abuse of antibiotics\nConventional gold standard detection methods are mainly etiology based, incurring high cost and labor intensity\nAlthough recently electronic health records (EHRs) become prevalent and widely used, their power for automatically determining pathogenic microorganism has not been investigated\nIn this paper, we formulate a new problem for automatically detecting pathogenic microorganism of CAP by considering patient biomedical features from EHRs, including time-varying body temperatures and common laboratory measurements\nWe further develop a Patient Attention based Recurrent Neural Network (PA-RNN) model to fuse different patient features for detection\nWe conduct experiments on a real dataset, demonstrating utilizing electronic health records yields promising performance and PA-RNN outperforms several alternatives\n"}, {"doc": "personalized recommender systems are pivotal in enhancing customers online shopping experiences, due to their ability to make personalized recommendations\nthese recommender systems learn user behavior from past observations, which are captured either through explicit user feedback, i.e. ratings or implicitly from user interaction with the system, i.e. the user purchase history, movies watched etc\nin real world systems, it is usually inexpensive to capture implicit feedback\nthe recommender systems generate a ranked list of targets i, for each personalized request represented through a given context c.\nthe ranking function r(i|c) \\\\rightarrow  \\\\mathbb{n}^{+}, defined in (1), generates an ordered ranking of the targets for each context c \\\\epsilon c, where \\\\hat{y}(i|c, \\\\theta ) is a model with model parameters \\\\theta\nr(i|c) = | \\\\left \\\\{ j|\\\\hat{y}(j|c, \\\\theta ) \\\\geq  \\\\hat{y}(i|c, \\\\theta ) \\\\right \\\\}| (1\nthe pairwise ranking algorithms [10] have shown to outperform pointwise ranking algorithms [5, 6] on the ranking task\nthe pairwise methods are well suited for implicit feedback as they learn a pairwise loss over a set of observed positive feedback and a set of unobserved feedback\nhowever, scalability studies of the pairwise methods for implicit feedback are limited, and generally focused on shared memory systems [11]\nin this paper we address the scalability of a pairwise ranking algorithm for a large scale dataset\nspecifically, we investigate the viability of existing distributed stochastic gradient optimization (sgo) algorithms [3, 5], which were originally designed for a pointwise loss and are mostly limited to a single relational matrix factorization (mf)\nthese methods block partition the rating matrix and create static blocks of model parameters\nhowever, the implicit feedback dataset only contains observed positive examples and unobserved examples are sampled\nthe static block partitioning of model parameters limits the sample space of unobserved examples, which can therefore introduce a bias in the gradient updates\nwe present a dynamic block partitioning and exchange strategy for model parameters by utilizing the information about the frequency of occurrence of features in each local data partition\n we demonstrate the applicability of our algorithm by using a generic framework based on factorization machines (fm) [9] to solve a pairwise ranking problem including multiple entity relationships\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Learning with pairwise ranking methods for implicit feedback datasets has shown promising results as compared to pointwise ranking methods for recommendation tasks\nHowever, there is limited effort in scaling the pairwise ranking methods in a large scale distributed setting\nIn this paper we address the scalability aspect of a pairwise ranking method using Factorization Machines in distributed settings\nOur proposed method is based on a block partitioning of the model parameters so that each distributed worker runs stochastic gradient updates on an independent block\nWe developed a dynamic block creation and exchange strategy by utilizing the frequency of occurrence of a feature in the local training data of a worker.\nEmpirical evidence on publicly available benchmark datasets indicates that the proposed method scales better than the static block based methods and outperforms competing state-ofthe- art methods\n"}, {"doc": "users at particular locations typically have information needs based on their immediate geographic context\nfor example, a user at a restaurant engaging with a search system is likely to be searching for that restaurant\u2019s menu\nrecent works have studied this kind of contextual information, even going so far as to consider zero-query ranking [4, 15]\nthese works focus on the web, where query log mining can provide an understanding of trends and global behavior\nin the personal search domain, the challenge becomes more difficult: one cannot simply learn location-wise trending behavior due to the privacy constraints of personal search\nusing a set of anonymized email search logs with location information provided by google, we explore whether location information can be leveraged for query auto-completion\nsince we are unable to submit new queries, we explore a simulated task on this raw log data: user-independent query suggestion\nwe ask whether we can predict the queries a user is going to issue based upon 1. any characters they have already entered (possibly none), and 2. the location information\nthe contributions of this paper are as follows\nwe validate that location information is valuable for personal search by demonstrating the ability to predict queries using location information\nwe validate that semantic location information is valuable, using a non-parametric click-context model that allows us to learn location information from queries and documents with and without location associations\nwe observe that users often manually expand their personal search queries with their location context, indicating that it is a strong signal for relevance\nwe demonstrate our first two contributions by focusing on a query prediction or suggestion task: using minimal or no query information, we try to use the location information in our log to predict the queries\nin doing so, we explore a handful of models and look at their ability to generalize and perform on this task\nwe find that hashes of gps location provide evidence that location is helpful, but the coverage of this technique is not ideal: the majority of unique locations in our test set remain unseen even though our training set is larger\nwith much more data we would expect this problem to dissipate, but we look to a better opportunity: semantic location information\nwe annotate our query logs with geographic entity look-up: that is, for every latitude/longitude point, we perform a search of the nearest point of interest item using the google places web api, and include the title of this point in our extended logs\n  these titles provide the basis for our generalization\nfinally, we analyze our performance on query completion and find some surprising behavior in this task.\nour core observation is that users manually expand their queries with location, and hypothesize that will be difficult to beat this \u201chuman-expansion\u201d baseline if we were to look at improving search satisfaction directly (until users realize they no longer need to manually include location names)\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": " Mobile devices are pervasive, which means that users have access to web content and their personal documents at all locations, not just their home or office\nExisting work has studied how locations can influence information needs, focusing on web queries\nWe explore whether or not location information can be helpful to users who are searching their own personal documents\nWe wish to study whether a users\u2019 location can predict their queries over their own personal data, so we focus on the task of query suggestion\nWhile we find that using location directly can be helpful, it does not generalize well to novel locations.\nTo improve this situation, we explore using semantic location: that is, rather than memorizing location-query associations, we generalize our location information to names of the closest point of interest\nBy using short, semantic descriptions of locations, we find that we can more robustly improve query completion and observe that users are already using locations to extend their own queries in this domain\nWe present a simple but effective model that can use location to predict queries for a user even before they type anything into a search box, and which learns effectively even when not all queries have location information\n"}, {"doc": "in recent years, recommender systems have played a significant part in the multimedia field\nhowever, within most of these web services, the number of users and the number of images/videos are dramatically growing, making the multimedia recommendation more challenging than ever before\nthe dominating web multimedia content requires modern recommender systems, in particular, those based on collaborative filtering (cf), to sift through massive multimedia content for users in a highly dynamic environment\ncollaborative filtering methods group people with similar interests and make recommendations on this basis.\nin the context of multimedia recommendation, item indicates different kinds of multimedia content\nmost cf methods rely on items star ratings, which provide explicit feedback [5, 9]\nhowever, when used in the multimedia field, traditional cf methods have two shortcomings.\nfirst, cf methods failed to focus on the multimedia content itself, which is the most important factor when users choose images or videos\nas content information of items is available, content-aware methods have been introduced\nsuch incorporation of content information usually leads to better recommendation performance [1, 10, 12].\nsecond, explicit ratings are not always available in many applications\nmore often, interaction data such as \"like\" of photos, or \"view\" of movies are more convenient to collect\nsuch data are based on implicit feedback\nto combine the multimedia content with the cf method and make the best use of implicit feedback, we propose a novel contentaware multimedia recommendation framework with graph autoencoder (graphcar)\nwe use two graph convolutional networks as the encoder to model latent factor of users and items, respectively\nafter that, we generate preference scores by using the inner product of two latent factor vectors\nwe evaluate graphcar extensively on two real-world datasets that represent a spectrum of different media: the amazon movies and tvs dataset and the vine dataset, with the former providing images features and the latter providing videos features\nthrough these experiments, we observe that graphcar is superior to competing methods of the best configuration, ranging from cf-based methods to content-based methods\nin summary, the main contributions of our work are\nwe propose a novel content-aware multimedia recommendation model with graph autoencoder (graphcar) to employ graph autoencoder in cf with implicit feedback\nto combine user-item interaction with user attributes and the multimedia content, we introduce two graph convolutional networks, both of which are neural networks that can be seamlessly incorporated into any neighborhood models with efficient end-to-end sgd training\nexperiments on two real datasets show that graphcar significantly outperforms state-of-the-art techniques of both cf and content-based methods\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Precisely recommending relevant multimedia items from massive candidates to a large number of users is an indispensable yet difficult task on many platforms\nA promising way is to project users and items into a latent space and recommend items via the inner product of latent factor vectors\nHowever, previous studies paid little attention to the multimedia content itself and couldn\u2019t make the best use of preference data like implicit feedback.\nTo fill this gap, we propose a Content-aware Multimedia Recommendation Model with Graph Autoencoder (GraphCAR), combining informative multimedia content with user-item interaction.\nSpecifically, user-item interaction, user attributes and multimedia contents (e.g., images, videos, audios, etc.) are taken as input of the autoencoder to generate the item preference scores for each user\nThrough extensive experiments on two real-world multimedia Web services: Amazon and Vine, we show that GraphCAR significantly outperforms state-of-the-art techniques of both collaborative filtering and content-based methods\n"}, {"doc": " recently, several deep learning models for information retrieval have been proposed [2, 4, 6, 8]\nthese models have demonstrated their potential to improve the effectiveness in ad-hoc search\nin general, a deep neural model is constructed to represent the content of the document and the query [4, 8], and/or their interactions or matching scores [2, 6].\nthe utilization of deep neural models is motivated by their ability to make high level and more abstract matching between the document and the query, thereby alleviating the vocabulary mismatch problem\nwe observe, however, that these models only use one level of final representation or matching score for any document-query pair\nfor example, in [8], several layers of convolutions are used to create more and more abstract representations for the document and the query, and the matching score only relies on the last layer of representation\nconvolution is an operation that aggregates lower-level features to produce more abstract features\na matching score at the highest level tends to reflect a conceptual matching\nin reality, user\u2019s queries may be of different nature\nsome queries such as \u201cron howard\u201d (a query in clueweb) asking for information about a celebrity would require a low level lexical matching rather than conceptual matching.\n we call them lexical queries\n on the other hand, a query like \u201clymphoma in dogs\u201d is intended to find document about corresponding concept(s), therefore a high level conceptual matching is preferred\nthese queries are called conceptual queries\nthese examples clearly show the need for matching document and query at different levels of abstraction.\ninspired by this intuition, in this paper we propose a multi-level abstraction convolutional model (macm), which integrates document-query matching at different levels of abstraction\nthis model is expected to have a better capability of coping with different types of user queries\nalthough neural ir models can focus either on document and query representation or on interactions between them, guo et al. [2] showed that the latter is more effective than the former\nbased on this observation, our model is built on document-query interactions rather than representations.\na critical problem in building deep neural models for ir is the requirement of a large amount of labeled training data, which is often unavailable\nthe idea of weak supervision by a traditional ir model is proposed recently [1] and shown to be effective\ninspired by this work, we employ the bm25 retrieval model [7] for weak supervision - the ranked documents retrieved by bm25 are used to train our deep neural model\nwe will see that this strategy is able to train our deep neural model, leading to superior effectiveness to bm25\nthe main contribution of our paper lies in a new neural model capable of coping with different types of queries by matching them with documents at different levels of abstraction\nthis idea can be easily adopted in other deep neural models, whether they are based on representations or interactions, use cnn or rnn\nour experiments on clueweb confirm that our approach can result in superior retrieval effectiveness\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Recent neural models for IR have produced good retrieval effectiveness compared with traditional models\nYet all of them assume that a single matching function should be used for all queries.\n In practice, user\u2019s queries may be of various nature which might require different levels of matching, from low level word matching to high level conceptual matching\nTo cope with this problem, we propose a multi-level abstraction convolutional model (MACM) that generates and aggregates several levels of matching scores\nWeak supervision is used to address the problem of large training data\nExperimental results demonstrated the effectiveness of our proposed MACM model\n"}, {"doc": "conversational assistants (cas) such as siri and cortana are becoming increasingly popular.\nusers can issue simple queries and commands to a ca by voice to conduct single-turn qa or goal-oriented tasks, such as asking for weather and setting timers\nhowever, cas are not yet capable of handling complicated information-seeking tasks which involve multiple turns of information exchange\nthese conversations are typically referred to as information-seeking conversations, where the information provider (agent) provides answers to a query from an information seeker (user) and the agent modifies the answers based on user feedback\nto build functional and natural cas that can reply to more complicated tasks we need to understand how users interact in these information-seeking environments\nthus, it is necessary to analyze and characterize user interactions and utterance intent\nat cair1 workshop at sigir17, researchers indicated that there is a lack of conversational datasets to conduct studies\ntherefore in this paper, we address this issue by collecting conversation data and creating the msdialog dataset\nwe present an analysis of user intent here, but msdialog could also be used to conduct other dialog related tasks including response ranking and user intent prediction\nfor effective analysis of user intent in an information-seeking process, the data should be multi-turn information-seeking dialogs\nto support natural dialogs, conversational systems should be modeled closely to human behavior, thus the data should come from conversation interactions between real humans\n as shown in table 1, we found that most existing dialog datasets are not appropriate for user intent analysis\nthe most similar data to ours is the ubuntu dialog corpus (udc), which also contains multi-turn qa conversations in the technical support domain\nhowever, the user intent in this dataset is unlabeled\nin addition, udc dialogs are in irc (internet relay chat) style\nthis informal language style contains a significant amount of typos, internet language, and abbreviations\nanother dataset, the dstc 6 conversation modeling track data contains knowledge grounded dialogs from twitter\nhowever, this dataset contains scenarios where users do not request information explicitly, which do not fit the information-seeking narrative\nthus these datasets are not appropriate for user intent analysis\nfor open-domain chatting, it is common practice to train chatbots with social media data such as twitter [13]\n similarly, real human-human multi-turn qa dialogs are the appropriate data for characterizing user intent in information-seeking conversations\nin technical support online forums, a thread is typically initiated by a user-generated question and answered by experienced users (agents)\nthe users may also exchange clarifications with the agents or give feedback based on answer quality\nthus the flow of a technical support thread resembles the information-seeking process if we consider threads as dialogs and posts as turns/utterances in dialogs\nwe created msdialog by crawling multi-turn qa threads from the microsoft community and annotate them with fine-grained user intent types on an utterance level based on crowdsourcing on amazon mechanical turk (mturk)\nwith this new dataset, we analyze the user intent distribution, co-occurrence patterns and flow patterns of large-scale qa dialogs\nwe gain insights on human intent dynamics during information seeking conversations\none of the most interesting findings is the high co-occurrence of negative feedback and further details, which typically occurs after a potential answer is given.\nthis cooccurrence pattern provides feedback about the retrieved answer and critical information about how to improve the previous answer\nin addition, negative feedback often leads to another answer response, indicating that co-occurrence and flow patterns associated with negative feedback can be the key to iterative answer finding\nto sum up, our contributions can be summarized as follows.\n(1) we create a large-scale annotated dataset for multi-turn informationseeking conversations, which is the first of its kind to the best of our knowledge\nwe will make our dataset freely available to encourage relevant studies.\n(2)we perform in-depth data analysis and characterization of multi-turn human qa conversations\n we analyze the user intent distribution, co-occurrence and flow patterns\n  our characterizations also hold in similar data (udc)\nour findings could be useful for designing conversational search systems\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Understanding and characterizing how people interact in information seeking conversations is crucial in developing conversational search systems.\nIn this paper, we introduce a new dataset designed for this purpose and use it to analyze information-seeking conversations by user intent distribution, co-occurrence, and flow patterns\n The MSDialog dataset is a labeled dialog dataset of question answering (QA) interactions between information seekers and providers from an online forum on Microsoft products.\nThe dataset contains more than 2,000 multi-turn QA dialogs with 10,000 utterances that are annotated with user intent on the utterance level\nAnnotations were done using crowd sourcing\nWith MSDialog, we find some highly recurring patterns in user intent during an information-seeking process\nThey could be useful for designing conversational search systems\nWe will make our dataset freely available to encourage exploration of information-seeking conversation models\n"}, {"doc": "there is a growing body of work investigating different factors which affect user\u2019s judgment of relevance [1, 2, 6, 7, 9, 11\u201313]\na multidimensional user relevance model (murm) was proposed [12, 13] which defined five dimensions of relevance namely \"novelty\", \"reliability\", \"scope\", \"topicality\" and \"understandability\"\nin a recent paper [7] an extended version of the murm comprising two additional dimensions \"habit\" and \"interest\" is proposed\nthe \"interest\" dimension refers to the topical preferences of users in the past, while \"habit\" refers to their behavioral preferences\nfor example, accessing specific websites for some particular information or task is considered under the \"habit\" dimension\nexperiments on real-world data show that certain dimensions, such as \"reliability\" and \"interest\", are more important for the user than \"topicality\", in judging a document\nour hypothesis is that in a particular search session or search task, there is a particular relevance dimension or a combination of relevance dimensions which the user has in mind before judging documents\nfor example, if the user wants to get a visa to a country, he or she would prefer documents which are more reliable (\"reliability\") for this task, but when looking to book flights to that country, the user might go to his or her preferred websites (\"habit\").\ntherefore, for next few queries of the session, \"habit\" dimension becomes more important\nthus, the importance given to relevance dimensions might change as the session progresses or tasks switch\nby capturing the importance assigned to each dimension for a query, we can model the dimensional importance and use it to improve the ranking for the subsequent queries\n the relevance dimensions are modeled using the hilbert space formalism of quantum theory which unifies the logical, probabilistic and vector space based approaches to ir [8]\nwe place the user\u2019s cognitive state with respect to a document at the center of the ir process\nsuch a state is modeled as an abstract vector with multiple representations existing at the same time in different basis corresponding to different relevance dimensions\nthis cognitive state comes into reality only when it is measured in the context of user interactions\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "It has been shown that relevance judgment of documents is influenced by multiple factors beyond topicality\nSome multidimensional user relevance models (MURM) proposed in literature have investigated the impact of different dimensions of relevance on user judgment.\nOur hypothesis is that a user might give more importance to certain relevance dimensions in a session which might change dynamically as the session progresses.\nThis motivates the need to capture the weights of different relevance dimensions using feedback and build a model to rank documents for subsequent queries according to these weights.\nWe propose a geometric model inspired by the mathematical framework of Quantum theory to capture the user\u2019s importance given to each dimension of relevance and test our hypothesis on data from a web search engine and TREC Session track\n"}, {"doc": "most of information retrieval (ir) contributions follow a standard structure: analysis of the state of the art, description of the approach, and empirical evaluation over a certain data set\nthe large amount of available annotated collections allows to improve models by trial and error\nhowever, this methodology does not match with the standard scientific procedures: hypothesis statement, definition of an experiment guided by the specific hypothesis, and result analysis\nas a consequence, the ir community tends to produce solutions to a greater extent than knowledge\nthe slow progress in creating new knowledge in ir is at least partly because it is not easy to import scientific methodologies from other areas such as physics or human sciences into ir; unlike in other engineering areas, the unpredictability of human behavior makes it difficult to find general laws that describe precise phenomena\n(note that in this paper we focus on effectiveness rather than efficiency or scalability in which the user is not involved.)\nregarding social and psychological researches, the need for effectiveness in systems makes futile the production of general principles\nthis situation makes us wonder whether the current practices in ir research are on the \u201cright\" track toward discovery of new knowledge about ir.\nthe more general question here is:\nwhat are the best methodologies (if any) that researchers should follow to optimally advance the knowledge in ir research\n to address this question, in this paper, we first try to quantify the current methodological trends in ir research and categorize existing ir methodologies along two dimensions: (1) empirical vs. theoretical, and (2) top-down vs. bottom-up\n we then identify six desirable properties and anaylze these four types of methodologies accordingly.\nthe analysis indicates that none of the methodologies can satisfy all the desirable properties but they are complementary to each other\nfor example, theoretical methodologies give theoretical foundations, interpretability, and robustness across new scenarios, while empirical methodologies provide evidence about the relative effectiveness of approaches in particular realistic scenarios, as well as providing statistical significance when comparing systems to each other\nfurthermore, we categorized the 167 full papers published in the sigir 2016 and 2017 as well as ictir 2017 conferences into the proposed four categories, i.e., empirical bottom-up, empirical top-down, theoretical bottom-up and theoretical bottom-down, and found that, to certain extent, empirical bottom-up methods are the most dominating methodology in the ir field, indicating a strong bias toward empirical rather than theoretical work\n motivated by the analysis results, we propose a general methodology for ir research that aims to leverage the strengths of both theoretical and empirical methodologies\n", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The unpredictability of user behavior and the need for effectiveness make it difficult to define a suitable research methodology for Information Retrieval (IR).\nIn order to tackle this challenge, we categorize existing IR methodologies along two dimensions: (1) empirical vs. theoretical, and (2) top-down vs. bottom-up\nThe strengths and drawbacks of the resulting categories are characterized according to 6 desirable aspects\nThe analysis suggests that different methodologies are complementary and therefore, equally necessary.\nThe categorization of the 167 full papers published in the last SIGIR (2016 and 2017) and ICTIR (2017) conferences suggest that most of existing work is empirical bottom-up, suggesting lack of some desirable aspects\nWith the hope of improving IR research practice, we propose a general methodology for IR that integrates the strengths of existing research methods\n"}]