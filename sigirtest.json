[{"doc": "the transition to web 2.0 transformed the business models of online marketing from a global ad approach based to individual opinions and targeted campaigns [2, 23, 35, 40]\nweb 2.0 not only took traditional marketing strategies to the extreme via viral marketing campaigns [31, 36, 43], but it also gave rise to new techniques of brand building and audience targeting via influencer marketing [12, 44].\nin fact, the use of micro-influencers, trusted individuals within their communities, has been seen as a more effective way to build a brand in terms of audience reception and return on investment [9, 25, 32]\ninstagram, which is a visual content sharing online social network (osn), has become a focal point for influencer marketing\nwith power users and micro-influencers publishing sponsored content companies need to rate these influencers and determine their value[17, 18, 38].\nmost of today\u2019s scoring themes rely on graphbased algorithms of a known network graph.\nsuch graphs are not always available, and building them for instagram users requires a great deal of resources, e.g., crawling time and computing costs\n a possible solution would be to infer the underlying network structure using the user activity logs, as described by barbieri et al.[7], but even in the event a graph is constructed it would not necessarily be of much use given that information decays exponentially along the graph even under optimal passive information propagation, which is not the case\nthe rest of the paper is organized as follows: in section 2 we described osns in greater detail aswell as current influence measuring schemes\nwe then present our annotations and formal description of the problem of measuring and ranking influence in section 3.\nthe dataset of instagram users and their posts is described in section 4, followed by discussion on the extracted and aggregated features of the testable data in section 4.2\nfollowing this, we present our testing methodology, baselines, regression models and experimental results in section 6\nfinally, we discuss our conclusion and possible future work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "This paper focuses on the problem of scoring and ranking influential users of Instagram, a visual content sharing online social network (OSN)\n Instagram is the second largest OSN in the world with 700 million active Instagram accounts, 32% of all worldwide Internet users1\nAmong the millions of users, photos shared by more influential users are viewed by more users than posts shared by less influential counterparts\nThis raises the question of how to identify those influential Instagram users\nIn our work, we present and discuss the lack of relevant tools and insufficient metrics for influence measurement, focusing on a network oblivious approach and show that the graph-based approach used in other OSNs is a poor fit for Instagram.\nIn our study, we consider user statistics, some of which are more intuitive than others, and several regression models to measure users\u2019 influenc"},
{"doc": "linkedin and lagou, has enabled the new paradigm for talent recruitment\n for instance, in 2017, there are 467 million users and 3 million active job listings in linkedin from about 200 countries and territories all over the world [3]\nwhile popular online recruitment services provide more convenient channels for both employers and job seekers, it also comes the challenge of person-job fit due to information explosion\naccording to the report [23], the recruiters now need about 42 days and $4,000 dollars in average for locking a suitable employee [23]\nclearly, more effective techniques are urgently required for the person-job fit task, which targets at measuring the matching degree between the talent qualification and the job requirements\nindeed, as a crucial task for job recruitment, person-job fit has been well studied from different perspectives, such as job-oriented skill measuring [34], candidate matching [22] and job recommendations [20, 27, 38]\nalong this line, some related tasks, such as talent sourcing [33, 39] and job transition [31] have also been studied\nhowever, these efforts largely depend on the manual inspection of features or key phrases from domain experts, and thus lead to high cost and the inefficient, inaccurate, and subjective judgments\nto this end, in this paper, we propose an end-to-end abilityaware person-job fit neural network (apjfnn) model, which has a goal of reducing the dependence on human labeling data and can provide better interpretation about the fitting results\nthe key idea of our approach is motivated by the example shown in figure 1\nthere are 4 requirements including 3 technical skill (programming, machine learning and big data processing) requirements and 1 comprehensive quality (communication and team work) requirement\nsince multiple abilities may fit the same requirement and different candidates may have different abilities, all the abilities should be weighed for a comprehensive score in order to compare the matching degree among different candidates\nduring this process, traditional methods, which simply rely on keywords/feature matching, may either ignore some abilities of candidates, or mislead recruiters by subjective and incomplete weighing of abilities/experiences from domain experts\ntherefore, for developing more effective and comprehensive person-job fit solution, abilities should be not only represented via the semantic understanding of rich textual information from large amount of job application data, but also automatically weighed based on the historical recruitment results\nalong this line, all the job postings and resumes should be comprehensively analyzed without relying on human judgement\nto be specific, for representing both the job-oriented abilities and experiences of candidates, we first propose a word-level semantic representation based on recurrent neural network (rnn) to learn the latent features of each word in a joint semantic space\nthen, two hierarchical ability-aware structures are designed to guide the learning of semantic representation for job requirements as well as the corresponding experiences of candidates\nin addition, for measuring the importance of different abilities, as well as the relevance between requirements and experiences, we also design four hierarchical ability-aware attention strategies to highlight those crucial abilities or experience\nthis scheme will not only improve the performance, but also enhance the interpretability of matching results\nfinally, extensive experiments on a large-scale real-world data set clearly validate the effectiveness of our apjfnn framework compared with several baselines\noverview\nthe rest of this paper is organized as follows\nin section 2, we briefly introduce some related works of our study\nin section 3, we introduce the preliminaries and formally define the problem of person-job fit\nthen, technical details of our abilityaware person-job fit neural network will be introduced in section 4\nafterwards,we comprehensively evaluate the model performance in section 5, with some further discussions on the interpretability of results\nfinally, in section 6, we conclude the pape", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The wide spread use of online recruitment services has led to information explosion in the job market\nAs a result, the recruiters have to seek the intelligent ways for Person-Job Fit, which is the bridge for adapting the right job seekers to the right positions\nExisting studies on Person-Job Fit have a focus on measuring the matching degree between the talent qualification and the job requirements mainly based on the manual inspection of human resource experts despite of the subjective, incomplete, and inefficient nature of the human judgement\nTo this end, in this paper, we propose a novel end-to-end Ability-aware Person-Job Fit Neural Network (APJFNN) model, which has a goal of reducing the dependence on manual labour and can provide better interpretation about the fitting results\nThe key idea is to exploit the rich information available at abundant historical job application data\nSpecifically, we propose a word-level semantic representation for both job requirements and job seekers\u2019 experiences based on Recurrent Neural Network (RNN)\nAlong this line, four hierarchical ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measuring the different contribution of each job experience to a specific ability requirement\nFinally, extensive experiments on a large-scale realworld data set clearly validate the effectiveness and interpretability of the APJFNN framework compared with several baseline"},
{"doc": "fake news, misinformation, rumor or hoaxes are one of the most concerning problems due to their popularity and negative effects on society\nparticularly, social networking sites (e.g., twitter and facebook) have become a medium to disseminate fake news\ntherefore, companies and government agencies have paid attention to solving fake news\nfor example, facebook has a plan to combat fake news and the fbi has investigated disinformation spread by russia and other countries\nto verify correctness of information, researchers proposed to (i) employ experts, who can fact-check information [59], (ii) use systems that can automatically check credibility of news [19, 33, 46]; and build models to detect fake news [7, 24, 35, 42, 53]\nin 2016, reporter lab reported that the number of fact-checking websites went up by 50%3\nhowever, fake news is still wildly disseminated on social media even when it has been debunked [36, 58]\na recent report [25] showed that 86% of american adults do not fact-check articles they read\na possible explanation for this is that people may trust content shared from their friends rather than other sources [25] or they may not have time to fact-check articles they read, or simply they may not know the existence of these fact-check websites\nit means that merely debunking fake news is not enough, and these systems are not fully utilized\nfurthermore, it has been shown that once absorbing misinformation from fake news, individuals are less likely to change their beliefs even when the fake news are debunked\nif the idea in the original fake news is especially similar to individuals\u2019 viewpoints, it will be even harder to change their minds [12, 40]\ntherefore, it is needed to deliver verified information quickly to online users before fake news reaches them\n   to achieve this aim, the volume of verified content should be large enough on social networks, so that online users may have a higher chance to be exposed to legitimate information before consuming fake news from other sources\nin this paper, we propose a framework to further utilize factchecked content\nparticularly, we collect a group of people and stimulate them to disseminate fact-checked content to other users\nhowever, achieving the goal is challenging because we have to solve the two following problems:\n(p1) how can we find a group of people (e.g. online users) who are willing to spread verified news?\n(p2) how can we stimulate them to disseminate fact-checked news/information\nto deal with the first problem (p1), we may deploy bots [27, 49] to disseminate information but it may violate terms of services of online platforms due to abusing behavior.\nanother approach is to hire crowd workers [29] and cyber troops to shape public opinion [5]\nhowever, this approach may cost a lot of money and is difficult to deploy in larger scale due to monetary constraints\ninspired by [18], we propose to rely on online users called guardians, who show interests in correcting false claims and fake news in online discussions by embedding fact-checking urls\nfigure 1 illustrates who a guardian is and helps us to describe terminologies that we use in this paper\nin the figure, two twitter users have a conversation, in which a user @sir_mycroft accused the clinton foundation of accepting money from uranium one company in exchange for the approval of the deal between uranium one and russian government in 2009\nafter just 15 minutes, this false accusation was debunked by a user @politics_pr, who referred to factcheck.org and snopes.com urls as evidences to support his factual correction\nwe call such direct replies, which contain factchecking urls, direct fact-checking tweets (d-tweets)\nusers, who posted d-tweets, are called direct guardians (d-guardians)\nthe user, to whom the d-guardian replied (i.e. @sir_mycroft), is called an original poster\nin addition, we observed that @politics_pr\u2019s response was retweeted 15 times\nwe call these retweeters secondary guardians (s-guardians), regardless of whether they added a comment or not inside the retweet\ntheir shares are called secondary tweets (s-tweets)\nboth d-guardians and s-guardians are called guardians, and both d-tweets and s-tweets are called fact-checking tweets.\nin section 4, we investigate whether both d-guardians and s-guardians play an important role in correcting claims and spreading fact-checked information\nto cope with the second problem (p2), we may directly ask the guardians to spread verified news like [28], but their response rate may be low because each guardian may be interested in different topics, and eventually, we may send unwanted requests to some of the guardians\nthus, we tackle the second problem by proposing a fact-checking url recommendation model\nby providing personalized recommendations, we may stimulate guardians\u2019 engagement in fact-checking activities toward spreading credible information to many other users and reducing the negative effects of fake news\nby addressing these two problems, we collect a large number of reliable guardians and propose a fact-checking url recommendation model which exploits recent success in embedding techniques [32] and utilizes auxiliary data to personalize fact-checking urls for the guardians.\nour main contributions are as follows\nwe are the first work to utilize guardians, who can help spread credible information and recommend fact-checking urls to the guardians as a pro-active way to combat fake news\nwe thoroughly analyze who guardians are, their temporal behavior, and topical interests\nwe propose a novel url recommendation model, which exploits fact-checking urls\u2019 content (i.e., linked fact-checking pages), social network structure, and recent tweets\u2019 content\nwe evaluate our proposed model against four state-of-the-art recommendation algorithms\nexperimental results show that our model outperforms the competing models by 11%~33", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "A large body of research work and efforts have been focused on detecting fake news and building online fact-check systems in order to debunk fake news as soon as possible\nDespite the existence of these systems, fake news is still wildly shared by online users.\nIt indicates that these systems may not be fully utilized\nAfter detecting fake news, what is the next step to stop people from sharing it? How can we improve the utilization of these fact-check systems\nTo fill this gap, in this paper, we (i) collect and analyze online users called guardians, who correct misinformation and fake news in online discussions by referring fact-checking URLs; and (ii) propose a novel fact-checking URL recommendation model to encourage the guardians to engage more in fact-checking activities\nWe found that the guardians usually took less than one day to reply to claims in online conversations and took another day to spread verified information to hundreds of millions of followers\nOur proposed recommendation model outperformed four state-of-the-art models by 11%\u223c33%.\n Our source code and dataset are available at http://web.cs.wpi.edu/~kmlee/data/gau.htm"},
{"doc": "with the increasing enthusiasm of users to share their daily life on social networks, a large amount of personal data, such as personal demographics, daily activities and even relations with the others, are made publicly available.\nit is reported that 66% of users\u2019 micro-posts are about themselves [24]\nthe huge amount of users\u2019 personal data accessible online may put the users at a high risk of privacy leakage due to the following reasons\non one hand, the default privacy settings usually make ugcs publicly accessible\nin fact, people are usually connected with heterogeneous circles on social networks, such as family members, casual friends and even strangers\nas a result, ugcs are probably seen by unexpected audience and hence cause unexpected consequences to users\ntake a real story as an example\na video podcaster\u2019s home was broken into and several video equipments were stolen during his travel\nit is ultimately found out that the break-in was caused by his detailed tweets regarding his leave [24]\non the other hand, users may even be unaware of the privacy leakage when they are posting on social networks, which is also the cause of the regrettable messages [36]\nconsequently, privacy leakage via user-generated contents (ugcs) in social networks deserves our special attention\nin fact, according to the report [35], 50% of internet users are concerned about the privacy exposure, up from about 30% in 2009\nprivacy is elaborated as a process of boundary regulation [13, 34], where individuals control over how much information about themselves can be divulged to others\ntherefore, maintaining appropriate levels of disclosure within one\u2019s social environment is of essential significance\nin fact, one\u2019s social circle can be organized into different groups based on their personal ties with the given user\nit is apparent that for different social circles, individuals hold different norms of what kind of information should be treated as privacy\nfor example, one\u2019s age may be kept private to his/her casual friends but visible to family members, while one\u2019s negative emotion may be better invisible to family members\nconsidering that information and audience both play pivotal roles in the privacy preserving, answering the question of who can see what is essential\nhowever, answering who can see what is non-trivial due to the following reasons\nfirstly, the personal aspects of users conveyed by their posts are usually not independent but can be organized into certain structures, such as groups, according to their relatedness\nfor example, given a set of aspects i = {age, current location, places planning to go}, aspects \u201ccurrent location\u201d and \u201cplaces to go\u201d are more correlated and should be modeled together in one group\nmore often than not, such structure can impose certain constraints to the feature space and enhance the performance of aspect detection\nconsequently, the main challenge is how to construct and leverage such structure to learn shared features and specific features\nsecond, thus far, no gold standard instruction is available to guide who can see what\nas the interpretation of privacy may be subjective and geographically specific, obtaining a unified instruction poses a crucial challenge for us\nthe third challenge lies in the lack of benchmark dataset and the way to extract a set of privacy-oriented features\nthis is because it is hard to distinguish the personal posts from the non-personal posts, and some posts are too short to provide sufficient contexts for feature extraction\nto address the aforementioned challenges, we present a novel scheme for boundary regulation, comprising of three components: description, prediction, and prescription\nas illustrated in figure 1, in the first component, we summarize the literature and pre-define a comprehensive taxonomy composed of 32 categories, where each category corresponds to one personal aspect of users\nto build a benchmark dataset, we then feed a list of keywords to twitter search service for each category\na set of privacy-oriented features, including linguistic and meta features are extracted to describe the given ugcs\nwe choose the real-time sharing website twitter as the study platform due to the following facts\n1) users in twitter are keen to share their personal events of various topics;\nand 2) the followers are broadly and disorderly mixed.\nbased on these features, the second component then endeavors to discover which personal aspect has been uncovered by the given post\nthe predefined structure in the first component has organized the 32 categories into eight groups, spanning from personal attributes to life milestones\nthe categories within each group hold both group-sharing features and aspect-specific features\nmeanwhile, we assume that there is a low dimensional latent feature space that is capable of capturing the higher-level semantics of ugcs as compared to the original features\nto learn the latent feature space and further boost the aspect detection performance, we treat each personal aspect as a task and propose a latent group multi-task learning (token) model that is able to leverage the pre-defined structure to learn group-sharing latent features and aspect-specific latent features simultaneously\nthe last component works towards triggering and suggesting users what they should act according to certain guidelines once their privacy leakage is detected by the second component\nconsidering the existence of cultural difference regarding users\u2019 information disclosure norms, we build guidelines by conducting a cross-cultural user study via amazon mechanical turk (amt)\nin designing this guideline, we regulate the boundary of users\u2019 posts by four tier social circles, namely, family members, close friends, casual friends and outsider audience\nour main contributions can be summarized in threefold\nwe established a taxonomy to comprehensively characterize users\u2019 personal aspects\nguided by this taxonomy, we proposed a token model to uncover the personal aspects disclosed by the user\u2019s posts\nregarding the optimization, we theoretically relaxed the non-smooth model to a smooth one and derived its closed-form solution\nwe constructed guidelines regarding users\u2019 information disclosure norms with four kinds of social circles\nthis user study with 400 users cannot be finished without the help of the crowdsourcing internet marketplace\u2014amt\nin addition, we studied the cultural similarities and differences of users\u2019 privacy perception\nwe collected a representative dataset via twitter search service and developed a rich set of privacy-oriented features\nwe have released the data to facilitate others to repeat experiments and verify their ideas\nthe remainder of this paper is structured as follows, section 2 briefly reviews the related work\nsections 3, 4 and 5 present the three components of token model, namely, description, prediction and prescription, respectively\nsection 6 details the experimental results and analyses, followed by our concluding remarks and future work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The booming of social networks has given rise to a large volume of user-generated contents (UGCs), most of which are free and publicly available\nA lot of users\u2019 personal aspects can be extracted from these UGCs to facilitate personalized applications as validated by many previous studies\nDespite their value, UGCs can place users at high privacy risks, which thus far remains largely untapped\nPrivacy is defined as the individual\u2019s ability to control what information is disclosed, to whom, when and under what circumstances\nAs people and information both play significant roles, privacy has been elaborated as a boundary regulation process, where individuals regulate interaction with others by altering the openness degree of themselves to others\nIn this paper, we aim to reduce users\u2019 privacy risks on social networks by answering the question of Who Can See What\nTowards this goal, we present a novel scheme, comprising of descriptive, predictive and prescriptive components\nIn particular, we first collect a set of posts and extract a group of privacy-oriented features to describe the posts\nWe then propose a novel taxonomy-guided multi-task learning model to predict which personal aspects are uncovered by the posts\nLastly, we construct standard guidelines by the user study with 400 users to regularize users\u2019 actions for preventing their privacy leakage\nExtensive experiments on a real-world dataset well verified our schem"},
{"doc": "motivation and problem\nrankings of subjects like people, hotels, or songs are at the heart of selection, matchmaking and recommender systems\nsuch systems are in use on a variety of platforms that affect different aspects of life \u2013 from entertainment and dating all the way to employment and income\nnotable examples of platforms with a tangible impact on people\u2019s livelihood include two sided sharing economy websites, such as airbnb or uber, or human resource matchmaking platforms, such as linkedin or taskrabbit. \nthe ongoing migration to online markets and the growing dependence of many users on these platforms in securing an income have spurred investigations into the issues of bias, discrimination and fairness in the platforms\u2019 mechanisms. [5, 26]\none aspect in particular has evaded scrutiny thus far \u2013 to be successful on these platforms, ranked subjects need to gain the attention of searchers.\nsince exposure on the platform is a prerequisite for attention, subjects have a strong desire to be highly ranked\nhowever, when inspecting ranked results, searchers are susceptible to position bias, which makes them pay most of their attention to the top-ranked subjects\nas a result, lower-ranked subjects often receive disproportionately less attention than they deserve according to the ranking relevance.\nposition bias has been studied in information retrieval in scenarios where subjects are documents such as web pages (e.g., [8, 10]).\nit has been shown that top-ranked documents receive most clicks often irrespective of their actual relevance [21]. \nsystemic correction for the bias becomes important when ranking positions potentially translate to financial gains or losses\n this is the case when ranking people on platforms like linkedin or uber, products on platforms like amazon, or creative works on platforms like spotify\nfor example, cumulating the exposure on a subset of drivers in ride-hailing platforms might lead to economic starvation of others, while low-ranked artists on music platforms might not get their deserved chance of earning royalties. \nobserving that attention is influenced by a human perception bias, while relevance is not, uncovers a fundamental problem: there necessarily exists a discrepancy between the attention that subjects receive at their respective ranks and their relevance in a given search task\nfor example, attention could decrease geometrically, whereas relevance scores may decrease linearly as the rank decreases\nif a ranking is displayed unchanged to many searchers over time, the lower-ranked subjects might be systematically and repeatedly disadvantaged in terms of the attention they receive\nproblem statement\na vast body of ranking models literature has focused on aligning system relevance scores with the true relevance of ranked subjects, and in this paper we assume the two are proportional.\nwhat we focus on instead is the relation between relevance and attention\nsince relevance can be thought of as a proxy for worthiness in the context of a given search task, the attention a subject receives from searchers should ideally be proportional to her relevance\nin economics and psychology, a similar idea of proportionality exists under the name of equity [31] and is employed as a fairness principle in the context of distributive justice [17]\n     thus, in this paper, we make a translational normative claim and argue for equity of attention in rankings\noperationally, the problem we address in this paper is to devise measures and mechanism which ensure that, for all subjects in the system, the received attention approximately equals the deserved attention, while preserving ranking quality\n for a single ranking this goal is infeasible, since attention is influenced by the position bias, while relevance is not\ntherefore, our approach looks at a series of rankings and aims at measures of amortized fairness. \nstate of the art and limitations\n fairness has become a major concern for decision-making systems based on machine learning methods (see, e.g., [9, 29])\nvarious notions of group fairness have been investigated [14, 20, 23, 28, 35], with the goal of making sure that protected attributes such as gender or race do not influence algorithmic decisions.\nfair classifiers are then trained to maximize accuracy subject to group fairness constraints\nthese approaches, however, do not distinguish between different subjects from within a group.\nthe notion of individual fairness [12, 24, 37] aims at treating each individual fairly by requiring that subjects who are similar to each other receive similar decision outcomes\n  for instance, the concept of meritocratic fairness requires that less qualified candidates are almost never preferred over more qualified ones when selecting candidates from a set of diverse populations\nrelevance based rankings, where more relevant subjects are ranked higher than less relevant ones, also satisfy meritocratic fairness\na stronger fairness concept, however, is needed for rankings to be a means of distributive justice. \n     prior work on fair rankings is scarce and includes approaches that perturb results to guarantee various types of group fairness\nthis goal is achieved by techniques similar to those for ranking result diversification [6, 34, 36], or by granting equal ranking exposure to groups [30]\n individual fairness is inherently beyond the scope of group-based perturbation\napproach and contribution\nour approach in this paper differs from the prior work in two major ways.\nfirst, the measures introduced here capture fairness at the level of individual subjects, and subsume group fairness as a special case\nsecond, as no single ranking can guarantee fair attention to every subject, we devise a novel mechanism that ensures amortized fairness, where attention is fairly distributed across a series of rankings. \nfor an intuitive example, consider a ranking where all the relevance scores are almost the same\nsuch tiny differences in relevance will push subjects apart in the display of the results, leading to a considerable difference in the attention received from searchers\nto compensate for the position bias, we can reorder the subjects in consecutive rankings so that everyone who is highly relevant is displayed at the top every now and then\nour goal is not just to balance attention, but to keep it proportional to relevance for all subjects while preserving ranking quality\nto this end, we permute subjects in each ranking so as to improve fairness subject to constraints on quality loss\n we cast this approach to an online optimization problem, formalizing it as an integer linear program (ilp)\n we moreover devise filters to prune the combinatorial space of the ilp, which ensures that it can be solved in an online system.\nexperiments with synthetic and real-life data demonstrate the viability of our method\nthis paper makes the following novel contributions\nto the best of our knowledge, we are the first to formalize the problem of individual equity-of-attention fairness in rankings, and define measures that capture the discrepancy between the deserved and received attention. \nwe propose online mechanisms for fairly amortizing attention over time in consecutive rankings\nwe investigate the properties and behavior of the proposed mechanisms in experiments with synthetic and real-world da", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Rankings of people and items are at the heart of selection-making, match-making, and recommender systems, ranging from employment sites to sharing economy platforms\nAs ranking positions influence the amount of attention the ranked subjects receive, biases in rankings can lead to unfair distribution of opportunities and resources such as jobs or income. \nThis paper proposes new measures and mechanisms to quantify and mitigate unfairness from a bias inherent to all rankings, namely, the position bias which leads to disproportionately less attention being paid to low-ranked subjects\nOur approach differs from recent fair ranking approaches in two important ways.\nFirst, existing works measure unfairness at the level of subject groups while our measures capture unfairness at the level of individual subjects, and as such subsume group unfairness\nSecond, as no single ranking can achieve individual attention fairness, we propose a novel mechanism that achieves amortized fairness, where attention accumulated across a series of rankings is proportional to accumulated relevance\nWe formulate the challenge of achieving amortized individual fairness subject to constraints on ranking quality as an online optimization problem and show that it can be solved as an integer linear program\nOur experimental evaluation reveals that unfair attention distribution in rankings can be substantial, and demonstrates that our method can improve individual fairness while retaining high ranking qualit"},
{"doc": "the use of ir methodologies and metrics for the evaluation of recommender systems has spread in recent years and is becoming common practice in the area, under the understanding of recommendation as a ranking task [14]\nyet ir metrics have been found to be strongly biased towards rewarding algorithms that recommend popular items, that is, items that many people know, like, rate or interact with [4,21,35]\nat the same time, state of the art recommendation algorithms have similarly been found to display a marked bias towards recommending items most people like [21]\nthis may naturally cast doubt on the reliability of common experiments and the outcome on which the best algorithms really are\nthis problem has been of no particular concern to ir methodology, as popularity biases do not occur, or not in such a dramatic way, in traditional search and ir tasks\nthe popularity bias is so strong in common datasets for recommender system evaluation that even a pure and simple popularity ranking appears to achieve suboptimal but non-negligible recommendation accuracy compared to the best state of the art personalized algorithms [14]\nand it is in fact not necessarily trivial to outperform, for instance, in high rating sparsity conditions\nresearch has therefore been recently undertaken addressing the issue, so far mainly focusing on confirming and measuring the popularity biases, and removing them [4,21,34,35].\nbut a basic question remains yet unanswered: is the popularity bias actually something we should get rid of at all?\nif recommending popular items happened to be the right thing to do, then should not both the evaluation metrics and the recommendation algorithms rightfully favor them? \nthe majority opinion is indeed useful information for people \u2013it is a simple yet fair and useful default criterion we keep in sight most of the time through our human decisions, even when we do not follow it\nand we in fact often do adopt it, for instance, in the absence of enough evidence to form one\u2019s own personal choice, or as guidance to reduce the cost of building a decision from scratch, or as a social learning mechanism [3]\nfrom an application point of view, a recommendation based on the choices of many can be acceptable in many circumstances [16] \u2013and requires minimum development skills and maintenance costs\nit is actually a widespread approach that many applications display in the form of top charts, best-selling lists, average people\u2019s ratings, etc\neven in the presence of a full-fledged personalized recommender system, majority listings are still a good resort for new or cold users\nthe effectiveness of majority taste makes indeed statistical sense: the items that many people like (according to the records of observed user activity) are liked by many people (in test data for evaluation) [19]\nyet from an experimental perspective, if the observations are somehow biased, and the bias is consistent across training to test data, the majority bias in recommendation might be accurately guessing where the observations have been placed by the experimenter, rather than where true user tastes are being actually most satisfied\nmoreover, the majority signal might be contaminated by trends that deviate from actual user appreciation [5,29]\nrecent studies show that majority formation involves a degree of chance, by which different outcomes are possible as to what choices make it to the top of popularity [31]\ncrowd dynamics are moreover known to be exposed to external and internal influence and bias factors [26,27,29], such as mass media [7], marketing, opinion management [6], algorithmic bias [28], or social conformity [13]\nthe issue is therefore open whether or not popularity is a truly effective ingredient to achieve accurate recommendations, to what extent and in what cases, and whether we are measuring it properly\nwe address the question by considering, analyzing and comparing two views on ir metrics: biased and unbiased\nthe former represents what is measured in common offline experiments in the literature, where relevance information is missing not at random (mnar) [23,24,25,34,35], and the latter represents the true metric value that would be obtained if the missing information became available. \nwe do this at both a theoretical and an empirical level. at the analytical level, we formulate a probabilistic expression of the problem\nstarting by a revised probability ranking principle [30] for recommender systems, we analyze popularity-based recommendation by comparison to the optimal ranking\nwe find that the effectiveness or ineffectiveness of popularity depends on the interplay of three main variables: item relevance, item discovery by users, and the decision by users to interact with discovered items\nwe identify the key probabilistic dependencies among these factors that determine the outcome for popularity, and we characterize a set of trends defined by different independence assumptions, each resulting in a particular pattern of behavior for popularity\nwe back our theoretical findings with empirical observations with a dataset we build on a crowdsourcing platform, in which we remove several of the common biases of publicly available datasets\namong other findings, we prove and illustrate qualitative contradictions between the accuracy that is measured in a common offline experimental setting, and the actual accuracy that can be estimated with unbiased observations\nwe identify conditions that guarantee popularity to be a safe element in recommendation, and we characterize and exemplify situations where, on the contrary, popularity can be a totally misleading direction to follow, to the point of leading to worse effectiveness than random recommendation\n       we furthermore find that the average rating can be more effective than the number of ratings as a trend to follow in recommendation in many cases, contrarily to what the biased metric values suggest \u2013which represent what the literature commonly reports [14,21].\nfinally, we look at the signification our findings can have in personalized collaborative filtering algorithms. ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The use of IR methodology in the evaluation of recommender systems has become common practice in recent years\nIR metrics have been found however to be strongly biased towards rewarding algorithms that recommend popular items \u2013the same bias that state of the art recommendation algorithms display\nRecent research has confirmed and measured such biases, and proposed methods to avoid them\nThe fundamental question remains open though whether popularity is really a bias we should avoid or not; whether it could be a useful and reliable signal in recommendation, or it may be unfairly rewarded by the experimental biases\nWe address this question at a formal level by identifying and modeling the conditions that can determine the answer, in terms of dependencies between key random variables, involving item rating, discovery and relevance\nWe find conditions that guarantee popularity to be effective or quite the opposite, and for the measured metric values to reflect a true effectiveness, or qualitatively deviate from it\nWe exemplify and confirm the theoretical findings with empirical results.\n  We build a crowdsourced dataset devoid of the usual biases displayed by common publicly available data, in which we illustrate contradictions between the accuracy that would be measured in a common biased offline experimental setting, and the actual accuracy that can be measured with unbiased observations"},
{"doc": "search tasks are a central component of interactive information retrieval (iir)\nas noted by toms [21], search tasks play two important roles in iir research\nfirst, they serve as a vehicle for research\nin iir studies, experimenters must assign search tasks to study participants in order to observe their behaviors and evaluate systems\nsecond, search tasks are also often used as the object of study (i.e., as independent variables)\nfrom this perspective, the study of search tasks helps us understand how task characteristics translate to specific challenges faced by searchers, and informs the design of novel tools to support users\na large body of research has focused on understanding how search tasks vary along different dimensions, including the search task\u2019s main activity (e.g., searching vs. browsing), goal (e.g., well defined vs. amorphous), and structure (e.g., task complexity) [16]\nsearch task complexity is one characteristic that has received considerable attention in recent work, and has been found to influence search behaviors and outcomes [3, 5, 7, 8, 11, 13, 25]\ntask complexity is itself a complicated construct that has been studied from different perspectives [24]\none influential approach initially proposed by bystr\u00f6m and j\u00e4rvelin [5] is to view task complexity through the lens of a priori determinability.\nthe a priori determinability of a task is defined by the degree of uncertainty about the task\u2019s required outcomes and the processes involved in gathering the information  needed to complete the task\n a task with low determinability (i.e., high complexity) is one with high uncertainty about the form of the solution and the processes involved in solving the task\nin this work, we aimed to manipulate the determinability of search tasks indirectly, by manipulating the scope of the task (i.e., open-ended versus narrowly focused)\nin order to control for other task characteristics, we focused on comparative search tasks\nfor example, one of our tasks asked participants to compare different fertilizers for a home garden\ncomparative tasks involve two important activities: (1) identifying different items or alternatives for the given category (e.g., organic, synthetic, liquid fertilizers) and (2) understanding how the items differ along different dimensions or attributes (e.g., cost, nutrient content, health concerns)\nwe manipulated the task scope by including or excluding specific items and/or dimensions for participants to consider as part of the comparative task\nour most open-ended tasks did not mention specific items or dimensions to consider\nin contrast, our most narrowly focused tasks instructed participants to consider two specific items and one dimension\nadditionally, we studied two types of dimensions: objective and subjective\nwe expected that addressing a subjective dimension would involve greater uncertainty (i.e., lower determinability)\nfor example, a subjective dimension might require gathering information from different perspectives and evaluating the credibility of information. \nwe report on a crowdsourced study (n = 144) that investigated the effects of our task manipulation on participants\u2019 perceptions about the task, search behaviors and strategies, and level of engagement during the task\nwe developed 12 task topics and 6 task versions per topic\ntask version was our main independent variable and varied based on the specification of items and/or (objective or subjective) dimensions that should be considered during the comparative search task\nwe used a within-subject design; each participant completed six search tasks (one per task version)\nour study investigates the following five research questions: \nrq1 rq2: what is the effect of task version on participants\u2019 pre-(rq1) and post-task (rq2) perceptions about the task\nwe focus on perceptions related to determinability, subjectivity, prior knowledge/knowledge increase, interest/interest increase, and expected/ experienced difficulty\nrq3: what is the effect of task version on participants\u2019 level of engagement during the task?\nwe measured aspects of engagement using o\u2019brien\u2019s user engagement scale [18]\nrq4: what is the effect of task version on participants\u2019 search behaviors\nwe examined measures associated with search effort and the extent to which participants diverged from each other in their choice of queries and clicks\nrq5: what is the effect of task version on participants\u2019 search strategies\nthrough a qualitative analysis of participants\u2019 queries, we investigated the differences in querying strategies observed for different task versions\nwe build on our previous work to investigate the relationships between task scope, determinability, and searchers\u2019 perceptions and behaviors [8].\nin this new study, we investigate differences between specifying objective and subjective dimensions in the task description, explore the effects of our task manipulation on user engagement, and present an analysis of participants\u2019 queries to explain why or how certain task versions affected task performanc", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": " An important area of IR research involves understanding how task characteristics influence search behaviors and outcomes.\nTask complexity is one characteristic that has received considerable attention\nOne view of task complexity is through the lens of a priori determinability\u2014the level of uncertainty about task outcomes and processes experienced by the searcher\nIn this work, we manipulated the determinability of comparative tasks\nOur task manipulation involved modifying the scope of the task by specifying exact items and/or exact (objective or subjective) dimensions to consider as part of the task\nThis paper reports on a within-subject study (N = 144) where we investigated how our task manipulation influenced participants\u2019 perceptions, levels of engagement, search effort, and choice of search strategies\n Our results suggest a complex relationship between task scope, determinability, and different outcome measures\nOur most open-ended tasks were perceived to have low determinability (high uncertainty), but were the least challenging for participants due to satisficing\nFurthermore, narrowing the scope of tasks by specifying items had a different effect than by specifying dimensions\nSpecifying items increased the task determinability (lower uncertainty) and made the task easier, while specifying dimensions did not increase the task determinability and made the task more challenging\n A qualitative analysis of participants\u2019 queries suggests that searching for dimensions is more challenging than for items\nFinally, we observed subtle differences between objective and subjective dimensions\n We discuss implications for the design of IIR studies and tools to support user"},
{"doc": "crimes classification over the rigorously defined legal articles is a tedious job in the juridical field\njudges usually need to consult several relevant cases to determine the specific legal articles that an evidence violated, which is time consuming and needs extensive professional knowledge\ntable 1 shows an example of an evidence in a legal case, as well as the legal article that the evidence violated\ngenerally, the task can be cast as a multi-label classification problem to enhance working efficiency and to save manual efforts\nin this work, we denote the multi-label classification problem from evidences to articles as the crimes classification task, which helps the judge to pinpoint potential articles quickly and accurately\nhowever, this problem is a difficult task and we may face two key challenges in practice\none is that the number of articles violated by different evidences is dynamic [10, 32, 42], i.e., the label dynamic problem\nthrough our analysis on a large scale real-world referee document dataset where 70 articles are considered, the article set size over evidences variants significantly, as shown in figure 1\nthe other challenge is the (class) label imbalance problem [3, 5, 34]\na multi-label classification dataset is regarded as imbalanced if some of its (minority) labels in the training set are heavily underpresented compared to other majority labels\nstatistics over the same dataset is shown in figure 2\nas we can see, the number of violated evidences for each article (label) follows a long-tailed distribution, which means that many articles are seldom violated by evidences\nmost traditional multi-label classification algorithms try to minimize the overall classification error during the training process, which implicitly assumes equivalent importance over all labels.\nthe skewed distribution of class labels makes classification algorithms under this equivalent assumption biased towards the majority class labels\nthough article definition can indicate some relations among different articles to alleviate the label imbalance problem (as shown in table 1, the definition of article 22 is similar to article 25), none of work has considered this information in crimes classification\nthe difficulty in crimes classification thus raises an interesting research question\ngiven a set of evidences and article definitions, can we classify the evidence automatically\nalthough recent studies suggest that multi-label classification is increasingly required in many applications, such as protein gene classification [2], music categorization [31], and semantic scene classification [22].\nto the best of our knowledge, no practice have been conducted on crimes classification in juridical scenarios. \nprevious work on multi-label classification usually exploits the label correlations, such as bp-mll [40], kernel method [10], and calibrated label ranking [6], etc\nhowever, all these methods learn the multi-label classification model and label threshold independently, and the label imbalance problem is largely ignored\nto tackle with the first problem, we propose a multi-task framework to learn the multi-label classification model and the threshold predictor jointly\nwhile for the second problem, we adopt the label descriptions to model the pairwise relations between labels, and extend the exact label set to a soft attention matrix over all the possible labels, which will alleviate the label imbalance problem as shown in our experiments\nin this paper we propose a unified model named dynamic pairwise attention model (dpam for short) for crimes classification.\nspecifically, we embed each evidence and article definition using the bag-of-word representations, and enumerate each article set into a pairwise label set, so that we can learn the pairwise label coveragebased classifiers from the transformed dataset\nbesides, a label attention matrix is constructed based on the article definitions to alleviate the label imbalance problem\nwe then design a regression model to learn a multi-label threshold predictor for each label automatically\nfinally, a multi-task framework is designed to learn the two tasks jointly thus to improve the generalization performance by leveraging the information contained in related tasks\noverall, the major contributions of our work are as follows\nwe make the first attempt to investigate the prediction power of evidences and article definitions for crimes classification in juridical scenario\nwe design a multi-task learning paradigm to learn multilabel classifier and threshold predictor jointly, thus dpam can improve the generalization performance by leveraging the information contained in related tasks\na pairwise attention model based on article definitions is incorporated to the classification model to alleviate the label imbalance problem\nwe conduct extensive experiments on two real-world datasets to verify the effectiveness of the proposed dpam model as compared with different baseline methods\n the rest of the paper is organized as follows.\nafter a summary of related work in section 2, we describe the problem formalization of crimes classification in juridical scenario and our proposed model in section 3\nwe provide experiments and evaluations in section 4\nsection 5 concludes this paper and discusses future direction", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "In juridical field, judges usually need to consult several relevant cases to determine the specific articles that the evidence violated, which is a task that is time consuming and needs extensive professional knowledge\nIn this paper, we focus on how to save the manual efforts and make the conviction process more efficient\n Specifically, we treat the evidences as documents, and articles as labels, thus the conviction process can be cast as a multi-label classification problem\nHowever, the challenge in this specific scenario lies in two aspects.\n One is that the number of articles that evidences violated is dynamic, which we denote as the label dynamic problem\n The other is that most articles are violated by only a few of the evidences, which we denote as the label imbalance problem\nPrevious methods usually learn the multi-label classification model and the label thresholds independently, and may ignore the label imbalance problem\nTo tackle with both challenges, we propose a unified Dynamic Pairwise Attention Model (DPAM for short) in this paper\nSpecifically, DPAM adopts the multi-task learning paradigm to learn the multi-label classifier and the threshold predictor jointly, and thus DPAM can improve the generalization performance by leveraging the information learned in both of the two tasks\n In addition, a pairwise attention model based on article definitions is incorporated into the classification model to help alleviate the label imbalance problem\n Experimental results on two real-world datasets show that our proposed approach significantly outperforms state-of-the-art multi-label classification method"},
{"doc": "with the rapid development of web techniques, recommender systems (rs) play a more and more important role in matching user needs with rich resources (called items) from various online platforms\nfor building an effective recommender system, a key factor is able to accurately characterize and understand users\u2019 interests and tastes, which are intrinsically dynamic and evolving\nto achieve this goal, the task of sequential recommendation has been proposed to better satisfy sequential user needs [26], which aims to predict the successive item(s) that a user is likely to interact with given her past interaction records\ntraditional recommendation methods (e.g., standard mf [17]) can\u2019t well solve the sequential recommendation task, since they usually model static user-item interactions\nfor capturing sequential patterns, the classic fpmc model [26] has been proposed to factorize user-specific transition matrix by considering the markov chain\na major problem of fpmc is that it still adopts the static representation for user preference.\nwith the revival of neural networks, many studies try to adapt powerful sequential neural models, i.e., recurrent neural networks (rnn), to sequential recommendation [35], including session-based rnn [15], user-based rnn [5] and attention-based rnn [18]\nrnn-based models have been shown effective to improve the performance of sequential recommendation [15]\nby encoding historical interaction records into a hidden state vector (called sequential preference representation), it is possible for these methods to capture dynamic user preference over time and measure the likelihood of the next item\nalthough the state vector is able to encode sequential dependency, it has limited representation power in capturing complicated user preference\nsince the state vector is encoded in a highly abstractive way, it is difficult to capture or recover fine-grained (e.g., attribute or feature level) user preference from the interaction sequence\nfurthermore, the latent vector representation is usually hard to understand and explain\nin recommender systems, interpretability is a very important factor to consider [14, 30]\n these issues make it challenging to develop an effective and interpretable sequential recommender. \nto enhance the capacity of modeling fine-grained user preference in an interpretable way, our idea is to incorporate external knowledge into the sequential recommender\nthe incorporated knowledge should be rich and flexible to characterize varying context information in different domains\na key problem is what kind of knowledge we can use and how we represent it\nin this paper, we propose to link items in recommender systems with existing knowledge base (kb) entities, and leverage structured entity information for improving sequential recommendation\nkbs store knowledge in triples of the form (head entity, relation, tail entity), typically corresponding to attribute information of entities\nkbs provide a general way to flexibly characterize context information of entities from various domains\nto obtain a compact representation for kb information, we adopt the kb embedding approach (i.e., transe [1]) to mapping entities and relations into low-dimensional vectors, called kb embeddings\nthe major difficulty in designing the knowledge-enhanced sequential recommender is rnn-based models usually have limited short-term memories [3], which are not suitable to store external knowledge (e.g., kb information) for long-term usage\ninspired by recent progress on improving the memory mechanism of neural networks [19, 21, 34], we propose to augment the rnn-based sequential recommender with external memories\nby explicitly setting up an external memory of storage slots, memory networks (mn) manipulate the memory according to the received data signal with a set of predefined operations, e.g., read and write\nit has been shown that mns are effective in memorizing long-term data characteristics [3], which can even evolve and update over time\n we use kb information as external knowledge\n considering the structural organization of entity information in kbs, we propose to incorporate kb knowledge via key-value memory networks (kv-mn). \n kv-mns [21] decompose each memory slot into a key vector and a value vector\na nice merit of kv-mn is that we can associate a key vector with a value vector, which supports associative search and read.\nwith kv-mns, we set a key vector to a relation embedding learned from kb data, corresponding to an entity attribute.\nfurthermore, given a key vector, we set up a user-specific value vector storing the preference characteristics of a user for the corresponding attribute.\nin this way, external kb knowledge is effectively incorporated into the kv-mns\n once the knowledge-enhanced kv-mns have been prepared, the next question is how to integrate it with rnn-based sequential recommender\ninstead of simply merging the output from both components, at each recommendation, we use the sequential preference representation from rnns as the query to read out the associated content of user-specific kv-mns, i.e., value vectors\nvalue vectors will be combined into an attributed-based preference representation with attentive weights derived from the sequential preference representation\nthe attributed-based preference representation together with sequential preference representation are combined as the final representation of user preference\nwe present the overview of the proposed model in fig. 1\nto summarize, in this paper, we propose a novel knowledge enhanced sequential recommender\nour model integrates rnn based networks (gru) with kv-mns\nrnn-based networks are good at capturing sequential user preference, while knowledge enhanced kv-mns are good at capturing attribute-based user preference\nby using a hybrid of rnns and kv-mns, it is expected to be endowed with the benefits of both components\ngiven a hidden sequential preference representation from rnns, our model is able to transform it into attentive weights over the key vectors corresponding to attributes, which provides attribute-level interpretability\n by setting user-specific value vectors, our model is able to learn the characteristics of user preference on some specific attribute, which further provides value-level interpretability.\nto our knowledge, it is the first time that sequential recommender is integrated with external memories by leveraging existing kb information\nfor evaluating our model, we prepare four rs datasets, and then link items of the four datasets with freebase entities\nextensive results on the four datasets have shown the superiority of the proposed model in both effectiveness and interpretabilit", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "With the revival of neural networks, many studies try to adapt powerful sequential neural models, i.e., Recurrent Neural Networks (RNN), to sequential recommendation\nRNN-based networks encode historical interaction records into a hidden state vector.\nAlthough the state vector is able to encode sequential dependency, it still has limited representation power in capturing complicated user preference\nIt is difficult to capture fine-grained user preference from the interaction sequence\nFurthermore, the latent vector representation is usually hard to understand and explain\nTo address these issues, in this paper, we propose a novel knowledge enhanced sequential recommender\nOur model integrates the RNN-based networks with Key-Value Memory Network (KV-MN)\nWe further incorporate knowledge base (KB) information to enhance the semantic representation of KV-MN. RNN-based models are good at capturing sequential user preference, while knowledge enhanced KV-MNs are good at capturing attribute-level user preference\nBy using a hybrid of RNNs and KV-MNs, it is expected to be endowed with both benefits from these two components\nThe sequential preference representation together with the attribute-level preference representation are combined as the final representation of user preference\n With the incorporation of KB information, our model is also highly interpretable\nTo our knowledge, it is the first time that sequential recommender is integrated with external memories by leveraging large-scale KB informatio"},
{"doc": "users in location-based social networks (lbsns), such as yelp and foursquare, can share their location with their friends by making checkins at venues (e.g. museums, restaurants and shops) they have visited, resulting in huge amounts of user check-in data.\neffective venue recommendation systems have become an essential application for lbsns that facilitate users finding interesting venues based on their historical checkins\ncollaborative filtering techniques such as matrix factorisation (mf) [17] are widely used to recommend a personalised ranked list of venues to the users\nmf-based approaches typically aim to embed the users\u2019 and venues\u2019 preferences within latent factors, which are combined with a dot product operator to estimate the user\u2019s preference for a given venue\napproaches on mf typically encapsulate contextual information about the user, which can help to make effective recommendations for users with few historical checkins, known as the cold-start problem [22, 30, 32]\nin recent years, various approaches have been proposed to leverage deep neural network (dnn) algorithms for recommendation systems [3, 10, 11, 21, 28, 31]\namong various dnn techniques, the recurrent neural network (rnn) models have been widely used to extend the mf-based approaches to capture users\u2019 short-term preferences from the users\u2019 sequence of observed feedback [1, 21, 26, 28, 31, 37]\nhere, the short-term (dynamic) preferences assume that the next venue visited by a user is influenced by his/her recently visited venues (e.g. users may prefer to visit a bar directly after dinner at a restaurant)\na common technique to incorporate rnn models (e.g. long short-term memory (lstm) units [13] and gated recurrent units (gru) [4]) into mf-based approaches is to feed a sequence of uservenue interactions/checkins into the recurrent models and use the hidden state of the recurrent models to represent the users\u2019 dynamic preferences [21, 28, 31, 35].\nnext, the user\u2019s preference of a target venue is estimated by calculating the dot product of this representation of the user\u2019s dynamic preferences (i.e. the output of the recurrent models) and a latent factor of the target venue\nalthough this technique can enhance the effectiveness of mf-based approaches, we argue that directly applying traditional rnn-based models to capture the users\u2019 dynamic preferences is not effective for context-aware venue recommendation (cavr)\nin particular, the traditional rnn models are limited as they can only take the sequential order of checkins into account and cannot incorporate the contextual information associated with the checkins (e.g. timestamp of a user\u2019s checkin and the geographical location of the checkin)\nindeed, such contexts have been shown to play an important role in producing effective cavr recommendations [6, 22, 30, 32]\nto address the above challenge, various approaches have been proposed to extend the rnn models to incorporate the contextual information of observed feedback into various recommendation settings excepting cavr [1, 14, 19, 23, 26, 29, 37].\nfor example, zhu et al. [37] proposed an extension of lstm (timelstm) by introducing time gates that control the influence of the hidden state of a previous lstm unit based on the time interval between successive observed feedbacks\nindeed, they assume that the shorter the time interval between two successive feedback, the stronger the correlation between these two feedbacks and vice versa\nhowever, their proposed model was designed for a particular type of contextual information (i.e. time intervals) and is not flexible to incorporate other types of context (e.g. distance between venues).\nwe argue that the time gates proposed by zhu et al. [37] are not effective to model the sequences of checkins in lbsns\nfigure 1 illustrates the user\u2019s sequential order of checkins.\nlet\u2019s consider the time intervals and distances between three successive checkins c_{\\\\tau - 1}, c_{\\\\tau} and c_{\\\\tau + 1}.\nwith zhu et al.\u2019s time gates, checkin  c_{\\\\tau - 1} (c_{\\\\tau}) will have a small impact on checkin c_{\\\\tau} (c_{\\\\tau} + 1) due to the long time interval between  c_{\\\\tau - 1} (c_{\\\\tau} ) and (c_{\\\\tau} ) c_{\\\\tau + 1}.\nthis is counter-intuitive since checkin (c_{\\\\tau}) may have a strong impact on checkin (c_{\\\\tau}) due to the geographical distance between them\nfor example, a user may decide to visit a museum near the restaurant they had dinner at the previous day.\nalthough the time interval from the previous checkin is long (> 24 hours), geographically, the restaurant and museum are close\nrecently, several works (e.g. cgru [26] and latentcross [1], for product and movie recommendation systems, respectively) have extended traditional rnn architectures for recommendation systems to incorporate different types of contextual information of the observed feedback sequences\nhowever, we argue that their proposed architectures are limited for context-aware venue recommendation in several respects\nin figure 1, we highlight two types of contextual information associated with sequences of checkins, namely: the ordinary and transition contexts\nthe ordinary context represents the (absolute) timestamp and the geographical position of the checkin, while the transition context represents the (relative) time interval and distance between successive checkins\na disadvantage of the aforementioned rnn architectures is that they rely on a quantised mapping procedure (i.e. to convert continuous values of time intervals and distances to discrete features and represent these transition contexts using low-dimensional embedding vectors), which may result in a loss of granularity\nin addition, their proposed architectures treat the ordinary and transition contexts dependently.\nhowever, we argue that these contexts influence the user\u2019s dynamic preference differently and should be considered independently\nindeed, the ordinary context reflects the user\u2019s contextual preference on a venue, while the transition context reflects the influence that one checkin has on its successor\nto address these challenges, we propose a contextual attention recurrent architecture (cara) that leverages the sequential of users\u2019 checkins to model the users\u2019 dynamic preferences.\nin particular, our contributions are summarised below\nwe propose a contextual attention recurrent architecture (cara) that independently incorporates different types of contextual information to model the users\u2019 dynamic preference for cavr\nour proposed recurrent architecture differs from the recently proposed cgru [26] and latentcross [1] architectures in three aspects\n(1) cara includes gating mechanisms that control the influence of the hidden states between recurrent units\n(2) cara supports both discrete and continuous inputs and\n(3) cara treats different types of context differently, in contrast, both cgru and latentcross do not support these features\nwithin the cara architecture, we propose two gating mechanisms: a contextual attention gate (cag) and a time- and spatial-based gate (tsg)\nthe cag controls the influence of context and previous visited venues, while tsg controls the influence of the hidden state of the previous rnn unit based on time interval and geographical distances between two successive checkins.\nnote that our proposed tsg differs from the time gates in timegru [37] as we can incorporate multiple types of context, whereas timegru supports only the time intervals\nto the best of our knowledge, this work is the first that incorporates geographical information into an rnn architecture for cavr\nwe conduct comprehensive experiments on 2 large-scale realworld datasets, from brightkite and foursquare, to demonstrate the effectiveness of our proposed cara architecture for cavr by comparing with state-of-the-art venue recommendation approaches\nthe experimental results demonstrate that cara consistently and significantly outperforms various existing strong rnn models\nthis paper is structured as follows\nsection 2 provides a background in the literature on cavr, as well as recent trends in applying deep neural networks to recommendation systems\nsection 3 details specific existing rnn-based recommendation architectures from the literature, and highlights 5 limitations in these approaches\nsection 4 details our proposed cara architecture that addresses all 5 limitations\nexperimental setup and results are provided in sections 5 6, respectively\nconcluding remarks follow in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Venue recommendation systems aim to effectively rank a list of interesting venues users should visit based on their historical feedback (e.g. checkins)\nSuch systems are increasingly deployed by Locationbased Social Networks (LBSNs) such as Foursquare and Yelp to enhance their usefulness to users\nRecently, various RNN architectures have been proposed to incorporate contextual information associated with the users\u2019 sequence of checkins (e.g. time of the day, location of venues) to effectively capture the users\u2019 dynamic preferences\nHowever, these architectures assume that different types of contexts have an identical impact on the users\u2019 preferences, which may not hold in practice\nFor example, an ordinary context \u2013 such as the time of the day \u2013 reflects the user\u2019s current contextual preferences, whereas a transition context \u2013 such as a time interval from their last visited venue \u2013 indicates a transition effect from past behaviour to future behaviour\nTo address these challenges, we propose a novel Contextual Attention Recurrent Architecture (CARA) that leverages both sequences of feedback and contextual information associated with the sequences to capture the users\u2019 dynamic preferences\n1) a contextual attention gate that controls the influence of the ordinary context on the users\u2019 contextual preferences and\n2) a time- and geo-based gate that controls the influence of the hidden state from the previous checkin based on the transition context\nThorough experiments on three large checkin and rating datasets from commercial LBSNs demonstrate the effectiveness of our proposed CARA architecture by significantly outperforming many state-of-the-art RNN architectures and factorisation approache"},
{"doc": "along with the rapid growth of knowledge graphs (kgs), billions of entities have been created from diverse sources\nentity resolution (er) aims to identify different entities referring to the same real world object\nwhile a significant number of automated approaches have been proposed to address the er problem, they are still being challenged by large-scale, heterogeneously-described entities\n for example, figure 1 depicts an er task with three real entities in dbpedia, freebase and wikidata, respectively\n each entity is described by hundreds of properties and values, and some of them are very similar, e.g., types, names and genders, which make automated er approaches difficult to decide whether all the three entities refer to the same person or not\nrecent studies have demonstrated great benefits of involving humans in the er loop\nfor instance, crowdsourcing, where human workers are recruited to solve micro-tasks like verifying the correctness of entity matches, offers a feasible solution to large-scale er [8, 25, 32, 34, 38]\n also in some cases, the human workers are actively engaged to improve the algorithms in automated er approaches with active learning [21, 27].\n although all these works request a human to judge which entities in a single er task refer to the same, little effort has been made on how to present the critical information (e.g., important properties and values) to help her complete the task more efficiently and accurately [4, 32]\ncurrent approaches\ngiven an er task to a human, there are two kinds of often-used approaches to present entities and their properties and values for the task.\nthe first kind uses pairwise presentation, which compares two entities at a time and aligns similar properties between them [4, 35]\nthe other kind displays multiple entities in a form of list [15, 19] (or grid, in particular for images [32]), just like what is typically seen from a web search engine\nbecause the lengthy and heterogeneous entity descriptions, such as hundreds of property-values in entities like e1, e2, e3 in figure 1, would overload a human with too much superfluous information, the two kinds of approaches both use pairwise or single entity summarization to help the human focus on a small portion of important properties and values to reduce her workload [4, 16, 17, 19, 31]\narguably, each of the two kinds of approaches has its strengths and weaknesses\nthe pairwise one allows humans to focus on two specific entity summaries at each time, but is challenging to scale due to the large number of entity pairs to be judged\nfor example, given a task containing 20 entities, a human may need to provide answers for up to \\\\binom{20}{2}= 190 entity pairs without using transitivity\non the other hand, the list-based needs humans to scroll among the entities and remember and compare their summaries in mind\nmore importantly, extra identity evidences observed from transitivity and grouping would be unseen if a set of entities is broken down to pairs or a list\nfigure 2 shows the result of a pairwise approach [4] for entities e_{1}, e_{2} in figure 1\nwithout seeing the other entity pairs, a human is very likely to decide that e_{1}, e_{2} refer to the same artist (which is wrong in this case!)\n so, it is natural to ask if there is any other solution to help human intervene in er\nour approach\nin this paper, we propose a table-based solution that presents an er task in a condensed, rather simple, and well structured fashion\nin fact, comparative table is considered suitable to present symbolic information and has long been used in decision-makings [33]\nfor instance, figure 3 depicts a comparative table for e_{1}, e_{2},e_{3} in figure 1\n  with the help of this table, a human would be confident of saying that e_{2},e_{3} refer to the same person, but e_{1} does not\n  given an mer task, we want to generate an optimal comparative table for this task\n to achieve this, we deal with several challenges\nfirstly, different entities often use various properties to convey the same meaning, and property heterogeneity causes the difficulty in comparison and the sparseness of the table\nwe use several similarity measures to find matched property pairs, and derive property cliques by maximizing the overall match probability estimate under a matching cardinality constraint\nsecondly, we study what factors contribute to the goodness of a property clique and a value, and design several scoring functions according to the intuitions related to how much information they provide and how important they are to humans.\n the challenge is how to measure the commonalities and differences among multiple entities\nthirdly, the goal of generating a comparative table is to help humans attain a quick comparison of the entities, and thus the comparative table must provide adequate information in a limited display space\ngiven the constraints on entity coverage and table cell size, we formulate the optimization problem of generating a comparative table with the best goodness and propose efficient algorithms\nthe main contributions of this paper are summarized as follows\nwe optimize the discovery of matched property cliques with the highest overall match probability estimate among those satisfying a global 1:1 matching constraint. (section 3\nwe propose scoring functions to measure the goodness of property cliques and values for a set of entities according to several intuitions. (section 4\nwe formulate the problem of optimal comparative table generation with the entity coverage constraint\nwe propose an efficient algorithm to obtain approximate solutions and prove its approximation ratio. (section 5\nwe conducted extensive experiments, comparison to state of-the-art methods and user study to verify the accuracy ofmatched properties, effectiveness of goodness measures and user satisfaction of comparative tables for mer. (section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Entity resolution (ER), the process of identifying entities that refer to the same real-world object, has long been studied in the knowledge graph (KG) community, among many others\nHumans, as a valuable source of background knowledge, are increasingly getting involved in this loop by crowdsourcing and active learning, where presenting condensed and easily-compared information is vital to help human intervene in an ER task\nHowever, current methods for single entity or pairwise summarization cannot well support humans to observe and compare multiple entities simultaneously, which impairs the efficiency and accuracy of human intervention. \nIn this paper, we propose an automated approach to select a few important properties and values for a set of entities, and assemble them by a comparative table\nWe formulate several optimization problems for generating an optimal comparative table according to intuitive goodness measures and various constraints.\n Our experiments on real-world datasets, comparison with related work and user study demonstrate the superior efficiency, precision and user satisfaction of our approach in multi-entity resolution (MER"},
{"doc": "tables are popular on the web because of their convenience for organizing and managing data\ntables can also be useful for presenting search results [20, 31]\nusers often search for a set of things, like music albums by a singer, films by an actor, restaurants nearby, etc\nin a typical information retrieval system, the matched entities are presented as a list.\n search, however, is often part of a larger work task, where the user might be interested in specific attributes of these entities\norganizing results, that is, entities and their attributes, in a tabular format facilitates a better overview\ne.g., for the query \u201cvideo albums of taylor swift,\u201d we can list the albums in a table, as shown in fig. 1\nthere exist two main families of methods that can return a table as answer to a keyword query by\n(i) performing table search to find existing tables on the web [4, 5, 19, 20, 25, 36], or (ii) assembling a table in a row-by-row fashion [31] or by joining columns from multiple tables [20]\nhowever, these methods are limited to returning tables that already exist in their entirety or at least partially (as complete rows/columns)\nanother line of work aims to translate a keyword or natural language query to a structured query language (e.g., sparql), which can be executed over a knowledge base [29]\nwhile in principle these techniques could return a list of tuples as the answer, in practice, they are targeted for factoid questions or at most a single attribute per answer entity.\nmore importantly, they require data to be available in a clean, structured form in a consolidated knowledge base\n instead, we propose to generate tables on the fly in a cell-by-cell basis, by combining information from existing tables as well as from a knowledge base, such that each cell\u2019s value can originate from a different source\nin this study, we focus on relational tables (also referred to as genuine tables [27, 28]), which describe a set of entities along with their attributes [15].\na relational table consists of three main elements\n(i) the core column entities e, (ii) the table schema s, which consists of the table\u2019s heading column labels, corresponding to entity attributes, and (iii) data cells, v , containing attribute values for each entity\nthe task of on-the-fly table generation is defined as follows\nanswering a free text query with an output table, where the core column lists all relevant entities and columns correspond the attributes of those entities.\n this task can naturally be decomposed into three main components\ncore column entity ranking, which is about identifying the entities to be included in the core column of the table\nschema determination, which is concerned with finding out what should be the column headings of the table, such that these attributes can effectively summarize answer entities\nvalue lookup, which is to find the values of corresponding attributes in existing tables or in a knowledge base. \nthe first subtask is strongly related to the problem of entity retrieval [12], while the second subtask is related to the problem of attribute retrieval [14]\nthese two subtasks, however, are not independent of each other\nwe postulate that core column entity ranking can be improved by knowing the schema of the table, and vice versa, having knowledge of the core column entities can be leveraged in schema determination\ntherefore, we develop a framework in which these two subtasks can be performed iteratively and can reinforce each other\n as for the third subtask, value lookup, the challenge there is to find a distinct value for an entity-attribute pair, with a traceable source, from multiple sources.\nin summary, the main contributions of this work are as follows: \nwe introduce the task of on-the-fly table generation and propose an iterative table generation algorithm (sect. 2)\nwe develop feature-based approaches for core column entity ranking (sect. 3) and schema determination (sect. 4), and design an entity-oriented fact catalog for fast and effective value lookup (sect. 5). \nwe perform extensive evaluation on the component level (sect. 7) and provide further insights and analysis (sect. 8)\nthe resources developed within this study are made publicly available at https://github.com/iai-group/sigir2018-tabl", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Many information needs revolve around entities, which would be better answered by summarizing results in a tabular format, rather than presenting them as a ranked list\nUnlike previous work, which is limited to retrieving existing tables, we aim to answer queries by automatically compiling a table in response to a query\nWe introduce and address the task of on-the-fly table generation\ngiven a query, generate a relational table that contains relevant entities (as rows) along with their key properties (as columns). \nThis problem is decomposed into three specific subtasks\n(i) core column entity ranking, (ii) schema determination, and (iii) value lookup\n We employ a feature-based approach for entity ranking and schema determination, combining deep semantic features with task specific signals\n  We further show that these two subtasks are not independent of each other and can assist each other in an iterative manner\nFor value lookup, we combine information from existing tables and a knowledge base\n  Using two sets of entity-oriented queries, we evaluate our approach both on the component level and on the end-to-end table generation tas"},
{"doc": "wikipedia articles are organized in sections.\nsections improve the readability of articles and provide a natural pathway for editors to break down the task of expanding a wikipedia article into smaller pieces\nhowever, knowing what sections belong to what types of articles in wikipedia is hard, especially for newcomers and less experienced users, as it requires having an overview of the broad \u201clandscape\u201d of wikipedia article types and inferring what sections are common and appropriate within each type. \ndespite the importance of sections, a large fraction of wikipedia articles does not have a satisfactory section structure yet\nless than 1% of all the roughly 5 million english wikipedia articles are considered to be of quality class \u201cgood\u201d or better, and 37% of all articles are stub\nfinally, there are many inconsistencies in section usage, even within a given wikipedia language;\ne.g., 80% of the section titles created in english wikipedia are used in only one article\ngiven wikipedia\u2019s popularity and influence\u2014with more than 500 million pageviews per day\u2014, there is an urgent need to expand its existing articles across languages to improve their quality as well as their consistency\nin other words, there is a need for a more systematic approach toward structuring wikipedia articles by means of sections\nfig. 1 shows the distribution of the number of sections per article for all english wikipedia articles, alongside the same distribution for the subset of articles considered to be of high quality, according to the objective revision evaluation service (ores),2 a scoring system used to assess the quality of wikipedia articles\nthe plot  shows that over one quarter of all articles have at most one section; also, the number of sections is considerably lower when averaged over all articles (3.4), compared to the high-quality subset (7.4). \nthe need for developing an approach to expand wikipedia articles is acknowledged in the literature, where the majority of the methods developed focus on automatic expansion techniques\nalgorithms are developed to propagate content across languages using the information in wikipedia\u2019s information boxes [17], to expand stubs by summarizing content from the web [3] or from wikipedia itself [2], and to enrich articles using knowledge bases such as dbpedia [18]\nhowever, these approaches are limited in their real-life applicability to wikipedia, for several reasons\n first, the type of content they generate is limited (e.g., information boxes or short statements).\nsecond, the accuracy of such approaches does not meet wikipedia\u2019s high bar of content accuracy, which prevents such algorithms from being used in wikipedia at scale\n and third, these approaches are not editor-centric, which is in fundamental contrast to how wikipedia is built and run\nthe desirability of an editor-centric approach for empowering editors to expand wikipedia articles, on the other hand, is acknowledged by experienced wikipedia editors and developers through the creation of manually curated templates which are surfaced in raw format on wikipedia how to pages as well as tools such as ma commune [19] and wikidaheim [7].\nthese efforts aim to help newcomers to wikipedia by providing recommendations on what sections to add to articles\nthe downside of these methods is that they do not scale and are very time-consuming to implement, as they rely on manually curated lists of sections per article type and for a given wikipedia language\ngapfinder [22] and suggestbot [8] are the only automatic editor-centric recommendation systems built and used in wikipedia\ngapfinder focuses on recommending what articles to create, while suggestbot recommends articles through calls for specific actions, such as adding citations\n none of the two systems, however, addresses the need for more in-depth recommendations on how to expand an already existing article\nin this paper, we take the first step for closing this gap in the literature by introducing and evaluating a series of editor-centric section recommendation methods\nthese methods differ in their source of signal (articles\u2019 topical content vs. wikipedia\u2019s category network) as well as the technology used to model the recommendations (simple counting vs. collaborative\" filtering)\nwe show that using wikipedia\u2019s category network along with the proposed countbased approach provides the best recommendations, achieving precision close to 80% when evaluated by human editors.\ncontributions\nour main contributions are as follows\nwe introduce the problem of recommending sections for wikipedia articles (sec. 2)\nwe devise multiple methods for addressing the problem (sec. 3\u20134)\nwe test our methods thoroughly in both an automatic and a human evaluation (sec. 5\u20137),\" finding that methods which leverage wikipedia\u2019s category system clearly work best\nin the evaluation section, we present results based on the english version of wikipedia, but our method is language-independent, as it does not depend on any linguistic features\nproject repository\nwe make code, data, and results available at https://github.com/ep!-dlab/structuring-wikipedia-article", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Sections are the building blocks of Wikipedia articles\nThey enhance readability and can be used as a structured entry point for creating and expanding articles\nStructuring a new or already existing Wikipedia article with sections is a hard task for humans, especially for newcomers or less experienced editors, as it requires significant knowledge about how a well-written article looks for each possible topic\nInspired by this need, the present paper defines the problem of section recommendation for Wikipedia articles and proposes several approaches for tackling it\nOur systems can help editors by recommending what sections to add to already existing or newly created Wikipedia articles\nOur basic paradigm is to generate recommendations by sourcing sections from articles that are similar to the input article\n We explore several ways of defining similarity for this purpose (based on topic modeling, collaborative \"filtering, and Wikipedia\u2019s category system)\nWe use both automatic and human evaluation approaches for assessing the performance of our recommendation system, concluding that the category-based approach works best, achieving precision@10 of about 80% in the human evaluatio"},
{"doc": "social media is built upon interaction and sharing, yet its openness also makes it vulnerable to abuse from its users\none form of such abuse is trolling, which has become more common in recent years, especially on news and social media sites\nherring et al. [25] defined a troll as an individual who baits and provokes other group members, often with the result of drawing them into a fruitless argument\na troll has also been identified as a person who engages in negatively-marked online behavior [22], a user who initially pretends to be a legitimate participant, but later attempts to disrupt the community [16], and a user who posts false or offensive comments in online communities to fool others [31]\ntrolls have been characterized as trouble makers [45], bad guys [17], sadists [9], and \u201ccreatures\u201d who take pleasure in upsetting others [28]\nwith its recent growth, the trolling phenomenon has been recognized as a serious threat to online communities [45].\nearly research on trolling behavior tended to be largely qualitative, generally involving case study analyses of a small number of manually-identified trolls [12].\nsuch studies focused on exploring trolling types [22], motivations [47], and response strategies [17]\nin recent years, more quantitative studies of trolling have started to emerge [13, 36, 37, 46, 48]\ndue to the difficulty in creating a large dataset of troll posts, most studies relied on a relatively modest set of examples, typically identified by manual annotation or the use of black lists of users who had been banned from the community\nhowever, a recent study [45] claims that such methods may often \u201cfail to capture what moderators and users consider trolling.\u201d\nin this work, we focus on trolling within one of the largest community question answering (cqa) websites, yahoo answers (ya)\ntrolling on ya can be performed both at the question level and the answer level and in this work we study the former\nwe identify troll questions based on the ya abuse report system, where any user can report deviant behavior by another user [27]\nwe combine both a user report and a subsequent deletion by a moderator in order to label troll questions\nthis combination of reports from the community and moderator actions serves as a good means for identifying ground-truth trolling, while also allowing to work at a scale and diversity larger than any prior work:\nwe identified over 400,000 troll questions on ya between the years 2007 and 2014, from over 200,000 different askers, across a wide variety of topics\nprevious work has mostly focused on trolling within forums, news sites, or commenting platforms [12, 13, 37, 45, 48]\nin such sites, trolling is performed at the comment (post) level, usually as part of a long thread, sometimes rich with other troll comments [37, 45], and often aiming at the thread\u2019s head article or a specific prior comment\nin contrast, question trolling is performed \u201cfrom scratch\u201d, without an explicit context, which makes it different in nature and potentially harder to detect\nthe position within the thread and the content of previous posts often serve as key features for comment trolling detection [37], yet such information is irrelevant in the case of question trolling\nto the best of our knowledge, this is the first study to explore trolling at the question level.\nin our analysis, we compare the set of troll questions to a set of non-troll (\u201cclean\u201d) questions of the same size\nwe examine a broad set of characteristics, including different types of metadata, linguistic properties of the text of the question and its answers, and past activity of the asker\nour findings reveal a handful of distinguishing qualities\nfor example, despite their shorter life span, troll questions receive more answers and votes than clean questions\nbased on these differences,we experiment with a basic classification setting, reaching an accuracy of 85.2% for the task of distinguishing troll and clean questions over a balanced dataset\ntextual features are found particularly useful for classification, with the answers\u2019 text being even more predictive than the text of the question itsel", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The phenomenon of trolling has emerged as a widespread form of abuse on news sites, online social networks, and other types of social media\nIn this paper,we study a particular type of trolling, performed by asking a provocative question on a community questionanswering website\nBy combining user reports with subsequent moderator deletions, we identify a set of over 400,000 troll questions on Yahoo Answers, i.e., questions aimed to inflame, upset, and draw attention from others on the community\nThis set of troll questions spans a lengthy period of time and a diverse set of topical categories\nOur analysis reveals unique characteristics of troll questions when compared to \u201cregular\u201d questions, with regards to their metadata, text, and askers.\nA classifier built upon these features reaches an accuracy of 85% over a balanced dataset\nThe answers\u2019 text and metadata, reflecting the community\u2019s response to the question, are found particularly productive for the classification tas"},
{"doc": "many job seekers use special-purpose online search systems to find employment opportunities\nsome of these systems not only gather job advertisements, but also provide a service that enables job seekers to immediately lodge applications\ndespite the popularity of such job search engines, there has been relatively little research on job search.\nsaha and arya [10] propose a personalized job-search learn-to-rank method, and spina et al. [12] suggest that job search (on the part of job seekers) and talent search (on the part of employers/ head hunters) have different aims to standard web or enterprise information search.\none common issue with web and enterprise search is mismatch between query and document collection, due to, for example, spelling errors or vocabulary differences\nthe way in which users amend (reformulate) their queries while searching on the web [2], together with techniques to categorize such amendments [7], has been much studied\n means of automatically aiding query amendment have also been extensively examined, including query spelling correction [5] and expansion [4]\nin this study we have investigated the impact of query amendment on a commercial job search engine.\nanalysing user logs and the reformulations they contain, we identified a set of 276 common queries that had small or empty answer sets, and for which we had high confidence that amendment would not confuse the users (including, for example, a set of spelling corrections)\nfor each we selected a single reformulation that captured the intent of the original query and was likely to match a greater number of position descriptions.\nevaluation of amendment algorithms typically employs offline methodologies, such as inferring user behavior from historical query logs [3, 5], or, more commonly, measuring effectiveness on conventional test collections [4].\nonline evaluation \u2013 the \u201cevaluation of a fully functioning system based on implicit measurement of real users\u2019 experiences\u201d [6] \u2013 provides another option\n our case study is tested using a living lab methodology based on a/b testing\nwith live deployment of the proposed approach and a significant user base, we are able to judge the extent to which the amendment benefits users\nin online evaluation, impact is commonly measured using clicks on search results, and via conversions, moments at which the user takes some form of action (for example, purchasing an item), implying satisfaction with the result [13]\nin job search, a conversion signal arises if the user lodges applications for one or more of the identified jobs\nin contrast to product search, for which conversion is typically a single purchase, job seekers may lodge multiple applications in a session spanning one or many searches\napplications in job search are also more effective than clicks in training learn-to-rank algorithms with weak supervision [10]\nin our case study we make use of clicks, applications, and subsequent query formulations.\n  query reformulations have been effectively used as a mechanism to predict user satisfaction in web search [1, 11]\nwe analyze query reformulations in the context of problematic queries, ones with fewer results than would fill a single page.\nby identifying common misformulations, we found that amendment can lead to dramatic increases in conversions\n on average, the reformulations led to a quadrupling of the number of clicks on job advertisements in that subset of the queries, and to an approximately six-fold increase in the number of applications lodged\n we also noted a substantial reduction in the number of subsequent reformulations in search sessions\n underpinning our work, we ask two research questions: (i) does query amendment improve user outcomes in job search in cases for which only a few results are returned by the original query?; and, if so, (ii) are those improvements reflected by changes in subsequent query reformulation", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Errors in formulation of queries made by users can lead to poor search results pages\nWe performed a living lab study using online A/B testing to measure the degree of improvement achieved with a query amendment technique when applied to a commercial job search engine\nOf particular interest in this case study is a clear \u201csuccess\u201d signal, namely, the number of job applications lodged by a user as a result of querying the service.\nA set of 276 queries was identified for amendment in four different categories through the use of word embeddings, with large gains in conversion rates being attained in all four of those categories.\nOur analysis of query reformulations also provides a better understanding of user satisfaction in the case of problematic queries (ones with fewer results than fill a single page) by observing that users tend to reformulate rewritten queries les"},
{"doc": "a knowledge graph is a heterogeneous, multi-relational, and directed graph comprises entities as nodes and relations as different type of edges\nfacts in knowledge graphs are represented as triplets, (head, relation, tail ), abbreviated as (h, r , t ), where relation is the relationship between head and tail entities\nalthough there are many instances in knowledge graphs, they are usually far from complete since they were constructed manually\nknowledge graph completion aims at predicting missing relations between entities based on existing triplets in the knowledge graphs\nrecently, a more efficient way is to represent each object (entities and relations) of knowledge graphs into a continuous vector space [9].\nthese vectors will contain not only the meanings of the objects themselves but also patterns between objects\ntranslation based models, such as transe [2], transh [13], transr [6] and ctransr [6] define their own score functions and apply to a margin based loss function.\nresults show that they have reached the state-of-the-art performance\nhowever, it is unreasonable for them to assume independence between triplets because entities are actually connected\ntherefore, context-aware models try to find dependent properties between objects\nfor example, gake [4] proposes a general framework to integrate graphical structural information and transe-nmm [10] considers each entity as a mixture model of its neighbors\nin this work, we propose a model called transn, a novel approach to capture more precise context information and to incorporate neighbor information dynamically.\nfirstly, we apply effective neighbor selection to reduce the number of neighbors.\nit is computationally inefficient to consider all neighbors and some of them might be noisy.\nsecond, we try to encode neighborhood information with context embeddings\nit is infeasible to use only one vector to capture both the meaning of the object and the context information at the same time since an entity plays distinct roles when being itself or a neighbor of others.\nthird, we further utilize attention mechanism to focus on most influential nodes since different neighbors provide different level of informatio", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Knowledge graph completion is a critical issue because many applications benefit from their structural and rich resources\nIn this paper, we propose a method named TransN, which considers the dependencies between triples and incorporates neighbor information dynamically\nIn experiments, we evaluate our model by link prediction and also conduct several qualitative analyses to prove effectiveness\nExperimental results show that our model could integrate neighbor information effectively and outperform state-of-the-art model"},
{"doc": "the main index structure in many current search engines[1] is the inverted index\nhowever, motivated by the high efficiency of bitwise operations, predecessors [3, 5] combined bitmaps and inverted indexes to speed up query processing\nrecently, goodwin et al. [2] introduced bitfunnel, a bitmap-like data structure based on a bloom filter [7]\nthe underlying data structure is split into shards according to document length, and each shard comprises of a collection of mapping matrices.\neach term maps to a rowor a fewrows in the mapping matrices in each shard (as determined by bloom filter multiplexing)\nordinarily, for each i \\\\epsilon  {0, 1, . . . , 6}, there is a mapping matrix of rank i\na column of the rank-i mapping matrix corresponds to a set of 2i documents, and the columns of the rank-0 mapping matrix corresponds to individual documents\nthis is illustrated in a toy example in figure 1\nin each shard, we independently compute the query intersection results\nroughly speaking, we intersect the relevant rows of equal rank, and we concatenate each rank\u2019s result with itself (thereby doubling its length), which is intersected with the next-lower rank intersection results\nwe do this until we reach rank 0, which determines the final intersection results\nto reduce the impact of false positives, we ensure all the mapping matrices are sparse; this is achieved by varying the number of rows (which affects the size of the mapping matrices, and thus the overall storage requirements)\nthere is some work on speeding up bitwise operations for bit- funnel [6], however bitfunnel\u2019s collection of mapping matrices is costly in terms of space, and is typically larger than the corresponding inverted index.\nmotivated by these observations, we propose a method for compressing bitfunnel mapping matrices\nmore specifically\n(1) we propose a dictionary-based compression method for bit- funnel mapping matrices, whereby frequently occurring blocks are replaced by indices to their corresponding blocks in a dictionary\n(2) we also design a document reordering strategy to increase the redundancy in the bitmap structure, reduce the additional false positives to facilitate compressio", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Large-scale search engines utilize inverted indexes which store ordered lists of document identifies (docIDs) relevant to query terms, which can be queried thousands of times per second.\nIn order to reduce storage requirements, we propose a dictionarybased compression approach for the recently proposed bitwise data-structure BitFunnel, which makes use of a Bloom filter\nCompression is achieved through storing frequently occurring blocks in a dictionary\nInfrequently occurring blocks (those which are not represented in the dictionary) are instead referenced using similar blocks that are in the dictionary, introducing additional false positive errors\nWe further introduce a docID reordering strategy to improve compression\nExperimental results indicate an improvement in compression by 27% to 30%, at the expense of increasing the query processing time by 16% to 48% and increasing the false positive rate by around 7.6 to 10.7 percentage point"},
{"doc": "convolutional neural networks (cnns) are increasingly used as feature extractors to support efficient content-based image retrieval (cbir) systems.\none of the obstacles to use these features is, however, in their high dimensionality, which prevents the use of standard space-partitioning data structures\nfor instance, in the well-known alexnet architecture [13] the output of the sixth layer (fc6) has 4,096 dimensions\nto overcome this problem, various partitioning methods have been proposed.\nfor example, the inverted multi-index [6], which outperforms the state of the art by a large margin, uses product quantization both to define the coarse level and to code residual vectors combined with binary compressed techniques\nin this paper, we propose an alternative approach suitable to be implemented in secondary memory\nwe exploit a surrogate text representation of the features that allows us to exploit an existing text retrieval engine to build a content-based image retrieval system\nthe key idea is to represent the cnn features as permutations and, in turn, to transform them into surrogate text using the basic approach developed in [7]\nthe cnn feature that we have used in this work is the regional maximum activation of convolutions (r-mac) [16], which is a very effective image representation for instance-level retrieval in cbir systems.\nthis feature is the result of the spatial aggregation of the activations of a convolution layer of a cnn and therefore robust to scale and translation\ngordo et al. [9] extended the rmac representation by improving the region pooling mechanism and including it in a differentiable pipeline trained end-to-end for retrieval tasks\nin this paper, we extend and improve our previous work on surrogate text representation [1] based on the approach of deep permutation [2], by adopting the concatenated rectified linear unit (crelu) transformation [15]\nthe advantage of this approach is a better estimate of the matching score among r-mac features by preserving both positive and negative activation information, which leads to an improvement of effectiveness at the same cost when using the conventional deep permutation approach\nwe have also improved the quality of the experimental evaluation providing a comparison with the faiss library [12], which implements pqcompressed inverted-file indexes in main memory\nmoreover, we have tested the scalability of our solution, by distributing our index across multiple nodes with elasticsearch (https://www.elastic.co)\nthe rest of the paper is organized as follows\nsection 2 provides background for the reader.\nin section 3, we introduce our approach to generate permutations for r-mac features\nsection 4 presents some experimental results using a real-life dataset\nsection 5 concludes the paper.", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Content-Based Image Retrieval in large archives through the use of visual features has become a very attractive research topic in recent years.\nThe cause of this strong impulse in this area of research is certainly to be attributed to the use of Convolutional Neural Network (CNN) activations as features and their outstanding performance\nHowever, practically all the available image retrieval systems are implemented in main memory, limiting their applicability and preventing their usage in big-data applications\n In this paper, we propose to transform CNN features into textual representations and index them with the well-known full-text retrieval engine Elasticsearch.\nWe validate our approach on a novel CNN feature, namely Regional Maximum Activations of Convolutions\nA preliminary experimental evaluation, conducted on the standard benchmark INRIA Holidays, shows the effectiveness and efficiency of the proposed approach and how it compares to state-of-the-art main-memory indexe"},
{"doc": "social media has become the main platform for people to share information and express their personal opinions\nthe diversity and modality of data in user-generated contents are increasing rapidly in recent year, leading to the ever-increasing of the proportion of visual contents in social media\nsentiment analysis as a core area of social media analytics has gone beyond traditional text-based analysis, and image information has proven an important source to analyze the sentiment of people [12]\nin contrast to traditional single modality based sentiment analysis, recent studies attempt to recognize sentiment expressed in multimodal data streams [1\u20133, 8, 10, 11, 13]\nalthough multimodal sentiment analysis is still in its infancy, it has demonstrated great potential in research and industry investment [8]\nearly work adopts feature-based methods\nfor example, borth et al. [2] extract 1200 adjective-noun pairs from image and calculates the textual sentiment scores based on english grammar and spelling style of text\nfeature engineering required in these methods is painstakingly detailed, biased, and labor-intensive\n with the booming of deep learning, neural network based models [1, 3, 10, 11, 13] have been proposed for multimodal sentiment analysis, with significant progress in performance\nbaecchi et al. [1] train word embeddings by continuous bag-of-words and concatenate them with the mid-level representation generated by denoising autoencoder for multimodal sentiment classification\ninspired by the performance of convolutional neural network in image and text classification tasks, the work in [3] and [13] employs pre-trained text cnn and image cnn to extract feature representations of text and image separately for sentiment analysis\nto incorporate the semantic information contained in image, xu et al. [10] extracts image captions which include detailed semantic visual information, and then processes tweet texts and image captions synchronously based on rnn\nthey further consider the influence of image to text by extracting visual features of scene and object from image, and absorb text words with these visual features in an attentional lstm model [11]\nthese previous work on neural network based models typically extracts features from image and text separately and then combine them directly to train the multimodal sentiment classifier, with the only exception of [11]\nalthough the work in [11] considers the single-direction influence of image to text, it still ignores the mutual reinforcing and complementary characteristics between visual and textual information, and in general lacks a fine-grained architectural framework to handle the interactions of multimodal contents.\n to address the above issue, in this paper, we propose a co-memory network for multimodal sentiment analysis, the key structure of which models the bi-directional interactions of image and text\n the contributions of our work are as follows\n we propose a novel co-memory network, which is the first time to model the mutual influence of visual and textual information in multimodal sentiment analysis\nwe design a co-memory attentional mechanism to capture the interactions of visual contents and textual words, and iteratively feed text information for finding visual key contents and image information for locating textual keywords\nexperimental results on two public multimodal sentiment datasets demonstrate that our proposed model outperforms the state-of-the-art method", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "With the rapid increase of diversity and modality of data in usergenerated contents, sentiment analysis as a core area of social media analytics has gone beyond traditional text-based analysis\n Multimodal sentiment analysis has become an important research topic in recent years\nMost of the existing work on multimodal sentiment analysis extracts features from image and text separately, and directly combine them to train a classifier.\nAs visual and textual information in multimodal data can mutually reinforce and complement each other in analyzing the sentiment of people, previous research all ignores this mutual influence between image and text\nTo fill this gap, in this paper, we consider the interrelation of visual and textual information, and propose a novel co-memory network to iteratively model the interactions between visual contents and textual words for multimodal sentiment analysis\nExperimental results on two public multimodal sentiment datasets demonstrate the effectiveness of our proposed model compared to the state-ofthe- art method"},
{"doc": "kterm hashing [18] provides an innovative approach to novelty detection\nwhen applied to streaming tasks, like first story detection (fsd), it exceeds the efficiency of state-of-the-art algorithms by several orders of magnitude, without sacrificing effectiveness\nkterm hashing forms compound terms, called kterms, from all unique terms in a document\nthe document\u2019s degree of novelty is computed by the number of unseen kterms in proportion to the document length\nwhen kterm hashing was first introduced, all kterms were considered as equally important for quantifying the degree of novelty\nwe believe that uniform kterm weights are sub-optimal for tasks like fsd, as kterms like {the, is, 23} would carry the same weight as the kterm {downtown, earthquake, la}\nby intuition, the latter one appears to be more helpful to discover new events in data streams\nwe propose to abandon the principle of uniform kterm importance and instead place weights on kterms to boost detection effectiveness\nlearning weights for all kterms is impractical because their number is high\ninstead of directly learning weights for each kterm, we learn weights for surrogate clusters\nthese clusters group kterms based on common characteristics, which allows associating a kterm\u2019s importance with the weight corresponding to its nearest cluster\nour experiments in section 4 show that parameterized kterm hashing can significantly outperform uniformly weighted kterm hashing for fs", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Kterm Hashing provides an innovative approach to novelty detection on massive data streams\nPrevious research focused on maximizing the efficiency of Kterm Hashing and succeeded in scaling First Story Detection to Twitter-size data stream without sacrificing detection accuracy.\n In this paper, we focus on improving the effectiveness of Kterm Hashing\n Traditionally, all kterms are considered as equally important when calculating a document\u2019s degree of novelty with respect to the past\n We believe that certain kterms are more important than others and hypothesize that uniform kterm weights are sub-optimal for determining novelty in data streams\nTo validate our hypothesis, we parameterize Kterm Hashing by assigning weights to kterms based on their characteristics\nOur experiments apply Kterm Hashing in a First Story Detection setting and reveal that parameterized Kterm Hashing can surpass state-of-the-art detection accuracy and significantly outperform the uniformly weighted approac"},
{"doc": "neural ir models have received much attention due to their continuous text representations, soft-matching of terms, and sophisticated non-linear models\nhowever, the non-convexity and stochastic training of neural ir models raises questions about their consistency compared to heuristic and learning-to-rank models that use discrete representations and simpler methods of combining evidence\nconsistent behavior under slightly different conditions is essential to reproducible research and deployment in industry\nthis paper studies the stability of k-nrm, a recent state-of-the-art neural ranking model [10]\nk-nrm learns the word embeddings and ranking model from relevance signals\nits effectiveness is due to word embeddings tailored for search tasks and kernels that group matches into bins of different quality\nto better understand its stability, we compare the behavior of multiple trained models under similar conditions\n we find that although k-nrm produces similar accuracy across different trials, it also produces rather different document rankings for individual queries\nanalysis of weights learned for k-nrm kernel scores (soft-match features) revealed that weights from different trials match one of two patterns.\nthe word embeddings reflect these patterns\n trials whose kernel weights have the same pattern have similar word embeddings\n interestingly, the two patterns are equally effective\nthe difference in the ranking patterns from different k-nrm trials makes them a good fit for ensembles\naggregating scores from different trials enables an ensemble to promote documents that multiple trials agree are most likely to be relevant\nexperimental results show that simple k-nrm ensembles significantly boost its ranking accuracy and improve its generalization abilit", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "This paper studies the consistency of the kernel-based neural ranking model (K-NRM), a recent state-of-the-art neural IR model, which is important for reproducible research and deployment in the industry\nWe find that K-NRM has low variance on relevance-based metrics across experimental trials\nIn spite of this low variance in overall performance, different trials produce different document rankings for individual queries\nThe main source of variance in our experiments was found to be different latent matching patterns captured by K-NRM\nIn the IR-customized word embeddings learned by K-NRM, the query-document word pairs follow two different matching patterns that are equally effective, but align word pairs differently in the embedding space\nThe different latent matching patterns enable a simple yet effective approach to construct ensemble rankers, which improve K-NRM\u2019s effectiveness and generalization abilitie"},
{"doc": "personalized recommender systems are pivotal in enhancing customers online shopping experiences, due to their ability to make personalized recommendations\nthese recommender systems learn user behavior from past observations, which are captured either through explicit user feedback, i.e. ratings or implicitly from user interaction with the system, i.e. the user purchase history, movies watched etc\nin real world systems, it is usually inexpensive to capture implicit feedback\nthe recommender systems generate a ranked list of targets i, for each personalized request represented through a given context c.\nthe ranking function r(i|c) \\\\rightarrow  \\\\mathbb{n}^{+}, defined in (1), generates an ordered ranking of the targets for each context c \\\\epsilon c, where \\\\hat{y}(i|c, \\\\theta ) is a model with model parameters \\\\theta\nr(i|c) = | \\\\left \\\\{ j|\\\\hat{y}(j|c, \\\\theta ) \\\\geq  \\\\hat{y}(i|c, \\\\theta ) \\\\right \\\\}| (1\nthe pairwise ranking algorithms [10] have shown to outperform pointwise ranking algorithms [5, 6] on the ranking task\nthe pairwise methods are well suited for implicit feedback as they learn a pairwise loss over a set of observed positive feedback and a set of unobserved feedback\nhowever, scalability studies of the pairwise methods for implicit feedback are limited, and generally focused on shared memory systems [11]\nin this paper we address the scalability of a pairwise ranking algorithm for a large scale dataset\nspecifically, we investigate the viability of existing distributed stochastic gradient optimization (sgo) algorithms [3, 5], which were originally designed for a pointwise loss and are mostly limited to a single relational matrix factorization (mf)\nthese methods block partition the rating matrix and create static blocks of model parameters\nhowever, the implicit feedback dataset only contains observed positive examples and unobserved examples are sampled\nthe static block partitioning of model parameters limits the sample space of unobserved examples, which can therefore introduce a bias in the gradient updates\nwe present a dynamic block partitioning and exchange strategy for model parameters by utilizing the information about the frequency of occurrence of features in each local data partition\n we demonstrate the applicability of our algorithm by using a generic framework based on factorization machines (fm) [9] to solve a pairwise ranking problem including multiple entity relationship", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Learning with pairwise ranking methods for implicit feedback datasets has shown promising results as compared to pointwise ranking methods for recommendation tasks\nHowever, there is limited effort in scaling the pairwise ranking methods in a large scale distributed setting\nIn this paper we address the scalability aspect of a pairwise ranking method using Factorization Machines in distributed settings\nOur proposed method is based on a block partitioning of the model parameters so that each distributed worker runs stochastic gradient updates on an independent block\nWe developed a dynamic block creation and exchange strategy by utilizing the frequency of occurrence of a feature in the local training data of a worker.\nEmpirical evidence on publicly available benchmark datasets indicates that the proposed method scales better than the static block based methods and outperforms competing state-ofthe- art method"},
{"doc": "users at particular locations typically have information needs based on their immediate geographic context\nfor example, a user at a restaurant engaging with a search system is likely to be searching for that restaurant\u2019s menu\nrecent works have studied this kind of contextual information, even going so far as to consider zero-query ranking [4, 15]\nthese works focus on the web, where query log mining can provide an understanding of trends and global behavior\nin the personal search domain, the challenge becomes more difficult: one cannot simply learn location-wise trending behavior due to the privacy constraints of personal search\nusing a set of anonymized email search logs with location information provided by google, we explore whether location information can be leveraged for query auto-completion\nsince we are unable to submit new queries, we explore a simulated task on this raw log data: user-independent query suggestion\nwe ask whether we can predict the queries a user is going to issue based upon 1. any characters they have already entered (possibly none), and 2. the location information\nthe contributions of this paper are as follows\nwe validate that location information is valuable for personal search by demonstrating the ability to predict queries using location information\nwe validate that semantic location information is valuable, using a non-parametric click-context model that allows us to learn location information from queries and documents with and without location associations\nwe observe that users often manually expand their personal search queries with their location context, indicating that it is a strong signal for relevance\nwe demonstrate our first two contributions by focusing on a query prediction or suggestion task: using minimal or no query information, we try to use the location information in our log to predict the queries\nin doing so, we explore a handful of models and look at their ability to generalize and perform on this task\nwe find that hashes of gps location provide evidence that location is helpful, but the coverage of this technique is not ideal: the majority of unique locations in our test set remain unseen even though our training set is larger\nwith much more data we would expect this problem to dissipate, but we look to a better opportunity: semantic location information\nwe annotate our query logs with geographic entity look-up: that is, for every latitude/longitude point, we perform a search of the nearest point of interest item using the google places web api, and include the title of this point in our extended logs\n  these titles provide the basis for our generalization\nfinally, we analyze our performance on query completion and find some surprising behavior in this task.\nour core observation is that users manually expand their queries with location, and hypothesize that will be difficult to beat this \u201chuman-expansion\u201d baseline if we were to look at improving search satisfaction directly (until users realize they no longer need to manually include location names", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": " Mobile devices are pervasive, which means that users have access to web content and their personal documents at all locations, not just their home or office\nExisting work has studied how locations can influence information needs, focusing on web queries\nWe explore whether or not location information can be helpful to users who are searching their own personal documents\nWe wish to study whether a users\u2019 location can predict their queries over their own personal data, so we focus on the task of query suggestion\nWhile we find that using location directly can be helpful, it does not generalize well to novel locations.\nTo improve this situation, we explore using semantic location: that is, rather than memorizing location-query associations, we generalize our location information to names of the closest point of interest\nBy using short, semantic descriptions of locations, we find that we can more robustly improve query completion and observe that users are already using locations to extend their own queries in this domain\nWe present a simple but effective model that can use location to predict queries for a user even before they type anything into a search box, and which learns effectively even when not all queries have location informatio"},
{"doc": "conversational assistants (cas) such as siri and cortana are becoming increasingly popular.\nusers can issue simple queries and commands to a ca by voice to conduct single-turn qa or goal-oriented tasks, such as asking for weather and setting timers\nhowever, cas are not yet capable of handling complicated information-seeking tasks which involve multiple turns of information exchange\nthese conversations are typically referred to as information-seeking conversations, where the information provider (agent) provides answers to a query from an information seeker (user) and the agent modifies the answers based on user feedback\nto build functional and natural cas that can reply to more complicated tasks we need to understand how users interact in these information-seeking environments\nthus, it is necessary to analyze and characterize user interactions and utterance intent\nat cair1 workshop at sigir17, researchers indicated that there is a lack of conversational datasets to conduct studies\ntherefore in this paper, we address this issue by collecting conversation data and creating the msdialog dataset\nwe present an analysis of user intent here, but msdialog could also be used to conduct other dialog related tasks including response ranking and user intent prediction\nfor effective analysis of user intent in an information-seeking process, the data should be multi-turn information-seeking dialogs\nto support natural dialogs, conversational systems should be modeled closely to human behavior, thus the data should come from conversation interactions between real humans\n as shown in table 1, we found that most existing dialog datasets are not appropriate for user intent analysis\nthe most similar data to ours is the ubuntu dialog corpus (udc), which also contains multi-turn qa conversations in the technical support domain\nhowever, the user intent in this dataset is unlabeled\nin addition, udc dialogs are in irc (internet relay chat) style\nthis informal language style contains a significant amount of typos, internet language, and abbreviations\nanother dataset, the dstc 6 conversation modeling track data contains knowledge grounded dialogs from twitter\nhowever, this dataset contains scenarios where users do not request information explicitly, which do not fit the information-seeking narrative\nthus these datasets are not appropriate for user intent analysis\nfor open-domain chatting, it is common practice to train chatbots with social media data such as twitter [13]\n similarly, real human-human multi-turn qa dialogs are the appropriate data for characterizing user intent in information-seeking conversations\nin technical support online forums, a thread is typically initiated by a user-generated question and answered by experienced users (agents)\nthe users may also exchange clarifications with the agents or give feedback based on answer quality\nthus the flow of a technical support thread resembles the information-seeking process if we consider threads as dialogs and posts as turns/utterances in dialogs\nwe created msdialog by crawling multi-turn qa threads from the microsoft community and annotate them with fine-grained user intent types on an utterance level based on crowdsourcing on amazon mechanical turk (mturk)\nwith this new dataset, we analyze the user intent distribution, co-occurrence patterns and flow patterns of large-scale qa dialogs\nwe gain insights on human intent dynamics during information seeking conversations\none of the most interesting findings is the high co-occurrence of negative feedback and further details, which typically occurs after a potential answer is given.\nthis cooccurrence pattern provides feedback about the retrieved answer and critical information about how to improve the previous answer\nin addition, negative feedback often leads to another answer response, indicating that co-occurrence and flow patterns associated with negative feedback can be the key to iterative answer finding\nto sum up, our contributions can be summarized as follows.\n(1) we create a large-scale annotated dataset for multi-turn informationseeking conversations, which is the first of its kind to the best of our knowledge\nwe will make our dataset freely available to encourage relevant studies.\n(2)we perform in-depth data analysis and characterization of multi-turn human qa conversations\n we analyze the user intent distribution, co-occurrence and flow patterns\n  our characterizations also hold in similar data (udc)\nour findings could be useful for designing conversational search system", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Understanding and characterizing how people interact in information seeking conversations is crucial in developing conversational search systems.\nIn this paper, we introduce a new dataset designed for this purpose and use it to analyze information-seeking conversations by user intent distribution, co-occurrence, and flow patterns\n The MSDialog dataset is a labeled dialog dataset of question answering (QA) interactions between information seekers and providers from an online forum on Microsoft products.\nThe dataset contains more than 2,000 multi-turn QA dialogs with 10,000 utterances that are annotated with user intent on the utterance level\nAnnotations were done using crowd sourcing\nWith MSDialog, we find some highly recurring patterns in user intent during an information-seeking process\nThey could be useful for designing conversational search systems\nWe will make our dataset freely available to encourage exploration of information-seeking conversation model"},
{"doc": "there is a growing body of work investigating different factors which affect user\u2019s judgment of relevance [1, 2, 6, 7, 9, 11\u201313]\na multidimensional user relevance model (murm) was proposed [12, 13] which defined five dimensions of relevance namely \"novelty\", \"reliability\", \"scope\", \"topicality\" and \"understandability\"\nin a recent paper [7] an extended version of the murm comprising two additional dimensions \"habit\" and \"interest\" is proposed\nthe \"interest\" dimension refers to the topical preferences of users in the past, while \"habit\" refers to their behavioral preferences\nfor example, accessing specific websites for some particular information or task is considered under the \"habit\" dimension\nexperiments on real-world data show that certain dimensions, such as \"reliability\" and \"interest\", are more important for the user than \"topicality\", in judging a document\nour hypothesis is that in a particular search session or search task, there is a particular relevance dimension or a combination of relevance dimensions which the user has in mind before judging documents\nfor example, if the user wants to get a visa to a country, he or she would prefer documents which are more reliable (\"reliability\") for this task, but when looking to book flights to that country, the user might go to his or her preferred websites (\"habit\").\ntherefore, for next few queries of the session, \"habit\" dimension becomes more important\nthus, the importance given to relevance dimensions might change as the session progresses or tasks switch\nby capturing the importance assigned to each dimension for a query, we can model the dimensional importance and use it to improve the ranking for the subsequent queries\n the relevance dimensions are modeled using the hilbert space formalism of quantum theory which unifies the logical, probabilistic and vector space based approaches to ir [8]\nwe place the user\u2019s cognitive state with respect to a document at the center of the ir process\nsuch a state is modeled as an abstract vector with multiple representations existing at the same time in different basis corresponding to different relevance dimensions\nthis cognitive state comes into reality only when it is measured in the context of user interaction", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "It has been shown that relevance judgment of documents is influenced by multiple factors beyond topicality\nSome multidimensional user relevance models (MURM) proposed in literature have investigated the impact of different dimensions of relevance on user judgment.\nOur hypothesis is that a user might give more importance to certain relevance dimensions in a session which might change dynamically as the session progresses.\nThis motivates the need to capture the weights of different relevance dimensions using feedback and build a model to rank documents for subsequent queries according to these weights.\nWe propose a geometric model inspired by the mathematical framework of Quantum theory to capture the user\u2019s importance given to each dimension of relevance and test our hypothesis on data from a web search engine and TREC Session trac"}]
