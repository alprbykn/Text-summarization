{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attractive-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "module_path = os.path.abspath(os.path.join('C:/Users/user/metin özütleme/hphaos summarunner/models'))\n",
    "import json\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "resistant-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class BasicModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        super(BasicModule,self).__init__()\n",
    "        self.args = args\n",
    "        print(self.args.device)\n",
    "        print()\n",
    "        self.model_name = str(type(self))\n",
    "\n",
    "    def pad_doc(self,words_out,doc_lens):\n",
    "        pad_dim = words_out.size(1)\n",
    "        max_doc_len = max(doc_lens)\n",
    "        sent_input = []\n",
    "        start = 0\n",
    "        for doc_len in doc_lens:\n",
    "            stop = start + doc_len\n",
    "            valid = words_out[start:stop]                                       # (doc_len,2*H)\n",
    "            start = stop\n",
    "            if doc_len == max_doc_len:\n",
    "                sent_input.append(valid.unsqueeze(0))\n",
    "            else:\n",
    "                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim))\n",
    "                if self.args.device is not None:\n",
    "                    pad = pad.cuda()\n",
    "                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))          # (1,max_len,2*H)\n",
    "        sent_input = torch.cat(sent_input,dim=0)                                # (B,max_len,2*H)\n",
    "        return sent_input\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint = {'model':self.state_dict(), 'args': self.args}\n",
    "        best_path = '%s%s_seed_%d.pt' % (self.args.save_dir,self.model_name,self.args.seed)\n",
    "        torch.save(checkpoint,best_path)\n",
    "\n",
    "        return best_path\n",
    "\n",
    "    def load(self, best_path):\n",
    "        if self.args.device is not None:\n",
    "            print(self.args.device)\n",
    "            data = torch.load(best_path)['model']\n",
    "        else:\n",
    "            data = torch.load(best_path, map_location=lambda storage, loc: storage)['model']\n",
    "        self.load_state_dict(data)\n",
    "        if self.args.device is not None:\n",
    "            return self.cuda()\n",
    "        else:\n",
    "            return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "general-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicModule import BasicModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN_RNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(CNN_RNN,self).__init__(args)\n",
    "        self.model_name = 'CNN_RNN'\n",
    "        self.args = args\n",
    "        \n",
    "        Ks = args.kernel_sizes\n",
    "        Ci = args.embed_dim\n",
    "        Co = args.kernel_num\n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.convs = nn.ModuleList([ nn.Sequential(\n",
    "                                            nn.Conv1d(Ci,Co,K),\n",
    "                                            nn.BatchNorm1d(Co),\n",
    "                                            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                                            nn.Conv1d(Co,Co,K),\n",
    "                                            nn.BatchNorm1d(Co),\n",
    "                                            nn.LeakyReLU(inplace=True)\n",
    "                                     )\n",
    "                                    for K in Ks])\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = Co * len(Ks),\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(2*H,2*H),\n",
    "                nn.BatchNorm1d(2*H),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "\n",
    "    def max_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.max_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def avg_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.avg_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def forward(self,x,doc_lens):\n",
    "        sent_lens = torch.sum(torch.sign(x),dim=1).data \n",
    "        H = self.args.hidden_size\n",
    "        x = self.embed(x)                                                       # (N,L,D)\n",
    "        # word level GRU\n",
    "        x = [conv(x.permute(0,2,1)) for conv in self.convs]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x,1)\n",
    "        # make sent features(pad with zeros)\n",
    "        x = self.pad_doc(x,doc_lens)\n",
    "\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
    "        docs = self.fc(docs)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = docs[index].unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriented-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4918, -0.8075, -3.1827, -0.7470]],\n",
      "\n",
      "        [[-0.6181, -0.9139, -0.2974,  0.3185]],\n",
      "\n",
      "        [[ 0.6534,  1.3140,  0.5316,  1.4086]],\n",
      "\n",
      "        [[-0.5043,  1.8300, -0.3793, -2.1729]],\n",
      "\n",
      "        [[-1.3261, -0.2732, -0.0319, -0.5234]],\n",
      "\n",
      "        [[ 0.3076, -1.0698, -0.6218,  0.6029]],\n",
      "\n",
      "        [[-1.8304,  0.3810, -0.0749, -2.1429]],\n",
      "\n",
      "        [[ 0.4508, -0.1929,  0.0575, -0.6249]],\n",
      "\n",
      "        [[ 0.7716, -0.4401,  0.3739,  0.7881]],\n",
      "\n",
      "        [[ 0.8114, -0.2754,  0.1713, -0.2009]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the query features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*query \\\\\n",
    "            attn_scores = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            attn_out = attn * context\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the query\n",
    "    Inputs: query, context\n",
    "        - **query** (batch, query_len, dimensions): tensor containing the query features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: query, attn\n",
    "        - **query** (batch, query_len, dimensions): tensor containing the attended query features from the decoder.\n",
    "        - **attn** (batch, query_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "    \n",
    "    \"\"\"\n",
    "        - query   (batch, query_len, dimensions): tensor containing the query features from the decoder.\n",
    "        - context (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    \"\"\"\n",
    "    def forward(self, query, context):\n",
    "        batch_size = query.size(0)\n",
    "        dim = query.size(2)\n",
    "        in_len = context.size(1)\n",
    "        # (batch, query_len, dim) * (batch, in_len, dim) -> (batch, query_len, in_len)\n",
    "        attn = torch.bmm(query, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn_scores = F.softmax(attn.view(-1, in_len),dim=1).view(batch_size, -1, in_len)\n",
    "\n",
    "        # (batch, query_len, in_len) * (batch, in_len, dim) -> (batch, query_len, dim)\n",
    "        attn_out = torch.bmm(attn_scores, context)\n",
    "\n",
    "        return attn_out, attn_scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(1)\n",
    "    attention = Attention()\n",
    "    context = Variable(torch.randn(10, 20, 4))\n",
    "    query = Variable(torch.randn(10, 1, 4))\n",
    "    query, attn = attention(query, context)\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "involved-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#coding:utf8\n",
    "from BasicModule import BasicModule\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from .Attention import Attention\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class AttnRNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(AttnRNN,self).__init__(args)\n",
    "        self.model_name = 'AttnRNN'\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.attn = Attention()\n",
    "        self.word_query = nn.Parameter(torch.randn(1,1,2*H))\n",
    "        self.sent_query = nn.Parameter(torch.randn(1,1,2*H))\n",
    "\n",
    "        self.word_RNN = nn.GRU(\n",
    "                        input_size = D,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = 2*H,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "               \n",
    "        self.fc = nn.Linear(2*H,2*H)\n",
    "\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "    def forward(self,x,doc_lens):\n",
    "        N = x.size(0)\n",
    "        L = x.size(1)\n",
    "        B = len(doc_lens)\n",
    "        H = self.args.hidden_size\n",
    "        word_mask = torch.ones_like(x) - torch.sign(x)\n",
    "        word_mask = word_mask.data.type(torch.cuda.ByteTensor).view(N,1,L)\n",
    "        \n",
    "        x = self.embed(x)                                # (N,L,D)\n",
    "        x,_ = self.word_RNN(x)\n",
    "        \n",
    "        # attention\n",
    "        query = self.word_query.expand(N,-1,-1).contiguous()\n",
    "        self.attn.set_mask(word_mask)\n",
    "        word_out = self.attn(query,x)[0].squeeze(1)      # (N,2*H)\n",
    "\n",
    "        x = self.pad_doc(word_out,doc_lens)\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
    "        max_doc_len = max(doc_lens)\n",
    "        mask = torch.ones(B,max_doc_len)\n",
    "        for i in range(B):\n",
    "            for j in range(doc_lens[i]):\n",
    "                mask[i][j] = 0\n",
    "        sent_mask = mask.type(torch.cuda.ByteTensor).view(B,1,max_doc_len)\n",
    "        \n",
    "        # attention\n",
    "        query = self.sent_query.expand(B,-1,-1).contiguous()\n",
    "        self.attn.set_mask(sent_mask)\n",
    "        docs = self.attn(query,x)[0].squeeze(1)      # (B,2*H)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                #print position,F.sigmoid(abs_p + rel_p)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "transparent-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self,embed,word2id):\n",
    "        self.embed = embed\n",
    "        self.word2id = word2id\n",
    "        self.id2word = {v:k for k,v in word2id.items()}\n",
    "        assert len(self.word2id) == len(self.id2word)\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.PAD_TOKEN = 'PAD_TOKEN'\n",
    "        self.UNK_TOKEN = 'UNK_TOKEN'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(word2id)\n",
    "\n",
    "    def i2w(self,idx):\n",
    "        return self.id2word[idx]\n",
    "    def w2i(self,w):\n",
    "        if w in self.word2id:\n",
    "            return self.word2id[w]\n",
    "        else:\n",
    "            return self.UNK_IDX\n",
    "    \n",
    "    def make_features(self,batch,sent_trunc=25,doc_trunc=50,split_token='\\n'):\n",
    "        sents_list,targets,doc_lens = [],[],[]\n",
    "        # trunc document\n",
    "        for doc,label in zip(batch['doc'],batch['labels']):\n",
    "            sents = doc.split(split_token)\n",
    "            labels = label.split(split_token)\n",
    "            labels = [int(l) for l in labels]\n",
    "            max_sent_num = min(doc_trunc,len(sents))##doküman 50 den az cümle içerebilir.\n",
    "            sents = sents[:max_sent_num]\n",
    "            labels = labels[:max_sent_num]\n",
    "            sents_list += sents\n",
    "            targets += labels\n",
    "            doc_lens.append(len(sents)) ##doküman uzunluğu maksimum cümle kadar içerir\n",
    "        # trunc or pad sent\n",
    "        max_sent_len = 0\n",
    "        batch_sents = []\n",
    "        for sent in sents_list:\n",
    "            words = sent.split()\n",
    "            if len(words) > sent_trunc:##cümle içindeki maksimum kelime sayısı\n",
    "                words = words[:sent_trunc]\n",
    "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "            batch_sents.append(words)\n",
    "        \n",
    "        features = []\n",
    "        for sent in batch_sents:\n",
    "            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len-len(sent))]\n",
    "            features.append(feature)\n",
    "        \n",
    "        features = torch.LongTensor(features)    \n",
    "        targets = torch.LongTensor(targets)\n",
    "        summaries = batch['summaries']\n",
    "\n",
    "        return features,targets,summaries,doc_lens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bound-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "#from .Vocab import Vocab\n",
    "import numpy as np\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, examples):\n",
    "        super(Dataset,self).__init__()\n",
    "        # data: {'sents':xxxx,'labels':'xxxx', 'summaries':[1,0]}\n",
    "        self.examples = examples \n",
    "        self.training = False\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        return self\n",
    "    def test(self):\n",
    "        self.training = False\n",
    "        return self\n",
    "    def shuffle(self,words):\n",
    "        np.random.shuffle(words)\n",
    "        return ' '.join(words)\n",
    "    def dropout(self,words,p=0.3):\n",
    "        l = len(words)\n",
    "        drop_index = np.random.choice(l,int(l*p))\n",
    "        keep_words = [words[i] for i in range(l) if i not in drop_index]\n",
    "        return ' '.join(keep_words)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        return ex,\n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ordinary-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicModule import BasicModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN_RNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(RNN_RNN, self).__init__(args)\n",
    "        self.model_name = 'RNN_RNN'\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.word_RNN = nn.GRU(\n",
    "                        input_size = D,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = 2*H,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.fc = nn.Linear(2*H,2*H)\n",
    "\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "\n",
    "    def max_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.max_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def avg_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.avg_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def forward(self,x,doc_lens):\n",
    "        sent_lens = torch.sum(torch.sign(x),dim=1).data \n",
    "        x = self.embed(x)                                                      # (N,L,D)\n",
    "        # word level GRU\n",
    "        H = self.args.hidden_size\n",
    "        x = self.word_RNN(x)[0]                                                 # (N,2*H,L)\n",
    "        #word_out = self.avg_pool1d(x,sent_lens)\n",
    "        word_out = self.max_pool1d(x,sent_lens)\n",
    "        # make sent features(pad with zeros)\n",
    "        x = self.pad_doc(word_out,doc_lens)\n",
    "\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
    "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dressed-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def sigirdata():\n",
    "    #with open('sigirtrain.json', 'w') as outfile:\n",
    "        filecount=0\n",
    "        abslinecount=0\n",
    "        dir = \"C:/Users/user/Desktop/sondönem/bitirme/sigir/SIGIR2018_Extracts/SIGIR_Sessions\" \n",
    "        finaldoc=[]\n",
    "        mydict={}\n",
    "        finalstr=\"\"\n",
    "        newlinecount=0\n",
    "        labels=\"1\"\n",
    "        keys=['doc', 'labels','summaries']\n",
    "        for i in keys:\n",
    "            mydict[i] = \"\"\n",
    "        for dirPath, foldersInDir,fileName in os.walk(dir):\n",
    "            if fileName is not []:\n",
    "                for file in fileName:\n",
    "                    if file.endswith('t.txt'):\n",
    "                        loc = os.sep.join([dirPath,file])\n",
    "                        abstract=open(loc,encoding=\"utf8\")\n",
    "                        abst=abstract.read()\n",
    "\n",
    "                        lines = abst.split(\"\\n\")\n",
    "                        #print(lines)\n",
    "                        #print(\"--------------------------\")\n",
    "                        m = re.findall('1\">(.+?).</', str(lines))\n",
    "                        if m:\n",
    "                            for t in m:\n",
    "                                abslinecount=abslinecount+1\n",
    "                                t = t.replace(\"',\", \"\")\n",
    "                                t = t.replace(\"'\", \"\")\n",
    "                                t=t.lower()\n",
    "                                t= t.translate(string.maketrans('\"\"','\"\"'), string.punctuation)\n",
    "                                finalstr=finalstr+t+\"\\n\"\n",
    "                                mydict[\"summaries\"]=finalstr[:-2].replace(\"'\",'\"')\n",
    "                    #abssizes.append(abslinecount)\n",
    "                    abslinecount=0\n",
    "                    finalstr=\"\"\n",
    "\n",
    "                    if file.endswith('o.txt'):\n",
    "\n",
    "                        #print(\"readin intro\")\n",
    "                        loc = os.sep.join([dirPath,file])\n",
    "                        doc=open(loc,encoding=\"utf8\")\n",
    "                        doc=doc.read()\n",
    "                        #print(doc)\n",
    "                        #print()\n",
    "                        lines = doc.split(\"\\n\")\n",
    "                        #print(lines)\n",
    "                        #print(\"--------------------------\")\n",
    "                        #text = 'gfgfdAAA1234ZZZuijjk'\n",
    "                        #while m!=[]:\n",
    "                        m = re.findall('\"[0-9]\">(.+?).</', str(lines))\n",
    "                        if m:\n",
    "                            for t in m:\n",
    "                                #print(t)\n",
    "                                #labels=labels+\"\"\n",
    "\n",
    "\n",
    "                                #labels=labels.replace(\" \",\"\")\n",
    "                                newlinecount=newlinecount+1\n",
    "                                t = t.replace(\"',\", \"\").lower()\n",
    "                                t = t.replace(\"'\", \"\")\n",
    "                                finalstr=finalstr+t+\"\\n\"\n",
    "                                labels=labels+\"k\\n\"+\"1\"\n",
    "                                labels=labels.replace(\"k\", \"\")\n",
    "                                mydict[\"doc\"]=finalstr[:-2].replace(\"'\",'\"')\n",
    "                                lb=str(labels)\n",
    "                                mydict[\"labels\"]=str(lb).replace(\"'\",'\"')\n",
    "                        \n",
    "                        #json.dump(mydict, outfile)\n",
    "                        #print(\"-------------\")\n",
    "                        finaldoc.append(dict(mydict))\n",
    "                        #print(finaldoc[filecount])\n",
    "                        #print(\"----------\")\n",
    "                        filecount+=1\n",
    "                        finalstr=\"\"\n",
    "                        labels=\"1\"\n",
    "                        newlinecount=0\n",
    "                        #print(finaldoc)\n",
    "                    \n",
    "        print(\"SIGIR DATA GENERATED\")\n",
    "\n",
    "        \"\"\"  with open(\"sigirtrain.json\", 'w') as file:\n",
    "            json_string = json.dumps(finaldoc, default=lambda o: o.__dict__, sort_keys=True, indent=2)\n",
    "            file.write(json_string)\n",
    "        #with open('sigirtrain.json', 'w') as outfile:\n",
    "            #for i in range(len(finaldoc)):\n",
    "            #json.dump(finaldoc, outfile,indent=2)\n",
    "        with open('sigirval.json', 'w') as outfile:\n",
    "            json.dump(finaldoc[-10:], outfile)\"\"\"\n",
    "    \n",
    "        return finaldoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "binding-bookmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, should run with -device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nif __name__=='__main__':\\n    if args.test:\\n        test()\\n    elif args.predict:\\n        with open(args.filename) as file:\\n            bod = [file.read()]\\n        predict(bod)\\n    else:\\n        train()\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import json\n",
    "import models\n",
    "#import utils\n",
    "import argparse,random,logging,numpy,os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')\n",
    "parser = argparse.ArgumentParser(description='extractive summary')\n",
    "# model\n",
    "parser.add_argument('-save_dir',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/sigir/sigircheckpoints/')\n",
    "parser.add_argument('-embed_dim',type=int,default=100)\n",
    "parser.add_argument('-embed_num',type=int,default=100)\n",
    "parser.add_argument('-pos_dim',type=int,default=50)\n",
    "parser.add_argument('-pos_num',type=int,default=100)\n",
    "parser.add_argument('-seg_num',type=int,default=10)\n",
    "parser.add_argument('-kernel_num',type=int,default=100)\n",
    "parser.add_argument('-kernel_sizes',type=str,default='3,4,5')\n",
    "parser.add_argument('-model',type=str,default='RNN_RNN')\n",
    "parser.add_argument('-hidden_size',type=int,default=200)\n",
    "# train\n",
    "parser.add_argument('-lr',type=float,default=1e-3)\n",
    "parser.add_argument('-batch_size',type=int,default=32)\n",
    "parser.add_argument('-epochs',type=int,default=3)\n",
    "parser.add_argument('-seed',type=int,default=1)\n",
    "parser.add_argument('-train_dir',type=str,default='data/train.json')\n",
    "parser.add_argument('-val_dir',type=str,default='data/val.json')\n",
    "parser.add_argument('-embedding',type=str,default='data/embedding.npz')\n",
    "parser.add_argument('-word2id',type=str,default='data/word2id.json')\n",
    "parser.add_argument('-report_every',type=int,default=1500)\n",
    "parser.add_argument('-seq_trunc',type=int,default=50)\n",
    "parser.add_argument('-max_norm',type=float,default=1.0)\n",
    "# test\n",
    "parser.add_argument('-load_dir',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/checkpoints/CNN_RNN_seed_1.pt')\n",
    "parser.add_argument('-sigir_dir',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/sigir/sigircheckpoints/RNN_RNN_seed_1.pt')\n",
    "parser.add_argument('-test_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/test.json')\n",
    "parser.add_argument('-ref',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/outputs/ref')\n",
    "parser.add_argument('-sigirref',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirref')\n",
    "parser.add_argument('-sigirhyp',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirhyp')\n",
    "parser.add_argument('-hyp',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/outputs/hyp')\n",
    "parser.add_argument('-filename',type=str,default='x.txt') # TextFile to be summarized\n",
    "parser.add_argument('-topk',type=int,default=7)\n",
    "# device\n",
    "parser.add_argument('-device',type=int)\n",
    "# option\n",
    "parser.add_argument('-test',action='store_true')\n",
    "parser.add_argument('-debug',action='store_true')\n",
    "parser.add_argument('-predict',action='store_true')\n",
    "args = parser.parse_args()\n",
    "use_gpu = args.device is not None\n",
    "\n",
    "if torch.cuda.is_available() and not use_gpu:\n",
    "    print(\"WARNING: You have a CUDA device, should run with -device 0\")\n",
    "\n",
    "# set cuda device and seed\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.device)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "numpy.random.seed(args.seed) \n",
    "    \n",
    "def eval(net,vocab,data_iter,criterion):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    for batch in data_iter:\n",
    "        features,targets,_,doc_lens = vocab.make_features(batch)\n",
    "        features,targets = Variable(features), Variable(targets.float())\n",
    "        if use_gpu:\n",
    "            features = features.cuda()\n",
    "            targets = targets.cuda()\n",
    "        probs = net(features,doc_lens)\n",
    "        loss = criterion(probs,targets)\n",
    "        total_loss += loss.data\n",
    "        batch_num += 1\n",
    "    loss = total_loss / batch_num\n",
    "    net.train()\n",
    "    return loss\n",
    "\n",
    "def trainsigir():\n",
    "    logging.info('Loading vocab,train and val dataset.Wait a second,please')\n",
    "    \n",
    "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
    "    with open(args.word2id) as f:\n",
    "        word2id = json.load(f)\n",
    "    vocab = Vocab(embed, word2id)\n",
    "\n",
    "    with open(\"C:/Users/user/metin özütleme/hphaos summarunner/sigirtrain.json\",encoding=\"utf8\") as f:\n",
    "        examples = json.load(f)\n",
    "    train_dataset = Dataset(examples)\n",
    "    with open(\"C:/Users/user/metin özütleme/hphaos summarunner/sigirval.json\",encoding=\"utf8\") as f:\n",
    "        examples = json.load(f)\n",
    "    val_dataset = Dataset(examples)\n",
    "\n",
    "    #with open(\"C:/Users/user/metin özütleme/hphaos summarunner/sigirval.json\",encoding=\"utf8\") as f:\n",
    "     #   examples = [json.loads(line) for line in f]\n",
    "    #val_dataset = Dataset(examples)\n",
    "    #with open(args.val_dir,encoding=\"utf8\") as f:\n",
    "        #examples = [json.loads(line) for line in f]\n",
    "    \n",
    "    \n",
    "    # update args\n",
    "    args.embed_num = embed.size(0)\n",
    "    args.embed_dim = embed.size(1)\n",
    "    args.kernel_sizes = ['3,4,5']\n",
    "    # build model\n",
    "    net = getattr(models,args.model)(args,embed)\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    # load dataset\n",
    "    train_iter = DataLoader(dataset=train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True)\n",
    "    val_iter = DataLoader(dataset=val_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False)\n",
    "    # loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    # model info\n",
    "    print(net)\n",
    "    params = sum(p.numel() for p in list(net.parameters())) / 1e6\n",
    "    print('#Params: %.1fM' % (params))\n",
    "    \n",
    "    min_loss = float('inf')\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
    "    net.train()\n",
    "    \n",
    "    t1 = time() \n",
    "    for epoch in range(1,args.epochs+1):\n",
    "        for i,batch in enumerate(train_iter):\n",
    "            print(batch)\n",
    "            features,targets,_,doc_lens = vocab.make_features(batch)\n",
    "            features,targets = Variable(features), Variable(targets.float())\n",
    "            if use_gpu:\n",
    "                features = features.cuda()\n",
    "                targets = targets.cuda()\n",
    "            probs = net(features,doc_lens)\n",
    "            \n",
    "            loss = criterion(probs,targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm(net.parameters(), args.max_norm)\n",
    "            optimizer.step()\n",
    "            if args.debug:\n",
    "                print('Batch ID:%d Loss:%f' %(i,loss.data[0]))\n",
    "                continue\n",
    "            if i % args.report_every == 0:\n",
    "                cur_loss = eval(net,vocab,val_iter,criterion)\n",
    "                if cur_loss < min_loss:\n",
    "                    min_loss = cur_loss\n",
    "                    best_path = net.save()\n",
    "                logging.info('Epoch: %2d Min_Val_Loss: %f Cur_Val_Loss: %f'\n",
    "                        % (epoch,min_loss,cur_loss))\n",
    "    t2 = time()\n",
    "    logging.info('Total Cost:%f h'%((t2-t1)/3600))\n",
    "\n",
    "def testsigir():\n",
    "     \n",
    "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
    "    with open(args.word2id) as f:\n",
    "        word2id = json.load(f)\n",
    "    vocab = Vocab(embed, word2id)\n",
    "    with open(\"C:/Users/user/metin özütleme/hphaos summarunner/sigirtest.json\",encoding=\"utf8\") as f:\n",
    "        examples = json.load(f)\n",
    "    test_dataset = Dataset(examples)\n",
    "\n",
    "    test_iter = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False)\n",
    "    if use_gpu:\n",
    "        checkpoint = torch.load(args.sigir_dir)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.sigir_dir, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    # checkpoint['args']['device'] saves the device used as train time\n",
    "    # if at test time, we are using a CPU, we must override device to None\n",
    "    if not use_gpu:\n",
    "        checkpoint['args'].device = None\n",
    "    net = getattr(models,checkpoint['args'].model)(checkpoint['args'])\n",
    "    net.load_state_dict(checkpoint['model'])\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    doc_num = len(test_dataset)\n",
    "    time_cost = 0\n",
    "    file_id = 1\n",
    "    for batch in tqdm(test_iter):\n",
    "        features,_,summaries,doc_lens = vocab.make_features(batch)\n",
    "        t1 = time()\n",
    "        if use_gpu:\n",
    "            probs = net(Variable(features).cuda(), doc_lens)\n",
    "        else:\n",
    "            probs = net(Variable(features), doc_lens)\n",
    "        t2 = time()\n",
    "        time_cost += t2 - t1\n",
    "        start = 0\n",
    "        for doc_id,doc_len in enumerate(doc_lens):\n",
    "            stop = start + doc_len\n",
    "            prob = probs[start:stop]\n",
    "            topk = min(args.topk,doc_len)\n",
    "            #topk=min(abssizes[file_id-1],doc_len)\n",
    "            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n",
    "            topk_indices.sort()\n",
    "            doc = batch['doc'][doc_id].split('\\n')[:doc_len]\n",
    "            hyp = [doc[index] for index in topk_indices]\n",
    "            ref = summaries[doc_id]\n",
    "            with open(os.path.join(args.sigirref,str(file_id)+'.txt'), 'w',encoding=\"utf8\") as f:\n",
    "                f.write(ref)\n",
    "            with open(os.path.join(args.sigirhyp,str(file_id)+'.txt'), 'w',encoding=\"utf8\") as f:\n",
    "                f.write('\\n'.join(hyp))\n",
    "            start = stop\n",
    "            file_id = file_id + 1\n",
    "    print('Speed: %.2f docs / s' % (doc_num / time_cost))\n",
    "    print(\"test ended\")\n",
    "\n",
    "\"\"\"\n",
    "if __name__=='__main__':\n",
    "    if args.test:\n",
    "        test()\n",
    "    elif args.predict:\n",
    "        with open(args.filename) as file:\n",
    "            bod = [file.read()]\n",
    "        predict(bod)\n",
    "    else:\n",
    "        train()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eight-orlando",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Rouge155.py, line 335)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\user\\anaconda3\\envs\\sigir\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3343\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-23-1589e5257923>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from pyrouge import Rouge155\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\anaconda3\\envs\\sigir\\lib\\site-packages\\pyrouge\\__init__.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from pyrouge.Rouge155 import Rouge155\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\anaconda3\\envs\\sigir\\lib\\site-packages\\pyrouge\\Rouge155.py\"\u001b[1;36m, line \u001b[1;32m335\u001b[0m\n\u001b[1;33m    \"Running ROUGE with command {}\".format(\" \".join(command))) command.insert(0,'perl')\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pyrouge import Rouge155\n",
    "def remove_broken_files():\n",
    "    error_id = []\n",
    "    for f in os.listdir('C:/Users/user/Desktop/sondönem/bitirme/outputs/ref'):\n",
    "        try:\n",
    "            open('ref/' + f).read()\n",
    "        except:\n",
    "            error_id.append(f)\n",
    "    for f in os.listdir('C:/Users/user/Desktop/sondönem/bitirme/outputs/hyp'):\n",
    "        try:\n",
    "            open('hyp/' + f).read()\n",
    "        except:\n",
    "            error_id.append(f)\n",
    "    error_set = set(error_id)\n",
    "    for f in error_set:\n",
    "        #os.remove('ref/' + f)\n",
    "        os.remove('hyp/' + f)\n",
    "\n",
    "def rouge():\n",
    "    r = Rouge155()\n",
    "    r.home_dir = '.'\n",
    "    r.system_dir = 'C:/Users/user/Desktop/sondönem/bitirme/outputs/hyp'\n",
    "    r.model_dir =  'C:/Users/user/Desktop/sondönem/bitirme/outputs/ref'\n",
    "    \n",
    "    r.system_filename_pattern = '(\\d+).txt'\n",
    "    r.model_filename_pattern = '#ID#.txt'\n",
    "\n",
    "    command = '-e C:/ROUGE-1.5.5/data -a -c 95 -m -n 2 -b 75'\n",
    "    output = r.convert_and_evaluate(rouge_args=command)\n",
    "    print(\"we done\")\n",
    "    print(output)\n",
    "def rougesigir():\n",
    "    r = Rouge155()\n",
    "    r.home_dir = '.'\n",
    "    r.system_dir = 'C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirhyp'\n",
    "    r.model_dir =  'C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirref'\n",
    "    \n",
    "    r.system_filename_pattern = '(\\d+).txt'\n",
    "    r.model_filename_pattern = '#ID#.txt'\n",
    "\n",
    "    command = '-e C:/ROUGE-1.5.5/data -a -c 95 -m -n 2 -b 75'\n",
    "    output = r.convert_and_evaluate(rouge_args=command)\n",
    "    print(\"we done\")\n",
    "    print(output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #remove_broken_files()\n",
    "    #rouge()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "loved-helena",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'string' has no attribute 'maketrans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3169c9fb0edd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtrainx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtestx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigirdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-271eef0976e9>\u001b[0m in \u001b[0;36msigirdata\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m                             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                             \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                             \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\"\"'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\"\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                             \u001b[0mfinalstr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinalstr\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                             \u001b[0mmydict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"summaries\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinalstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'string' has no attribute 'maketrans'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "#print(len(sigirdata()))\n",
    "kfold = KFold(5, True, 1)\n",
    "# enumerate splits\n",
    "counter=0\n",
    "trainx=[]\n",
    "testx=[]\n",
    "data=sigirdata()\n",
    "\n",
    "for train, test in kfold.split(data):\n",
    "    for t in range (0,len(train)):\n",
    "        val=data[train[t]]\n",
    "        \n",
    "        trainx.append(val)\n",
    "        with open('sigirtrain.json', 'w') as fp:\n",
    "            fp.write(\n",
    "                '[' +\n",
    "                ',\\n'.join(json.dumps(i) for i in trainx) +\n",
    "                ']\\n')\n",
    "        with open('sigirval.json', 'w') as fp:\n",
    "            fp.write(\n",
    "                '[' +\n",
    "                ',\\n'.join(json.dumps(i) for i in trainx[90:100]) +\n",
    "                ']\\n')\n",
    "    for i in range (0,len(test)):\n",
    "        val=data[test[i]]\n",
    "        testx.append(val)\n",
    "        with open('sigirtest.json', 'w') as fp:\n",
    "            fp.write(\n",
    "                '[' +\n",
    "                ',\\n'.join(json.dumps(i) for i in testx) +\n",
    "                ']\\n')\n",
    "    print(len(testx),\"sended to test\")\n",
    "    print(len(trainx),\"sended to train\")\n",
    "    #print(type(trianx[]))\n",
    "    \n",
    "    trainsigir()\n",
    "    testsigir()\n",
    "    try:\n",
    "        rougesigir()\n",
    "    except Exception:\n",
    "        pass\n",
    "    sleep(30)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d18bdb-4859-4273-a4db-13e504b46dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=sigirdata()\n",
    "print(\"dsad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "nervous-opportunity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc': 'the transition to web 2.0 transformed the business models of online marketing from a global ad approach based to individual opinions and targeted campaigns [2, 23, 35, 40]\\nweb 2.0 not only took traditional marketing strategies to the extreme via viral marketing campaigns [31, 36, 43], but it also gave rise to new techniques of brand building and audience targeting via influencer marketing [12, 44].\\nin fact, the use of micro-influencers, trusted individuals within their communities, has been seen as a more effective way to build a brand in terms of audience reception and return on investment [9, 25, 32]\\ninstagram, which is a visual content sharing online social network (osn), has become a focal point for influencer marketing\\nwith power users and micro-influencers publishing sponsored content companies need to rate these influencers and determine their value[17, 18, 38].\\nmost of today’s scoring themes rely on graphbased algorithms of a known network graph.\\nsuch graphs are not always available, and building them for instagram users requires a great deal of resources, e.g., crawling time and computing costs\\n a possible solution would be to infer the underlying network structure using the user activity logs, as described by barbieri et al.[7], but even in the event a graph is constructed it would not necessarily be of much use given that information decays exponentially along the graph even under optimal passive information propagation, which is not the case\\nthe rest of the paper is organized as follows: in section 2 we described osns in greater detail aswell as current influence measuring schemes\\nwe then present our annotations and formal description of the problem of measuring and ranking influence in section 3.\\nthe dataset of instagram users and their posts is described in section 4, followed by discussion on the extracted and aggregated features of the testable data in section 4.2\\nfollowing this, we present our testing methodology, baselines, regression models and experimental results in section 6\\nfinally, we discuss our conclusion and possible future work in section ', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'This paper focuses on the problem of scoring and ranking influential users of Instagram, a visual content sharing online social network (OSN)\\n Instagram is the second largest OSN in the world with 700 million active Instagram accounts, 32% of all worldwide Internet users1\\nAmong the millions of users, photos shared by more influential users are viewed by more users than posts shared by less influential counterparts\\nThis raises the question of how to identify those influential Instagram users\\nIn our work, we present and discuss the lack of relevant tools and insufficient metrics for influence measurement, focusing on a network oblivious approach and show that the graph-based approach used in other OSNs is a poor fit for Instagram.\\nIn our study, we consider user statistics, some of which are more intuitive than others, and several regression models to measure users’ influenc'}\n",
      "\n",
      "\n",
      "{'doc': 'linkedin and lagou, has enabled the new paradigm for talent recruitment\\n for instance, in 2017, there are 467 million users and 3 million active job listings in linkedin from about 200 countries and territories all over the world [3]\\nwhile popular online recruitment services provide more convenient channels for both employers and job seekers, it also comes the challenge of person-job fit due to information explosion\\naccording to the report [23], the recruiters now need about 42 days and $4,000 dollars in average for locking a suitable employee [23]\\nclearly, more effective techniques are urgently required for the person-job fit task, which targets at measuring the matching degree between the talent qualification and the job requirements\\nindeed, as a crucial task for job recruitment, person-job fit has been well studied from different perspectives, such as job-oriented skill measuring [34], candidate matching [22] and job recommendations [20, 27, 38]\\nalong this line, some related tasks, such as talent sourcing [33, 39] and job transition [31] have also been studied\\nhowever, these efforts largely depend on the manual inspection of features or key phrases from domain experts, and thus lead to high cost and the inefficient, inaccurate, and subjective judgments\\nto this end, in this paper, we propose an end-to-end abilityaware person-job fit neural network (apjfnn) model, which has a goal of reducing the dependence on human labeling data and can provide better interpretation about the fitting results\\nthe key idea of our approach is motivated by the example shown in figure 1\\nthere are 4 requirements including 3 technical skill (programming, machine learning and big data processing) requirements and 1 comprehensive quality (communication and team work) requirement\\nsince multiple abilities may fit the same requirement and different candidates may have different abilities, all the abilities should be weighed for a comprehensive score in order to compare the matching degree among different candidates\\nduring this process, traditional methods, which simply rely on keywords/feature matching, may either ignore some abilities of candidates, or mislead recruiters by subjective and incomplete weighing of abilities/experiences from domain experts\\ntherefore, for developing more effective and comprehensive person-job fit solution, abilities should be not only represented via the semantic understanding of rich textual information from large amount of job application data, but also automatically weighed based on the historical recruitment results\\nalong this line, all the job postings and resumes should be comprehensively analyzed without relying on human judgement\\nto be specific, for representing both the job-oriented abilities and experiences of candidates, we first propose a word-level semantic representation based on recurrent neural network (rnn) to learn the latent features of each word in a joint semantic space\\nthen, two hierarchical ability-aware structures are designed to guide the learning of semantic representation for job requirements as well as the corresponding experiences of candidates\\nin addition, for measuring the importance of different abilities, as well as the relevance between requirements and experiences, we also design four hierarchical ability-aware attention strategies to highlight those crucial abilities or experience\\nthis scheme will not only improve the performance, but also enhance the interpretability of matching results\\nfinally, extensive experiments on a large-scale real-world data set clearly validate the effectiveness of our apjfnn framework compared with several baselines\\noverview\\nthe rest of this paper is organized as follows\\nin section 2, we briefly introduce some related works of our study\\nin section 3, we introduce the preliminaries and formally define the problem of person-job fit\\nthen, technical details of our abilityaware person-job fit neural network will be introduced in section 4\\nafterwards,we comprehensively evaluate the model performance in section 5, with some further discussions on the interpretability of results\\nfinally, in section 6, we conclude the pape', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'The wide spread use of online recruitment services has led to information explosion in the job market\\nAs a result, the recruiters have to seek the intelligent ways for Person-Job Fit, which is the bridge for adapting the right job seekers to the right positions\\nExisting studies on Person-Job Fit have a focus on measuring the matching degree between the talent qualification and the job requirements mainly based on the manual inspection of human resource experts despite of the subjective, incomplete, and inefficient nature of the human judgement\\nTo this end, in this paper, we propose a novel end-to-end Ability-aware Person-Job Fit Neural Network (APJFNN) model, which has a goal of reducing the dependence on manual labour and can provide better interpretation about the fitting results\\nThe key idea is to exploit the rich information available at abundant historical job application data\\nSpecifically, we propose a word-level semantic representation for both job requirements and job seekers’ experiences based on Recurrent Neural Network (RNN)\\nAlong this line, four hierarchical ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measuring the different contribution of each job experience to a specific ability requirement\\nFinally, extensive experiments on a large-scale realworld data set clearly validate the effectiveness and interpretability of the APJFNN framework compared with several baseline'}\n",
      "\n",
      "\n",
      "{'doc': 'fake news, misinformation, rumor or hoaxes are one of the most concerning problems due to their popularity and negative effects on society\\nparticularly, social networking sites (e.g., twitter and facebook) have become a medium to disseminate fake news\\ntherefore, companies and government agencies have paid attention to solving fake news\\nfor example, facebook has a plan to combat fake news and the fbi has investigated disinformation spread by russia and other countries\\nto verify correctness of information, researchers proposed to (i) employ experts, who can fact-check information [59], (ii) use systems that can automatically check credibility of news [19, 33, 46]; and build models to detect fake news [7, 24, 35, 42, 53]\\nin 2016, reporter lab reported that the number of fact-checking websites went up by 50%3\\nhowever, fake news is still wildly disseminated on social media even when it has been debunked [36, 58]\\na recent report [25] showed that 86% of american adults do not fact-check articles they read\\na possible explanation for this is that people may trust content shared from their friends rather than other sources [25] or they may not have time to fact-check articles they read, or simply they may not know the existence of these fact-check websites\\nit means that merely debunking fake news is not enough, and these systems are not fully utilized\\nfurthermore, it has been shown that once absorbing misinformation from fake news, individuals are less likely to change their beliefs even when the fake news are debunked\\nif the idea in the original fake news is especially similar to individuals’ viewpoints, it will be even harder to change their minds [12, 40]\\ntherefore, it is needed to deliver verified information quickly to online users before fake news reaches them\\n   to achieve this aim, the volume of verified content should be large enough on social networks, so that online users may have a higher chance to be exposed to legitimate information before consuming fake news from other sources\\nin this paper, we propose a framework to further utilize factchecked content\\nparticularly, we collect a group of people and stimulate them to disseminate fact-checked content to other users\\nhowever, achieving the goal is challenging because we have to solve the two following problems:\\n(p1) how can we find a group of people (e.g. online users) who are willing to spread verified news?\\n(p2) how can we stimulate them to disseminate fact-checked news/information\\nto deal with the first problem (p1), we may deploy bots [27, 49] to disseminate information but it may violate terms of services of online platforms due to abusing behavior.\\nanother approach is to hire crowd workers [29] and cyber troops to shape public opinion [5]\\nhowever, this approach may cost a lot of money and is difficult to deploy in larger scale due to monetary constraints\\ninspired by [18], we propose to rely on online users called guardians, who show interests in correcting false claims and fake news in online discussions by embedding fact-checking urls\\nfigure 1 illustrates who a guardian is and helps us to describe terminologies that we use in this paper\\nin the figure, two twitter users have a conversation, in which a user @sir_mycroft accused the clinton foundation of accepting money from uranium one company in exchange for the approval of the deal between uranium one and russian government in 2009\\nafter just 15 minutes, this false accusation was debunked by a user @politics_pr, who referred to factcheck.org and snopes.com urls as evidences to support his factual correction\\nwe call such direct replies, which contain factchecking urls, direct fact-checking tweets (d-tweets)\\nusers, who posted d-tweets, are called direct guardians (d-guardians)\\nthe user, to whom the d-guardian replied (i.e. @sir_mycroft), is called an original poster\\nin addition, we observed that @politics_pr’s response was retweeted 15 times\\nwe call these retweeters secondary guardians (s-guardians), regardless of whether they added a comment or not inside the retweet\\ntheir shares are called secondary tweets (s-tweets)\\nboth d-guardians and s-guardians are called guardians, and both d-tweets and s-tweets are called fact-checking tweets.\\nin section 4, we investigate whether both d-guardians and s-guardians play an important role in correcting claims and spreading fact-checked information\\nto cope with the second problem (p2), we may directly ask the guardians to spread verified news like [28], but their response rate may be low because each guardian may be interested in different topics, and eventually, we may send unwanted requests to some of the guardians\\nthus, we tackle the second problem by proposing a fact-checking url recommendation model\\nby providing personalized recommendations, we may stimulate guardians’ engagement in fact-checking activities toward spreading credible information to many other users and reducing the negative effects of fake news\\nby addressing these two problems, we collect a large number of reliable guardians and propose a fact-checking url recommendation model which exploits recent success in embedding techniques [32] and utilizes auxiliary data to personalize fact-checking urls for the guardians.\\nour main contributions are as follows\\nwe are the first work to utilize guardians, who can help spread credible information and recommend fact-checking urls to the guardians as a pro-active way to combat fake news\\nwe thoroughly analyze who guardians are, their temporal behavior, and topical interests\\nwe propose a novel url recommendation model, which exploits fact-checking urls’ content (i.e., linked fact-checking pages), social network structure, and recent tweets’ content\\nwe evaluate our proposed model against four state-of-the-art recommendation algorithms\\nexperimental results show that our model outperforms the competing models by 11%~33', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'A large body of research work and efforts have been focused on detecting fake news and building online fact-check systems in order to debunk fake news as soon as possible\\nDespite the existence of these systems, fake news is still wildly shared by online users.\\nIt indicates that these systems may not be fully utilized\\nAfter detecting fake news, what is the next step to stop people from sharing it? How can we improve the utilization of these fact-check systems\\nTo fill this gap, in this paper, we (i) collect and analyze online users called guardians, who correct misinformation and fake news in online discussions by referring fact-checking URLs; and (ii) propose a novel fact-checking URL recommendation model to encourage the guardians to engage more in fact-checking activities\\nWe found that the guardians usually took less than one day to reply to claims in online conversations and took another day to spread verified information to hundreds of millions of followers\\nOur proposed recommendation model outperformed four state-of-the-art models by 11%∼33%.\\n Our source code and dataset are available at http://web.cs.wpi.edu/~kmlee/data/gau.htm'}\n",
      "\n",
      "\n",
      "{'doc': 'with the increasing enthusiasm of users to share their daily life on social networks, a large amount of personal data, such as personal demographics, daily activities and even relations with the others, are made publicly available.\\nit is reported that 66% of users’ micro-posts are about themselves [24]\\nthe huge amount of users’ personal data accessible online may put the users at a high risk of privacy leakage due to the following reasons\\non one hand, the default privacy settings usually make ugcs publicly accessible\\nin fact, people are usually connected with heterogeneous circles on social networks, such as family members, casual friends and even strangers\\nas a result, ugcs are probably seen by unexpected audience and hence cause unexpected consequences to users\\ntake a real story as an example\\na video podcaster’s home was broken into and several video equipments were stolen during his travel\\nit is ultimately found out that the break-in was caused by his detailed tweets regarding his leave [24]\\non the other hand, users may even be unaware of the privacy leakage when they are posting on social networks, which is also the cause of the regrettable messages [36]\\nconsequently, privacy leakage via user-generated contents (ugcs) in social networks deserves our special attention\\nin fact, according to the report [35], 50% of internet users are concerned about the privacy exposure, up from about 30% in 2009\\nprivacy is elaborated as a process of boundary regulation [13, 34], where individuals control over how much information about themselves can be divulged to others\\ntherefore, maintaining appropriate levels of disclosure within one’s social environment is of essential significance\\nin fact, one’s social circle can be organized into different groups based on their personal ties with the given user\\nit is apparent that for different social circles, individuals hold different norms of what kind of information should be treated as privacy\\nfor example, one’s age may be kept private to his/her casual friends but visible to family members, while one’s negative emotion may be better invisible to family members\\nconsidering that information and audience both play pivotal roles in the privacy preserving, answering the question of who can see what is essential\\nhowever, answering who can see what is non-trivial due to the following reasons\\nfirstly, the personal aspects of users conveyed by their posts are usually not independent but can be organized into certain structures, such as groups, according to their relatedness\\nfor example, given a set of aspects i = {age, current location, places planning to go}, aspects “current location” and “places to go” are more correlated and should be modeled together in one group\\nmore often than not, such structure can impose certain constraints to the feature space and enhance the performance of aspect detection\\nconsequently, the main challenge is how to construct and leverage such structure to learn shared features and specific features\\nsecond, thus far, no gold standard instruction is available to guide who can see what\\nas the interpretation of privacy may be subjective and geographically specific, obtaining a unified instruction poses a crucial challenge for us\\nthe third challenge lies in the lack of benchmark dataset and the way to extract a set of privacy-oriented features\\nthis is because it is hard to distinguish the personal posts from the non-personal posts, and some posts are too short to provide sufficient contexts for feature extraction\\nto address the aforementioned challenges, we present a novel scheme for boundary regulation, comprising of three components: description, prediction, and prescription\\nas illustrated in figure 1, in the first component, we summarize the literature and pre-define a comprehensive taxonomy composed of 32 categories, where each category corresponds to one personal aspect of users\\nto build a benchmark dataset, we then feed a list of keywords to twitter search service for each category\\na set of privacy-oriented features, including linguistic and meta features are extracted to describe the given ugcs\\nwe choose the real-time sharing website twitter as the study platform due to the following facts\\n1) users in twitter are keen to share their personal events of various topics;\\nand 2) the followers are broadly and disorderly mixed.\\nbased on these features, the second component then endeavors to discover which personal aspect has been uncovered by the given post\\nthe predefined structure in the first component has organized the 32 categories into eight groups, spanning from personal attributes to life milestones\\nthe categories within each group hold both group-sharing features and aspect-specific features\\nmeanwhile, we assume that there is a low dimensional latent feature space that is capable of capturing the higher-level semantics of ugcs as compared to the original features\\nto learn the latent feature space and further boost the aspect detection performance, we treat each personal aspect as a task and propose a latent group multi-task learning (token) model that is able to leverage the pre-defined structure to learn group-sharing latent features and aspect-specific latent features simultaneously\\nthe last component works towards triggering and suggesting users what they should act according to certain guidelines once their privacy leakage is detected by the second component\\nconsidering the existence of cultural difference regarding users’ information disclosure norms, we build guidelines by conducting a cross-cultural user study via amazon mechanical turk (amt)\\nin designing this guideline, we regulate the boundary of users’ posts by four tier social circles, namely, family members, close friends, casual friends and outsider audience\\nour main contributions can be summarized in threefold\\nwe established a taxonomy to comprehensively characterize users’ personal aspects\\nguided by this taxonomy, we proposed a token model to uncover the personal aspects disclosed by the user’s posts\\nregarding the optimization, we theoretically relaxed the non-smooth model to a smooth one and derived its closed-form solution\\nwe constructed guidelines regarding users’ information disclosure norms with four kinds of social circles\\nthis user study with 400 users cannot be finished without the help of the crowdsourcing internet marketplace—amt\\nin addition, we studied the cultural similarities and differences of users’ privacy perception\\nwe collected a representative dataset via twitter search service and developed a rich set of privacy-oriented features\\nwe have released the data to facilitate others to repeat experiments and verify their ideas\\nthe remainder of this paper is structured as follows, section 2 briefly reviews the related work\\nsections 3, 4 and 5 present the three components of token model, namely, description, prediction and prescription, respectively\\nsection 6 details the experimental results and analyses, followed by our concluding remarks and future work in section ', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'The booming of social networks has given rise to a large volume of user-generated contents (UGCs), most of which are free and publicly available\\nA lot of users’ personal aspects can be extracted from these UGCs to facilitate personalized applications as validated by many previous studies\\nDespite their value, UGCs can place users at high privacy risks, which thus far remains largely untapped\\nPrivacy is defined as the individual’s ability to control what information is disclosed, to whom, when and under what circumstances\\nAs people and information both play significant roles, privacy has been elaborated as a boundary regulation process, where individuals regulate interaction with others by altering the openness degree of themselves to others\\nIn this paper, we aim to reduce users’ privacy risks on social networks by answering the question of Who Can See What\\nTowards this goal, we present a novel scheme, comprising of descriptive, predictive and prescriptive components\\nIn particular, we first collect a set of posts and extract a group of privacy-oriented features to describe the posts\\nWe then propose a novel taxonomy-guided multi-task learning model to predict which personal aspects are uncovered by the posts\\nLastly, we construct standard guidelines by the user study with 400 users to regularize users’ actions for preventing their privacy leakage\\nExtensive experiments on a real-world dataset well verified our schem'}\n",
      "\n",
      "\n",
      "{'doc': 'motivation and problem\\nrankings of subjects like people, hotels, or songs are at the heart of selection, matchmaking and recommender systems\\nsuch systems are in use on a variety of platforms that affect different aspects of life – from entertainment and dating all the way to employment and income\\nnotable examples of platforms with a tangible impact on people’s livelihood include two sided sharing economy websites, such as airbnb or uber, or human resource matchmaking platforms, such as linkedin or taskrabbit. \\nthe ongoing migration to online markets and the growing dependence of many users on these platforms in securing an income have spurred investigations into the issues of bias, discrimination and fairness in the platforms’ mechanisms. [5, 26]\\none aspect in particular has evaded scrutiny thus far – to be successful on these platforms, ranked subjects need to gain the attention of searchers.\\nsince exposure on the platform is a prerequisite for attention, subjects have a strong desire to be highly ranked\\nhowever, when inspecting ranked results, searchers are susceptible to position bias, which makes them pay most of their attention to the top-ranked subjects\\nas a result, lower-ranked subjects often receive disproportionately less attention than they deserve according to the ranking relevance.\\nposition bias has been studied in information retrieval in scenarios where subjects are documents such as web pages (e.g., [8, 10]).\\nit has been shown that top-ranked documents receive most clicks often irrespective of their actual relevance [21]. \\nsystemic correction for the bias becomes important when ranking positions potentially translate to financial gains or losses\\n this is the case when ranking people on platforms like linkedin or uber, products on platforms like amazon, or creative works on platforms like spotify\\nfor example, cumulating the exposure on a subset of drivers in ride-hailing platforms might lead to economic starvation of others, while low-ranked artists on music platforms might not get their deserved chance of earning royalties. \\nobserving that attention is influenced by a human perception bias, while relevance is not, uncovers a fundamental problem: there necessarily exists a discrepancy between the attention that subjects receive at their respective ranks and their relevance in a given search task\\nfor example, attention could decrease geometrically, whereas relevance scores may decrease linearly as the rank decreases\\nif a ranking is displayed unchanged to many searchers over time, the lower-ranked subjects might be systematically and repeatedly disadvantaged in terms of the attention they receive\\nproblem statement\\na vast body of ranking models literature has focused on aligning system relevance scores with the true relevance of ranked subjects, and in this paper we assume the two are proportional.\\nwhat we focus on instead is the relation between relevance and attention\\nsince relevance can be thought of as a proxy for worthiness in the context of a given search task, the attention a subject receives from searchers should ideally be proportional to her relevance\\nin economics and psychology, a similar idea of proportionality exists under the name of equity [31] and is employed as a fairness principle in the context of distributive justice [17]\\n     thus, in this paper, we make a translational normative claim and argue for equity of attention in rankings\\noperationally, the problem we address in this paper is to devise measures and mechanism which ensure that, for all subjects in the system, the received attention approximately equals the deserved attention, while preserving ranking quality\\n for a single ranking this goal is infeasible, since attention is influenced by the position bias, while relevance is not\\ntherefore, our approach looks at a series of rankings and aims at measures of amortized fairness. \\nstate of the art and limitations\\n fairness has become a major concern for decision-making systems based on machine learning methods (see, e.g., [9, 29])\\nvarious notions of group fairness have been investigated [14, 20, 23, 28, 35], with the goal of making sure that protected attributes such as gender or race do not influence algorithmic decisions.\\nfair classifiers are then trained to maximize accuracy subject to group fairness constraints\\nthese approaches, however, do not distinguish between different subjects from within a group.\\nthe notion of individual fairness [12, 24, 37] aims at treating each individual fairly by requiring that subjects who are similar to each other receive similar decision outcomes\\n  for instance, the concept of meritocratic fairness requires that less qualified candidates are almost never preferred over more qualified ones when selecting candidates from a set of diverse populations\\nrelevance based rankings, where more relevant subjects are ranked higher than less relevant ones, also satisfy meritocratic fairness\\na stronger fairness concept, however, is needed for rankings to be a means of distributive justice. \\n     prior work on fair rankings is scarce and includes approaches that perturb results to guarantee various types of group fairness\\nthis goal is achieved by techniques similar to those for ranking result diversification [6, 34, 36], or by granting equal ranking exposure to groups [30]\\n individual fairness is inherently beyond the scope of group-based perturbation\\napproach and contribution\\nour approach in this paper differs from the prior work in two major ways.\\nfirst, the measures introduced here capture fairness at the level of individual subjects, and subsume group fairness as a special case\\nsecond, as no single ranking can guarantee fair attention to every subject, we devise a novel mechanism that ensures amortized fairness, where attention is fairly distributed across a series of rankings. \\nfor an intuitive example, consider a ranking where all the relevance scores are almost the same\\nsuch tiny differences in relevance will push subjects apart in the display of the results, leading to a considerable difference in the attention received from searchers\\nto compensate for the position bias, we can reorder the subjects in consecutive rankings so that everyone who is highly relevant is displayed at the top every now and then\\nour goal is not just to balance attention, but to keep it proportional to relevance for all subjects while preserving ranking quality\\nto this end, we permute subjects in each ranking so as to improve fairness subject to constraints on quality loss\\n we cast this approach to an online optimization problem, formalizing it as an integer linear program (ilp)\\n we moreover devise filters to prune the combinatorial space of the ilp, which ensures that it can be solved in an online system.\\nexperiments with synthetic and real-life data demonstrate the viability of our method\\nthis paper makes the following novel contributions\\nto the best of our knowledge, we are the first to formalize the problem of individual equity-of-attention fairness in rankings, and define measures that capture the discrepancy between the deserved and received attention. \\nwe propose online mechanisms for fairly amortizing attention over time in consecutive rankings\\nwe investigate the properties and behavior of the proposed mechanisms in experiments with synthetic and real-world da', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'Rankings of people and items are at the heart of selection-making, match-making, and recommender systems, ranging from employment sites to sharing economy platforms\\nAs ranking positions influence the amount of attention the ranked subjects receive, biases in rankings can lead to unfair distribution of opportunities and resources such as jobs or income. \\nThis paper proposes new measures and mechanisms to quantify and mitigate unfairness from a bias inherent to all rankings, namely, the position bias which leads to disproportionately less attention being paid to low-ranked subjects\\nOur approach differs from recent fair ranking approaches in two important ways.\\nFirst, existing works measure unfairness at the level of subject groups while our measures capture unfairness at the level of individual subjects, and as such subsume group unfairness\\nSecond, as no single ranking can achieve individual attention fairness, we propose a novel mechanism that achieves amortized fairness, where attention accumulated across a series of rankings is proportional to accumulated relevance\\nWe formulate the challenge of achieving amortized individual fairness subject to constraints on ranking quality as an online optimization problem and show that it can be solved as an integer linear program\\nOur experimental evaluation reveals that unfair attention distribution in rankings can be substantial, and demonstrates that our method can improve individual fairness while retaining high ranking qualit'}\n",
      "\n",
      "\n",
      "{'doc': 'the use of ir methodologies and metrics for the evaluation of recommender systems has spread in recent years and is becoming common practice in the area, under the understanding of recommendation as a ranking task [14]\\nyet ir metrics have been found to be strongly biased towards rewarding algorithms that recommend popular items, that is, items that many people know, like, rate or interact with [4,21,35]\\nat the same time, state of the art recommendation algorithms have similarly been found to display a marked bias towards recommending items most people like [21]\\nthis may naturally cast doubt on the reliability of common experiments and the outcome on which the best algorithms really are\\nthis problem has been of no particular concern to ir methodology, as popularity biases do not occur, or not in such a dramatic way, in traditional search and ir tasks\\nthe popularity bias is so strong in common datasets for recommender system evaluation that even a pure and simple popularity ranking appears to achieve suboptimal but non-negligible recommendation accuracy compared to the best state of the art personalized algorithms [14]\\nand it is in fact not necessarily trivial to outperform, for instance, in high rating sparsity conditions\\nresearch has therefore been recently undertaken addressing the issue, so far mainly focusing on confirming and measuring the popularity biases, and removing them [4,21,34,35].\\nbut a basic question remains yet unanswered: is the popularity bias actually something we should get rid of at all?\\nif recommending popular items happened to be the right thing to do, then should not both the evaluation metrics and the recommendation algorithms rightfully favor them? \\nthe majority opinion is indeed useful information for people –it is a simple yet fair and useful default criterion we keep in sight most of the time through our human decisions, even when we do not follow it\\nand we in fact often do adopt it, for instance, in the absence of enough evidence to form one’s own personal choice, or as guidance to reduce the cost of building a decision from scratch, or as a social learning mechanism [3]\\nfrom an application point of view, a recommendation based on the choices of many can be acceptable in many circumstances [16] –and requires minimum development skills and maintenance costs\\nit is actually a widespread approach that many applications display in the form of top charts, best-selling lists, average people’s ratings, etc\\neven in the presence of a full-fledged personalized recommender system, majority listings are still a good resort for new or cold users\\nthe effectiveness of majority taste makes indeed statistical sense: the items that many people like (according to the records of observed user activity) are liked by many people (in test data for evaluation) [19]\\nyet from an experimental perspective, if the observations are somehow biased, and the bias is consistent across training to test data, the majority bias in recommendation might be accurately guessing where the observations have been placed by the experimenter, rather than where true user tastes are being actually most satisfied\\nmoreover, the majority signal might be contaminated by trends that deviate from actual user appreciation [5,29]\\nrecent studies show that majority formation involves a degree of chance, by which different outcomes are possible as to what choices make it to the top of popularity [31]\\ncrowd dynamics are moreover known to be exposed to external and internal influence and bias factors [26,27,29], such as mass media [7], marketing, opinion management [6], algorithmic bias [28], or social conformity [13]\\nthe issue is therefore open whether or not popularity is a truly effective ingredient to achieve accurate recommendations, to what extent and in what cases, and whether we are measuring it properly\\nwe address the question by considering, analyzing and comparing two views on ir metrics: biased and unbiased\\nthe former represents what is measured in common offline experiments in the literature, where relevance information is missing not at random (mnar) [23,24,25,34,35], and the latter represents the true metric value that would be obtained if the missing information became available. \\nwe do this at both a theoretical and an empirical level. at the analytical level, we formulate a probabilistic expression of the problem\\nstarting by a revised probability ranking principle [30] for recommender systems, we analyze popularity-based recommendation by comparison to the optimal ranking\\nwe find that the effectiveness or ineffectiveness of popularity depends on the interplay of three main variables: item relevance, item discovery by users, and the decision by users to interact with discovered items\\nwe identify the key probabilistic dependencies among these factors that determine the outcome for popularity, and we characterize a set of trends defined by different independence assumptions, each resulting in a particular pattern of behavior for popularity\\nwe back our theoretical findings with empirical observations with a dataset we build on a crowdsourcing platform, in which we remove several of the common biases of publicly available datasets\\namong other findings, we prove and illustrate qualitative contradictions between the accuracy that is measured in a common offline experimental setting, and the actual accuracy that can be estimated with unbiased observations\\nwe identify conditions that guarantee popularity to be a safe element in recommendation, and we characterize and exemplify situations where, on the contrary, popularity can be a totally misleading direction to follow, to the point of leading to worse effectiveness than random recommendation\\n       we furthermore find that the average rating can be more effective than the number of ratings as a trend to follow in recommendation in many cases, contrarily to what the biased metric values suggest –which represent what the literature commonly reports [14,21].\\nfinally, we look at the signification our findings can have in personalized collaborative filtering algorithms. ', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'The use of IR methodology in the evaluation of recommender systems has become common practice in recent years\\nIR metrics have been found however to be strongly biased towards rewarding algorithms that recommend popular items –the same bias that state of the art recommendation algorithms display\\nRecent research has confirmed and measured such biases, and proposed methods to avoid them\\nThe fundamental question remains open though whether popularity is really a bias we should avoid or not; whether it could be a useful and reliable signal in recommendation, or it may be unfairly rewarded by the experimental biases\\nWe address this question at a formal level by identifying and modeling the conditions that can determine the answer, in terms of dependencies between key random variables, involving item rating, discovery and relevance\\nWe find conditions that guarantee popularity to be effective or quite the opposite, and for the measured metric values to reflect a true effectiveness, or qualitatively deviate from it\\nWe exemplify and confirm the theoretical findings with empirical results.\\n  We build a crowdsourced dataset devoid of the usual biases displayed by common publicly available data, in which we illustrate contradictions between the accuracy that would be measured in a common biased offline experimental setting, and the actual accuracy that can be measured with unbiased observations'}\n",
      "\n",
      "\n",
      "{'doc': 'search tasks are a central component of interactive information retrieval (iir)\\nas noted by toms [21], search tasks play two important roles in iir research\\nfirst, they serve as a vehicle for research\\nin iir studies, experimenters must assign search tasks to study participants in order to observe their behaviors and evaluate systems\\nsecond, search tasks are also often used as the object of study (i.e., as independent variables)\\nfrom this perspective, the study of search tasks helps us understand how task characteristics translate to specific challenges faced by searchers, and informs the design of novel tools to support users\\na large body of research has focused on understanding how search tasks vary along different dimensions, including the search task’s main activity (e.g., searching vs. browsing), goal (e.g., well defined vs. amorphous), and structure (e.g., task complexity) [16]\\nsearch task complexity is one characteristic that has received considerable attention in recent work, and has been found to influence search behaviors and outcomes [3, 5, 7, 8, 11, 13, 25]\\ntask complexity is itself a complicated construct that has been studied from different perspectives [24]\\none influential approach initially proposed by byström and järvelin [5] is to view task complexity through the lens of a priori determinability.\\nthe a priori determinability of a task is defined by the degree of uncertainty about the task’s required outcomes and the processes involved in gathering the information  needed to complete the task\\n a task with low determinability (i.e., high complexity) is one with high uncertainty about the form of the solution and the processes involved in solving the task\\nin this work, we aimed to manipulate the determinability of search tasks indirectly, by manipulating the scope of the task (i.e., open-ended versus narrowly focused)\\nin order to control for other task characteristics, we focused on comparative search tasks\\nfor example, one of our tasks asked participants to compare different fertilizers for a home garden\\ncomparative tasks involve two important activities: (1) identifying different items or alternatives for the given category (e.g., organic, synthetic, liquid fertilizers) and (2) understanding how the items differ along different dimensions or attributes (e.g., cost, nutrient content, health concerns)\\nwe manipulated the task scope by including or excluding specific items and/or dimensions for participants to consider as part of the comparative task\\nour most open-ended tasks did not mention specific items or dimensions to consider\\nin contrast, our most narrowly focused tasks instructed participants to consider two specific items and one dimension\\nadditionally, we studied two types of dimensions: objective and subjective\\nwe expected that addressing a subjective dimension would involve greater uncertainty (i.e., lower determinability)\\nfor example, a subjective dimension might require gathering information from different perspectives and evaluating the credibility of information. \\nwe report on a crowdsourced study (n = 144) that investigated the effects of our task manipulation on participants’ perceptions about the task, search behaviors and strategies, and level of engagement during the task\\nwe developed 12 task topics and 6 task versions per topic\\ntask version was our main independent variable and varied based on the specification of items and/or (objective or subjective) dimensions that should be considered during the comparative search task\\nwe used a within-subject design; each participant completed six search tasks (one per task version)\\nour study investigates the following five research questions: \\nrq1 rq2: what is the effect of task version on participants’ pre-(rq1) and post-task (rq2) perceptions about the task\\nwe focus on perceptions related to determinability, subjectivity, prior knowledge/knowledge increase, interest/interest increase, and expected/ experienced difficulty\\nrq3: what is the effect of task version on participants’ level of engagement during the task?\\nwe measured aspects of engagement using o’brien’s user engagement scale [18]\\nrq4: what is the effect of task version on participants’ search behaviors\\nwe examined measures associated with search effort and the extent to which participants diverged from each other in their choice of queries and clicks\\nrq5: what is the effect of task version on participants’ search strategies\\nthrough a qualitative analysis of participants’ queries, we investigated the differences in querying strategies observed for different task versions\\nwe build on our previous work to investigate the relationships between task scope, determinability, and searchers’ perceptions and behaviors [8].\\nin this new study, we investigate differences between specifying objective and subjective dimensions in the task description, explore the effects of our task manipulation on user engagement, and present an analysis of participants’ queries to explain why or how certain task versions affected task performanc', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': ' An important area of IR research involves understanding how task characteristics influence search behaviors and outcomes.\\nTask complexity is one characteristic that has received considerable attention\\nOne view of task complexity is through the lens of a priori determinability—the level of uncertainty about task outcomes and processes experienced by the searcher\\nIn this work, we manipulated the determinability of comparative tasks\\nOur task manipulation involved modifying the scope of the task by specifying exact items and/or exact (objective or subjective) dimensions to consider as part of the task\\nThis paper reports on a within-subject study (N = 144) where we investigated how our task manipulation influenced participants’ perceptions, levels of engagement, search effort, and choice of search strategies\\n Our results suggest a complex relationship between task scope, determinability, and different outcome measures\\nOur most open-ended tasks were perceived to have low determinability (high uncertainty), but were the least challenging for participants due to satisficing\\nFurthermore, narrowing the scope of tasks by specifying items had a different effect than by specifying dimensions\\nSpecifying items increased the task determinability (lower uncertainty) and made the task easier, while specifying dimensions did not increase the task determinability and made the task more challenging\\n A qualitative analysis of participants’ queries suggests that searching for dimensions is more challenging than for items\\nFinally, we observed subtle differences between objective and subjective dimensions\\n We discuss implications for the design of IIR studies and tools to support user'}\n",
      "\n",
      "\n",
      "{'doc': 'crimes classification over the rigorously defined legal articles is a tedious job in the juridical field\\njudges usually need to consult several relevant cases to determine the specific legal articles that an evidence violated, which is time consuming and needs extensive professional knowledge\\ntable 1 shows an example of an evidence in a legal case, as well as the legal article that the evidence violated\\ngenerally, the task can be cast as a multi-label classification problem to enhance working efficiency and to save manual efforts\\nin this work, we denote the multi-label classification problem from evidences to articles as the crimes classification task, which helps the judge to pinpoint potential articles quickly and accurately\\nhowever, this problem is a difficult task and we may face two key challenges in practice\\none is that the number of articles violated by different evidences is dynamic [10, 32, 42], i.e., the label dynamic problem\\nthrough our analysis on a large scale real-world referee document dataset where 70 articles are considered, the article set size over evidences variants significantly, as shown in figure 1\\nthe other challenge is the (class) label imbalance problem [3, 5, 34]\\na multi-label classification dataset is regarded as imbalanced if some of its (minority) labels in the training set are heavily underpresented compared to other majority labels\\nstatistics over the same dataset is shown in figure 2\\nas we can see, the number of violated evidences for each article (label) follows a long-tailed distribution, which means that many articles are seldom violated by evidences\\nmost traditional multi-label classification algorithms try to minimize the overall classification error during the training process, which implicitly assumes equivalent importance over all labels.\\nthe skewed distribution of class labels makes classification algorithms under this equivalent assumption biased towards the majority class labels\\nthough article definition can indicate some relations among different articles to alleviate the label imbalance problem (as shown in table 1, the definition of article 22 is similar to article 25), none of work has considered this information in crimes classification\\nthe difficulty in crimes classification thus raises an interesting research question\\ngiven a set of evidences and article definitions, can we classify the evidence automatically\\nalthough recent studies suggest that multi-label classification is increasingly required in many applications, such as protein gene classification [2], music categorization [31], and semantic scene classification [22].\\nto the best of our knowledge, no practice have been conducted on crimes classification in juridical scenarios. \\nprevious work on multi-label classification usually exploits the label correlations, such as bp-mll [40], kernel method [10], and calibrated label ranking [6], etc\\nhowever, all these methods learn the multi-label classification model and label threshold independently, and the label imbalance problem is largely ignored\\nto tackle with the first problem, we propose a multi-task framework to learn the multi-label classification model and the threshold predictor jointly\\nwhile for the second problem, we adopt the label descriptions to model the pairwise relations between labels, and extend the exact label set to a soft attention matrix over all the possible labels, which will alleviate the label imbalance problem as shown in our experiments\\nin this paper we propose a unified model named dynamic pairwise attention model (dpam for short) for crimes classification.\\nspecifically, we embed each evidence and article definition using the bag-of-word representations, and enumerate each article set into a pairwise label set, so that we can learn the pairwise label coveragebased classifiers from the transformed dataset\\nbesides, a label attention matrix is constructed based on the article definitions to alleviate the label imbalance problem\\nwe then design a regression model to learn a multi-label threshold predictor for each label automatically\\nfinally, a multi-task framework is designed to learn the two tasks jointly thus to improve the generalization performance by leveraging the information contained in related tasks\\noverall, the major contributions of our work are as follows\\nwe make the first attempt to investigate the prediction power of evidences and article definitions for crimes classification in juridical scenario\\nwe design a multi-task learning paradigm to learn multilabel classifier and threshold predictor jointly, thus dpam can improve the generalization performance by leveraging the information contained in related tasks\\na pairwise attention model based on article definitions is incorporated to the classification model to alleviate the label imbalance problem\\nwe conduct extensive experiments on two real-world datasets to verify the effectiveness of the proposed dpam model as compared with different baseline methods\\n the rest of the paper is organized as follows.\\nafter a summary of related work in section 2, we describe the problem formalization of crimes classification in juridical scenario and our proposed model in section 3\\nwe provide experiments and evaluations in section 4\\nsection 5 concludes this paper and discusses future direction', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'In juridical field, judges usually need to consult several relevant cases to determine the specific articles that the evidence violated, which is a task that is time consuming and needs extensive professional knowledge\\nIn this paper, we focus on how to save the manual efforts and make the conviction process more efficient\\n Specifically, we treat the evidences as documents, and articles as labels, thus the conviction process can be cast as a multi-label classification problem\\nHowever, the challenge in this specific scenario lies in two aspects.\\n One is that the number of articles that evidences violated is dynamic, which we denote as the label dynamic problem\\n The other is that most articles are violated by only a few of the evidences, which we denote as the label imbalance problem\\nPrevious methods usually learn the multi-label classification model and the label thresholds independently, and may ignore the label imbalance problem\\nTo tackle with both challenges, we propose a unified Dynamic Pairwise Attention Model (DPAM for short) in this paper\\nSpecifically, DPAM adopts the multi-task learning paradigm to learn the multi-label classifier and the threshold predictor jointly, and thus DPAM can improve the generalization performance by leveraging the information learned in both of the two tasks\\n In addition, a pairwise attention model based on article definitions is incorporated into the classification model to help alleviate the label imbalance problem\\n Experimental results on two real-world datasets show that our proposed approach significantly outperforms state-of-the-art multi-label classification method'}\n",
      "\n",
      "\n",
      "{'doc': 'with the rapid development of web techniques, recommender systems (rs) play a more and more important role in matching user needs with rich resources (called items) from various online platforms\\nfor building an effective recommender system, a key factor is able to accurately characterize and understand users’ interests and tastes, which are intrinsically dynamic and evolving\\nto achieve this goal, the task of sequential recommendation has been proposed to better satisfy sequential user needs [26], which aims to predict the successive item(s) that a user is likely to interact with given her past interaction records\\ntraditional recommendation methods (e.g., standard mf [17]) can’t well solve the sequential recommendation task, since they usually model static user-item interactions\\nfor capturing sequential patterns, the classic fpmc model [26] has been proposed to factorize user-specific transition matrix by considering the markov chain\\na major problem of fpmc is that it still adopts the static representation for user preference.\\nwith the revival of neural networks, many studies try to adapt powerful sequential neural models, i.e., recurrent neural networks (rnn), to sequential recommendation [35], including session-based rnn [15], user-based rnn [5] and attention-based rnn [18]\\nrnn-based models have been shown effective to improve the performance of sequential recommendation [15]\\nby encoding historical interaction records into a hidden state vector (called sequential preference representation), it is possible for these methods to capture dynamic user preference over time and measure the likelihood of the next item\\nalthough the state vector is able to encode sequential dependency, it has limited representation power in capturing complicated user preference\\nsince the state vector is encoded in a highly abstractive way, it is difficult to capture or recover fine-grained (e.g., attribute or feature level) user preference from the interaction sequence\\nfurthermore, the latent vector representation is usually hard to understand and explain\\nin recommender systems, interpretability is a very important factor to consider [14, 30]\\n these issues make it challenging to develop an effective and interpretable sequential recommender. \\nto enhance the capacity of modeling fine-grained user preference in an interpretable way, our idea is to incorporate external knowledge into the sequential recommender\\nthe incorporated knowledge should be rich and flexible to characterize varying context information in different domains\\na key problem is what kind of knowledge we can use and how we represent it\\nin this paper, we propose to link items in recommender systems with existing knowledge base (kb) entities, and leverage structured entity information for improving sequential recommendation\\nkbs store knowledge in triples of the form (head entity, relation, tail entity), typically corresponding to attribute information of entities\\nkbs provide a general way to flexibly characterize context information of entities from various domains\\nto obtain a compact representation for kb information, we adopt the kb embedding approach (i.e., transe [1]) to mapping entities and relations into low-dimensional vectors, called kb embeddings\\nthe major difficulty in designing the knowledge-enhanced sequential recommender is rnn-based models usually have limited short-term memories [3], which are not suitable to store external knowledge (e.g., kb information) for long-term usage\\ninspired by recent progress on improving the memory mechanism of neural networks [19, 21, 34], we propose to augment the rnn-based sequential recommender with external memories\\nby explicitly setting up an external memory of storage slots, memory networks (mn) manipulate the memory according to the received data signal with a set of predefined operations, e.g., read and write\\nit has been shown that mns are effective in memorizing long-term data characteristics [3], which can even evolve and update over time\\n we use kb information as external knowledge\\n considering the structural organization of entity information in kbs, we propose to incorporate kb knowledge via key-value memory networks (kv-mn). \\n kv-mns [21] decompose each memory slot into a key vector and a value vector\\na nice merit of kv-mn is that we can associate a key vector with a value vector, which supports associative search and read.\\nwith kv-mns, we set a key vector to a relation embedding learned from kb data, corresponding to an entity attribute.\\nfurthermore, given a key vector, we set up a user-specific value vector storing the preference characteristics of a user for the corresponding attribute.\\nin this way, external kb knowledge is effectively incorporated into the kv-mns\\n once the knowledge-enhanced kv-mns have been prepared, the next question is how to integrate it with rnn-based sequential recommender\\ninstead of simply merging the output from both components, at each recommendation, we use the sequential preference representation from rnns as the query to read out the associated content of user-specific kv-mns, i.e., value vectors\\nvalue vectors will be combined into an attributed-based preference representation with attentive weights derived from the sequential preference representation\\nthe attributed-based preference representation together with sequential preference representation are combined as the final representation of user preference\\nwe present the overview of the proposed model in fig. 1\\nto summarize, in this paper, we propose a novel knowledge enhanced sequential recommender\\nour model integrates rnn based networks (gru) with kv-mns\\nrnn-based networks are good at capturing sequential user preference, while knowledge enhanced kv-mns are good at capturing attribute-based user preference\\nby using a hybrid of rnns and kv-mns, it is expected to be endowed with the benefits of both components\\ngiven a hidden sequential preference representation from rnns, our model is able to transform it into attentive weights over the key vectors corresponding to attributes, which provides attribute-level interpretability\\n by setting user-specific value vectors, our model is able to learn the characteristics of user preference on some specific attribute, which further provides value-level interpretability.\\nto our knowledge, it is the first time that sequential recommender is integrated with external memories by leveraging existing kb information\\nfor evaluating our model, we prepare four rs datasets, and then link items of the four datasets with freebase entities\\nextensive results on the four datasets have shown the superiority of the proposed model in both effectiveness and interpretabilit', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'With the revival of neural networks, many studies try to adapt powerful sequential neural models, i.e., Recurrent Neural Networks (RNN), to sequential recommendation\\nRNN-based networks encode historical interaction records into a hidden state vector.\\nAlthough the state vector is able to encode sequential dependency, it still has limited representation power in capturing complicated user preference\\nIt is difficult to capture fine-grained user preference from the interaction sequence\\nFurthermore, the latent vector representation is usually hard to understand and explain\\nTo address these issues, in this paper, we propose a novel knowledge enhanced sequential recommender\\nOur model integrates the RNN-based networks with Key-Value Memory Network (KV-MN)\\nWe further incorporate knowledge base (KB) information to enhance the semantic representation of KV-MN. RNN-based models are good at capturing sequential user preference, while knowledge enhanced KV-MNs are good at capturing attribute-level user preference\\nBy using a hybrid of RNNs and KV-MNs, it is expected to be endowed with both benefits from these two components\\nThe sequential preference representation together with the attribute-level preference representation are combined as the final representation of user preference\\n With the incorporation of KB information, our model is also highly interpretable\\nTo our knowledge, it is the first time that sequential recommender is integrated with external memories by leveraging large-scale KB informatio'}\n",
      "\n",
      "\n",
      "{'doc': 'users in location-based social networks (lbsns), such as yelp and foursquare, can share their location with their friends by making checkins at venues (e.g. museums, restaurants and shops) they have visited, resulting in huge amounts of user check-in data.\\neffective venue recommendation systems have become an essential application for lbsns that facilitate users finding interesting venues based on their historical checkins\\ncollaborative filtering techniques such as matrix factorisation (mf) [17] are widely used to recommend a personalised ranked list of venues to the users\\nmf-based approaches typically aim to embed the users’ and venues’ preferences within latent factors, which are combined with a dot product operator to estimate the user’s preference for a given venue\\napproaches on mf typically encapsulate contextual information about the user, which can help to make effective recommendations for users with few historical checkins, known as the cold-start problem [22, 30, 32]\\nin recent years, various approaches have been proposed to leverage deep neural network (dnn) algorithms for recommendation systems [3, 10, 11, 21, 28, 31]\\namong various dnn techniques, the recurrent neural network (rnn) models have been widely used to extend the mf-based approaches to capture users’ short-term preferences from the users’ sequence of observed feedback [1, 21, 26, 28, 31, 37]\\nhere, the short-term (dynamic) preferences assume that the next venue visited by a user is influenced by his/her recently visited venues (e.g. users may prefer to visit a bar directly after dinner at a restaurant)\\na common technique to incorporate rnn models (e.g. long short-term memory (lstm) units [13] and gated recurrent units (gru) [4]) into mf-based approaches is to feed a sequence of uservenue interactions/checkins into the recurrent models and use the hidden state of the recurrent models to represent the users’ dynamic preferences [21, 28, 31, 35].\\nnext, the user’s preference of a target venue is estimated by calculating the dot product of this representation of the user’s dynamic preferences (i.e. the output of the recurrent models) and a latent factor of the target venue\\nalthough this technique can enhance the effectiveness of mf-based approaches, we argue that directly applying traditional rnn-based models to capture the users’ dynamic preferences is not effective for context-aware venue recommendation (cavr)\\nin particular, the traditional rnn models are limited as they can only take the sequential order of checkins into account and cannot incorporate the contextual information associated with the checkins (e.g. timestamp of a user’s checkin and the geographical location of the checkin)\\nindeed, such contexts have been shown to play an important role in producing effective cavr recommendations [6, 22, 30, 32]\\nto address the above challenge, various approaches have been proposed to extend the rnn models to incorporate the contextual information of observed feedback into various recommendation settings excepting cavr [1, 14, 19, 23, 26, 29, 37].\\nfor example, zhu et al. [37] proposed an extension of lstm (timelstm) by introducing time gates that control the influence of the hidden state of a previous lstm unit based on the time interval between successive observed feedbacks\\nindeed, they assume that the shorter the time interval between two successive feedback, the stronger the correlation between these two feedbacks and vice versa\\nhowever, their proposed model was designed for a particular type of contextual information (i.e. time intervals) and is not flexible to incorporate other types of context (e.g. distance between venues).\\nwe argue that the time gates proposed by zhu et al. [37] are not effective to model the sequences of checkins in lbsns\\nfigure 1 illustrates the user’s sequential order of checkins.\\nlet’s consider the time intervals and distances between three successive checkins c_{\\\\\\\\tau - 1}, c_{\\\\\\\\tau} and c_{\\\\\\\\tau + 1}.\\nwith zhu et al.’s time gates, checkin  c_{\\\\\\\\tau - 1} (c_{\\\\\\\\tau}) will have a small impact on checkin c_{\\\\\\\\tau} (c_{\\\\\\\\tau} + 1) due to the long time interval between  c_{\\\\\\\\tau - 1} (c_{\\\\\\\\tau} ) and (c_{\\\\\\\\tau} ) c_{\\\\\\\\tau + 1}.\\nthis is counter-intuitive since checkin (c_{\\\\\\\\tau}) may have a strong impact on checkin (c_{\\\\\\\\tau}) due to the geographical distance between them\\nfor example, a user may decide to visit a museum near the restaurant they had dinner at the previous day.\\nalthough the time interval from the previous checkin is long (> 24 hours), geographically, the restaurant and museum are close\\nrecently, several works (e.g. cgru [26] and latentcross [1], for product and movie recommendation systems, respectively) have extended traditional rnn architectures for recommendation systems to incorporate different types of contextual information of the observed feedback sequences\\nhowever, we argue that their proposed architectures are limited for context-aware venue recommendation in several respects\\nin figure 1, we highlight two types of contextual information associated with sequences of checkins, namely: the ordinary and transition contexts\\nthe ordinary context represents the (absolute) timestamp and the geographical position of the checkin, while the transition context represents the (relative) time interval and distance between successive checkins\\na disadvantage of the aforementioned rnn architectures is that they rely on a quantised mapping procedure (i.e. to convert continuous values of time intervals and distances to discrete features and represent these transition contexts using low-dimensional embedding vectors), which may result in a loss of granularity\\nin addition, their proposed architectures treat the ordinary and transition contexts dependently.\\nhowever, we argue that these contexts influence the user’s dynamic preference differently and should be considered independently\\nindeed, the ordinary context reflects the user’s contextual preference on a venue, while the transition context reflects the influence that one checkin has on its successor\\nto address these challenges, we propose a contextual attention recurrent architecture (cara) that leverages the sequential of users’ checkins to model the users’ dynamic preferences.\\nin particular, our contributions are summarised below\\nwe propose a contextual attention recurrent architecture (cara) that independently incorporates different types of contextual information to model the users’ dynamic preference for cavr\\nour proposed recurrent architecture differs from the recently proposed cgru [26] and latentcross [1] architectures in three aspects\\n(1) cara includes gating mechanisms that control the influence of the hidden states between recurrent units\\n(2) cara supports both discrete and continuous inputs and\\n(3) cara treats different types of context differently, in contrast, both cgru and latentcross do not support these features\\nwithin the cara architecture, we propose two gating mechanisms: a contextual attention gate (cag) and a time- and spatial-based gate (tsg)\\nthe cag controls the influence of context and previous visited venues, while tsg controls the influence of the hidden state of the previous rnn unit based on time interval and geographical distances between two successive checkins.\\nnote that our proposed tsg differs from the time gates in timegru [37] as we can incorporate multiple types of context, whereas timegru supports only the time intervals\\nto the best of our knowledge, this work is the first that incorporates geographical information into an rnn architecture for cavr\\nwe conduct comprehensive experiments on 2 large-scale realworld datasets, from brightkite and foursquare, to demonstrate the effectiveness of our proposed cara architecture for cavr by comparing with state-of-the-art venue recommendation approaches\\nthe experimental results demonstrate that cara consistently and significantly outperforms various existing strong rnn models\\nthis paper is structured as follows\\nsection 2 provides a background in the literature on cavr, as well as recent trends in applying deep neural networks to recommendation systems\\nsection 3 details specific existing rnn-based recommendation architectures from the literature, and highlights 5 limitations in these approaches\\nsection 4 details our proposed cara architecture that addresses all 5 limitations\\nexperimental setup and results are provided in sections 5 6, respectively\\nconcluding remarks follow in section ', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'Venue recommendation systems aim to effectively rank a list of interesting venues users should visit based on their historical feedback (e.g. checkins)\\nSuch systems are increasingly deployed by Locationbased Social Networks (LBSNs) such as Foursquare and Yelp to enhance their usefulness to users\\nRecently, various RNN architectures have been proposed to incorporate contextual information associated with the users’ sequence of checkins (e.g. time of the day, location of venues) to effectively capture the users’ dynamic preferences\\nHowever, these architectures assume that different types of contexts have an identical impact on the users’ preferences, which may not hold in practice\\nFor example, an ordinary context – such as the time of the day – reflects the user’s current contextual preferences, whereas a transition context – such as a time interval from their last visited venue – indicates a transition effect from past behaviour to future behaviour\\nTo address these challenges, we propose a novel Contextual Attention Recurrent Architecture (CARA) that leverages both sequences of feedback and contextual information associated with the sequences to capture the users’ dynamic preferences\\n1) a contextual attention gate that controls the influence of the ordinary context on the users’ contextual preferences and\\n2) a time- and geo-based gate that controls the influence of the hidden state from the previous checkin based on the transition context\\nThorough experiments on three large checkin and rating datasets from commercial LBSNs demonstrate the effectiveness of our proposed CARA architecture by significantly outperforming many state-of-the-art RNN architectures and factorisation approache'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/user/metin özütleme/hphaos summarunner/sigirtest.json\",encoding=\"utf8\") as f:\n",
    "        examples = json.load(f)\n",
    "        for i in range(0,10):\n",
    "            print(examples[i])\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-mercy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
