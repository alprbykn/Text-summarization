[{"doc": "traditionally, medical doctors and care providers have been the main source of information for patients who suffer from chronic or life-threatening diseases\n however, with the advent of the internet and the creation of many online health communities (ohcs), e.g., everyday health, cancer survivors\u2019 network, and webmd, patients use these health communities increasingly as an integral source for finding health-related information [5]\nohcs provide an environment for patients, their family members and friends to interact with other participants and share experiences and information (e.g., recommendations and feedback) on issues related to prescribed medicines, side effects, therapeutic processes, mental health, and feelings\ntable 1 shows examples of posts that contain health-related information shared among patients in an online cancer community\nthis information is very unique and is often not available elsewhere, e.g., referring to the medication sertraline, a patient writes: doctors also say it helps with hot flashes.\ni don\u2019t know about that since i still get them (see example 1 in the table)\nseveral studies showed that using ohcs to obtain information from people who went through the same or similar experiences (either by direct interactions or sifting through the online posts) brings better feelings and fewer mortality odds to patients [8].\nthus, the large and growing amounts of user-generated content in ohcs need to be accurately classified for a variety of applications, e.g., designing smart information retrieval systems for content recommendation\nrecent computational studies in ohcs started to investigate the high level identification of informational posts [1, 19], however, with no emphasis on the unique challenges associated with the detection of the information type, e.g., therapeutic procedures vs. side effects\na deep understanding of the text and the writer\u2019s intention is required in order to correctly extract the types of information present in ohcs messages\nexample 1 in table 1 refers to therapeutic procedure, whereas example 2 refers to side effects through various medication (sertraline and anzamet, respectively)\nin this paper, we propose to analyze messages in ohcs to extract the information type that they contain, i.e., therapeutic procedures (any medical treatment, activity, or behavior that have a positive impact on patients\u2019 health, precisely, can help prevent, cure or improve a patient\u2019s condition) and side effects (any medical treatment, activity, or behavior that have a negative impact on patients\u2019 health, precisely, a secondary, often undesirable effect of a drug or medical treatment)\nto achieve this, we design a computational model that is able to exploit the semantic information from text, and coherently combines high-level (abstract) features with surface-level and lexicon-based features.\nour contributions are as follows\n(1) we propose to extract fine-grained information types from messages posted in ohcs.\nidentifying information types provides doctors, health practitioners and ohcs\u2019 moderators with an insightful view of patients\u2019 physical status during various treatments\nin addition, it can provide new diagnosed patients with information about what they should expect throughout their treatments and help them in making informed decisions about their disease more effectively [14]\nto our knowledge, we are the first to address fine-grained information type extraction in ohcs\n(2) we design and explore a computational model that can identify messages belonging to therapeutic procedures and side effects with high accuracy\nour model is a hybrid neural network combined with lexicon-based features\n(3) we show empirically that our model significantly outperforms strong baselines and prior works and continues to perform well even in the absence of lexicon-based feature", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Online health communities have become a medium for patients to share their personal experiences and interact with peers on topics related to a disease, medication, side effects, and therapeutic processes\nAnalyzing informational posts in these communities can provide an insightful view about the dominant health issues and can help patients find the information that they need easier\nIn this paper, we propose a computational model that mines user content in online health communities to detect positive experiences and suggestions on health improvement as well as negative impacts or side effects that cause suffering throughout fighting with a disease\nSpecifically, we combine high-level, abstract features extracted from a convolutional neural network with lexicon-based features and features extracted from a long short term memory network to capture the semantics in the data\nWe show that our model, with and without lexicon-based features, outperforms strong baseline"},
{"doc": "quantitative information can be very useful for augmenting the summary of an event or for adding more context to a topic in the form of annotations\nfor example, the outcome of the 2016 us elections can be described with two numbers: 304 and 227, which represent the number of electoral votes for trump and clinton.\n two more numbers for the same event are also relevant: 66m and 63m, in this case the number of popular votes (in millions) for clinton and trump respectively\nwhile the overall topic is the surprise victory of the republican candidate, news articles and web pages describing the event contain such numbers to quantify the result\nwe are interested in recognizing fragments of text that report \u201cmeasures\u201d, extracting the corresponding quantity, and capturing a description of what is being measured.\nthese annotations can then be used for a wide range of applications like search, question answering, passage retrieval, unsupervised learning, or information visualization\nwe believe that extracting relevant quantitative information from social data can be a beneficial annotation to a topic and useful in search scenarios where numerical data can help with context, summaries, or query suggestions\nidentifying numbers in text is somewhat straightforward but extracting relevant quantities is a challenging task.\nthe noisy characteristics of social data makes this task complex but, at the same time, there is potential to utilize human sensing at scale to signal which numbers are relevant to a wider audience.\nhow much quantitative information is available on a social network like twitter\ncan we extract useful numerical data that is relevant\nwe perform experiments to quantify numerical data in a number of topics and analyze the findings in the context of the event at hand\nin this paper we propose the problem of annotating a topic with quantities and how such quantities can be extracted\n as part of our research, we use the notion of a quantfrag, that is, a small portion of text that contains quantitative data\ninformation extraction (ie) is the task of automatically extracting structured information as records from documents in different domains [1]\nwe are interested in detecting relevant quantitative information, that is, numbers and their associated context\nprevious work have focused on extracting dates and named entities, building knowledge bases [5], and analyzing disasters through social media [4].\naugmenting tables with semantic information about quantities is presented in [3]\nquantitative data has received limited attention in information retrieval and even in nlp, there is little published research on quantities in language understanding\nrecent work by roy et al. [6] describe steps for reasoning about quantities and present the novel task of quantity entailmen", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Social data is a rich data source for identifying trends and topics of interest based on user activity\nSocial data also provides opportunities to collect numerical data about events like elections, sport games, disasters or economic news\nWe propose the problem of identifying relevant quantitative information from social data as annotations for a topic\nWe investigate how to extract quantitative information and perform a number of experiments and analysis with Twitter dat"},
{"doc": "real-world news events, such as wars, natural calamities, elections, etc., involve several actors, e.g., persons, organizations, countries, and temporal and spatial aspects [1]\nrepresenting news events as latent feature vectors is essential for various tasks, such as computing similarity values between news events and linking news events from one source to another.\nconsider for example, a journalist or a historian researching about past civil wars that resemble \u201csyrian civil war\u201d or previous civil wars in syria and other civil wars around the same era (e.g, \u201ciraqi civilwar\u201d)\nin such situations, latent feature representation of news events readily helps in news recommendation by identifying similar news events\nnews events have been represented before either in vector form or bag of words of textual, semantic, and temporal features derived from news articles [2]\n this involves manually engineering several features and coming up with different ways to combine them [14]\nmoreover, since news events span multiple news articles, these representations are not optimal for capturing the non-linear relationship between different elements of news events\nto address this problem, there are proposals to model news events in the form of networks [8, 17]\nan example of such an event network can be seen in figure 1, in which there are events, event types, entities, and temporal information\nin principle, news events can also be treated as pure text and word embeddings can be used to learn the latent features [10, 13]\nin recent work, news events vectors are also represented as simple aggregations of associated word vectors [3]\nin recent years, deep neural networks have been proposed for information retrieval and text classification based on the principle of learning the relevance between queries and documents [7]\nsince news events are fundamentally a sequence of news articles, these models could also be used for learning news event representations\nhowever, the structure of text documents, where words are written sequentially in a linear fashion, is fundamentally different.\nthe above mentioned techniques are not suitable for capturing the network structures arising due to the cross-document relationship of several classes of entities related to the news events\nfor example, as shown in figure 1, even though \u201csyrian civil war\u201d is not directly connected to \u201ciraqi civil war\u201d, they are connected via entities iraq and syria\ninspired by this, there are several proposals to learn embeddings to capture the non-linear structures of the networks [5, 15]\nthese approaches employ random walks to efficiently learn the feature vectors\nthe problem, we address in this paper is to automatically learn network embeddings from networked representations of news events\nexisting network embedding solutions, however, are not suitable for events since the nodes in event networks are of different classes or types\nto address this problem, we propose biased random walks tailored for event networks.\nin this regard, we compute the transition probabilities for the biased random walk while preserving the semantics of events.\nin summary, our main contributions are\nevent2vec a novel way to learn latent features for news events by extending biased random walks to respect event semantic\napplication of network embeddings for news events to news recommendation and news linking task\nevaluation with real data and ground truth from wikipedia and google knowledge graph and several strong baseline\nmanual evaluation using crowdflow", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Representation of news events as latent feature vectors is essential for several tasks, such as news recommendation, news event linking, etc\nHowever, representations proposed in the past fail to capture the complex network structure of news events\nIn this paper we propose Event2Vec, a novel way to learn latent feature vectors for news events using a network\nWe use recently proposed network embedding techniques, which are proven to be very effective for various prediction tasks in networks\nAs events involve different classes of nodes, such as named entities, temporal information, etc, general purpose network embeddings are agnostic to event semantics\nTo address this problem, we propose biased random walks that are tailored to capture the neighborhoods of news events in event networks\nWe then show that these learned embeddings are effective for news event recommendation and news event linking tasks using strong baselines, such as vanilla Node2Vec, and other state-of-the-art graph-based event ranking technique"},
{"doc": "forest-based regression models are the leading approach to train a learning to rank model [9, 19, 22], especially in industry [3, 5, 26]\nthese models have numerous benefits: they can be trained directly based on observed gradients of traditional ir metrics, unlike other learning to rank approaches [15], and learning is fairly efficient while achieving state-of-the-art results\nthe drawback of forest models for production systems is that the cost of prediction is quite high: the naive algorithm for executing a tree is interpretation.\nthis means that for a forest of size t and depth d, an interpreter will visit o(td) nodes for every point, and at each node it must compare a feature value and branch on the result\ngiven that pipelining is the ubiquitous strategy to making modern cpu architectures fast, these branch-heavy models are a worst-case scenario\nat every decision point, pipeline is flushed, and actual instruction-level parallelism and therefore throughput will be quite low\nthis limits both query throughput and query latency of a learning to rank server, using a lot of machines and resources\nefficiency of learning to rank approaches has drawn a greater amount of interest in recent years [19]\nbefore that work, asadi and lin argued that we should train these models to be more runtimeaware [1].\n for many years now, researchers have been pursuing the question \u201chow do we minimize the runtime cost of forest-based learning models?\u201d [1, 4, 8, 13, 14, 19, 21, 25].\nin this work,we propose answering this question by a key observation about the nature of these ranking ensembles\nour key observation is that any ranking model will produce scores, given a set of document features as input\nonce such a model is trained and validated, our goal for ranking is to output the predictions of that model as fast as possible\ntherefore, if we had available to us a black box that could produce the same scores but faster, then we would be effectively executing our learned model\n   hornik identifies feed-forward neural networks as valid universal function approximators [11]\nin practice, this means that we can take these popular ranking ensembles and fully approximate them\nin this work, we first present analytic arguments for the effectiveness of feed-forward neural networks as full-approximators of regression forests (\u00a73)\nnext, we provide an empirical demonstration of this technique: showing that we can learn approximations with no loss in mean average precision for lambdamart ensembles trained on the msn30k dataset, and for a random forest ranker trained on mq2007 and trained on gov2 (to demonstrate generalizability under more train/test skew)\nsimultaneously, we demonstrate the difficulties of directly training the same neural model on document judgments which reflects that using a generalized model (lambdamart) leads to a generalized neural model\nfinally, we present a brief sketch of our performance gains and observe a 2-10x improvement (table 2) over previous published results\nin some sense, the core task we propose is not novel as it was possible and known since the introduction of the xor-problem in 1969 alongside the perceptron [24], and confirmed for our specific functions later [11, 18]\n however, recent works on faster algorithms for learning to rank ensembles (e.g., [19]) suggest that our revisiting of this theoretical work and empirical confirmation is of significant research value and will lead to important discussions in the learning to rank and information retrieval communities\nwe hope that industrial production systems employing such models can use our techniques to reduce their energy footprint while still satisfying user", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Learning to rank is a key component of modern information retrieval systems\nRecently, regression forest models (i.e., random forests, LambdaMART and gradient boosted regression trees) have come to dominate learning to rank systems in practice, as they provide the ability to learn from large scale data while generalizing well to additional test queries\nAs a result, efficient implementations of these models is a concern in production systems, as evidenced by past work\nWe propose an alternate method for optimizing the execution of learned models: converting these expensive ensembles to a feedforward neural network.\nThis simple neural architecture is quite efficient to execute: we show that the resulting chain of matrix multiplies is quite efficient while maintaining the effectiveness of the original, more-expensive forest model\nOur neural approach has the advantage of being easier to train than any direct neural models, since it can match the previously-learned regression rather than learn to generalize relevance judgments directly\nWe observe CPU document scoring speed improvements of up to 400x over traditional algorithms and up to 10x over state-of-theart algorithms with no measurable loss in mean average precision\nWith a GPU available, our algorithm is able to score every document in a batch in parallel for another 10-100x improvement.\nWhile we are not the first work to observe that neural networks are efficient as well as being effective, our application of this observation to learning to rank is novel and will have large real-world impac"},
{"doc": "we have been witnessing rapid growth in the number of smart mobile devices recently, followed by the increase of mobile data traffic of 74% in 2015 and 63% in 2016, and it is further estimated that in the near future the majority of the overall internet consumption will be mobile\nsuch climate of ubiquitous mobile usage provides a growing market opportunity for mobile advertising with greater reach than more traditional online advertising [1]\nin order to continuously improve advertising efforts, a better understanding of mobile usage patterns is necessary, especially those usage actions which lead to a mobile purchase\nmining users\u2019 mobile app actions and purchasing habits is a fundamental task for enabling better context-aware design and delivery of advertisements and recommendations in the form of relevant apps or services that could be of interest to the user\nin this light, monitoring users\u2019 mobile activities and generating app event logs from users who opted for anonymous advertising studies serves as a rich source of data.\nanother relevant piece of information comes from users\u2019 purchase habits, which can be found in app store purchase receipts\nin our study, the data will ultimately consist of registered users that made purchases on a mobile app store and use one or more apps registered on proprietary advertisement sdk\nthe usefulness of the app event signals for predicting mobile purchase can be taken from the fact that approximately 50% of purchases were preceded by more than ten signals from apps registered to a proprietary sdk in a one-hour window.\ncoupling the two data sources would result in sessions as shown in figure 1.\napp event logs are thus perceived as context of purchased items.\nin our example we observe a user over a specific time window, spending a period of time communicating with friends and listening to music, followed by playing a mobile game (angry birds in our example), after which he purchased coins for the game.\nthe example session depicts a user whose main intent at that time was to be entertained, which often results in purchasing of in-app goods for a complete experience\npredicting users\u2019 purchase intent is a difficult problem primarily due to large universe of products and services (items) that a user can buy\nfurthermore, app events are traditionally defined by the app developers who provide a free-form textual description of the event, therefore lacking uniformity over the universe of apps, which poses a challenge of using this information\nthe goal of this study is to propose a model capable of coping with the aforementioned challenges and capturing useful patterns from app event sequences that can be used to anticipate users\u2019 purchase intents, improving ad targeting and app recommendation capabilities\nmodeling user actions as sessions and learning low dimensional distributed representations of events in a sequence has recently brought benefits over more traditional approaches such as collaborative filtering or clustering\nhowever, these approaches are primarily unsupervised, or rather they do not model supervised information explicitly\nwe aim to employ a family of models that can exploit the power of neural embedding models, while retaining the ability to explicitly model supervised information given a sequence\nnamely, we exploit the recently proposed deep memory networks models [5], capable of explicitly modeling memory relevant for predicting current events\nfor example, users that made an in-app purchase for a mobile game or bought songs within a mobile store are likely to do so again\nour results show that the proposed approach outperforms other approaches on mobile purchase prediction while learning meaningful representations of noisy event", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Rapid expansion of mobile devices has brought an unprecedented opportunity for mobile operators and content publishers to reach many users at any point in time\nUnderstanding usage patterns of mobile applications (apps) is an integral task that precedes advertising efforts of providing relevant recommendations to users\nHowever, this task can be very arduous due to the unstructured nature of app data, with sparseness in available information\nThis study proposes a novel approach to learn representations of mobile user actions using Deep Memory Networks\nWe validate the proposed approach on millions of app usage sessions built from large scale feeds of mobile app events and mobile purchase receipts\nThe empirical study demonstrates that the proposed approach performed better compared to several competitive baselines in terms of recommendation precision quality\nTo the best of our knowledge this is the first study analyzing app usage patterns for purchase recommendatio"},
{"doc": "several neural ranking models have been proposed recently that estimate the relevance of a document to a query by considering the raw query-document text [14] or based on the patterns of exact query term matches in the document [5], or a combination of both [10]\n these models typically learn to distinguish between the input feature distributions corresponding to a relevant and a less relevant query-document pair by observing a large number of relevant and non-relevant samples during training.\nunlike traditional learning to rank (ltr) models that depend on hand-crafted features [8], these deep neural models learn higher level representations useful for the target task directly from the data\ntheir ability to learn features from the training data is a powerful attribute that enables them to potentially discover new relationships not captured by hand-crafted features\nhowever, as mitra and craswell [9] discuss, the ability to learn new features may come at the cost of poor generalization and performance on domains not observed during training\nthe model, for example, may observe that certain pairs of phrases\u2014e.g., \u201ctheresa may\u201d and \u201cprime minister\u201d\u2014co-occur together more often than others in the training corpus.\nor, the model may conclude that it is more important to learn a good representation for \u201ctheresa may\u201d than for \u201cjohn major\u201d based on their relative frequency of occurrences in training queries\n while these correlations and distributions are important if our goal is to achieve the best performance on a single domain, the model must learn to be more robust to them if we instead care about \u201cout of box\u201d performance on unseen domains, e.g., older trec collections [19]\nin contrast, traditional retrieval models (e.g.bm25 [12]) and ltr models based on aggregated count based features\u2014that make fewer distributional assumptions\u2014typically exhibit more robust cross domain performances\nour goal is to train deep neural ranking models that learn useful representations from the data without \u201coverfitting\u201d to the distributions of the training domains\nrecently, adversarial learning has been shown to be an effective cross domain regularizer suitable for classification tasks [3, 17]\nwe adapt a similar strategy to force neural ranking models to learn more domain invariant representations\nwe train our neural ranking model on a small set of domains and evaluate its performance on held out domains\nduring training, we combine our ranking model with an adversarial discriminator that tries to predict the domain of the training sample based on the representations learned by the ranking model\nthe gradients from the adversarial components are reversed when backpropagating through the layers of the ranking model\nthis provides a negative feedback signal to the ranking model to discourage it from learning representations that may be significant only for specific domains\nour experiments show consistent improvements in ranking performance on held out domains from the proposed adversarial training\u2014sometimes up to 30% improvement on precision@", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Unlike traditional learning to rank models that depend on handcrafted features, neural representation learning models learn higher level features for the ranking task by training on large datasets\nTheir ability to learn new features directly from the data, however, may come at a price\nWithout any special supervision, these models learn relationships that may hold only in the domain from which the training data is sampled, and generalize poorly to domains not observed during training\nWe study the effectiveness of adversarial learning as a cross domain regularizer in the context of the ranking task\nWe use an adversarial discriminator and train our neural ranking model on a small set of domains\nThe discriminator provides a negative feedback signal to discourage the model from learning domain specific representations\nOur experiments show consistently better performance on held out domains in the presence of the adversarial discriminator\u2014sometimes up to 30% on precision@"},
{"doc": "sarcasm and irony are a sophisticated form of symbolic or nonliteral language use where one says or writes the opposite of what they mean.\ndue to this intentional ambiguity, detecting sarcasm, especially in written communication where the usual cues such as the tone of voice or facial expression are unavailable, is a particularly challenging task\nconsider a few examples of sarcastic text utterances presented in table 1\nextensive research in psychology points towards a strong correlation between affect and sarcasm [3, 8], and while some recent models of computational sarcasm detection [10, 12, 24, 26] incorporate affective features, the affective information is derived through extensive feature engineering and limited-sized affective resources\nmoreover, the distinct role of sentiment versus emotion spectrums of affect remains unexplored\n while word representations trained from larger corpora of text can overcome the issues of limited training data and manual feature engineering, and provide a wider vocabulary coverage [9, 13], most of these word embeddings have been obtained using only contextual information, without incorporating any affective information\nin this paper, we seek to benefit from bridging the two avenues of research (word representations and affective knowledge) for detecting sarcasm\n we propose affective word embeddings for sarcasm (awes), a framework for jointly modeling affective as well as contextual information, in order to obtain affectively richer word representations making them more suitable for detecting sarcasm in text\nwe investigate the use of information stemming from two different spectrums of affect: sentiment and emotion.\nthe proposed model projects words with similar affective orientations into neighboring regions of the embedding space\nin particular, to prepare for training affective word embeddings, we use distant supervision to automatically label two large corpora of product reviews with (noisy) sentiment or emotion labels\nthen, a bidirectional long short-term memory (blstm) neural network model is trained using one of the labeled corpora for incorporating affective and contextual information into word representations, where the affective knowledge is derived via the noisy affective labels, and the sequences of words capture the contextual information\nthe main contributions of our work include:\n(i) a framework for learning two types of novel affective word representations (sentiment-aware and emotion-aware) for sarcasm detection\n(ii) an extensive evaluation on six benchmark sarcasm datasets across three domains (tweets, product reviews and forum posts)\n(iii) a novel finding that sentiment-aware representations are most effective for short text sarcasm detection and emotion-aware representations are most effective for detecting sarcasm in longer text", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Sarcasm detection from text has gained increasing attention.\nWhile one thread of research has emphasized the importance of affective content in sarcasm detection, another avenue of research has explored the effectiveness of word representations\nIn this paper, we introduce a novel model for automated sarcasm detection in text, called AffectiveWord Embeddings for Sarcasm (AWES), which incorporates affective information into word representations\nExtensive evaluation on sarcasm detection on six datasets across three domains of text (tweets, reviews and forum posts) demonstrates the effectiveness of the proposed model\nThe experimental results indicate that while sentiment affective representations yield best results on datasets comprising of short length text such as tweets, richer representations derived from fine-grained emotions are more suitable for detecting sarcasm from longer length documents such as product reviews and discussion forum post"},
{"doc": "snippets are an essential part of a search results page: they incite users to view (to click) or to skip viewing a retrieved document\nalready in 1991, pedersen, cutting, and tukey [15] proposed querybiased snippets, and they have proven useful until today [23, 25, 26]\nthe more surprising appears google\u2019s recent decision to remove snippets altogether from its redesigned news portal, without offering any explanation\nrecall that google has notoriously been \u201cquestioning the unquestioned,\u201d subjecting virtually every detail of its search interfaces to a/b tests\n if this policy has not been changed, can the redesigned news portal be interpreted as evidence that snippets are not so useful after all (in the domain of news search)\na more plausible explanation can be found in the changing interpretation of the copyright: publishers from all over the world are now raising claims for compensation for displaying text extracted from their news articles\nespecially in europe their lobbying for political support was successful: an ancillary copyright for news publishers, which basically exempts their intellectual property from fair use, has been passed into law in germany and spain, and it is currently discussed as part of an eu-wide copyright reform\nin light of these developments, the removal of snippets from google news appears as an act of anticipatory obedience\nin as much as today\u2019s information economy on the web is financed by advertisements, the welfare of publishers partaking in this ecosystem depends on consumers visiting their web pages\nthis is also true for a large portion of the revenue of news publishers; some well-known publishers even stopped printing newspapers\nit is hence no surprise that news publishers protect their online assets more fiercely than they used to do\nnews publishers form a well-organized business community with traditionally strong ties to politics and public opinion, yet, other online communities may follow\neven wikipedia\u2019s contents are regularly lifted onto the search results pages of many search engines, depriving the encyclopedia of visitors, which may have contributed to the ongoing decline of active wikipedia editors since 2007 [13, 21].\ncommercial search engines, whose operations remained unchallenged in this respect for more than two decades, may therefore face a turn\nare information scientists forced to pick a side?\nprobably not, since it is not our business to protect business models, neither that of search engines nor that of publishers\nrather, we should uphold the vision of developing the \u201cperfect\u201d information system, which is what the information society needs\nwhen it comes to snippets, text reuse has been popular because it is easy\nlooking forward, potthast et al. [17] propose deep-learning-based text generation as a promising, yet more difficult alternative.\nmoreover, users\u2019 expectations may include reuse snippets\nthe paper in hand debunks this notion: we investigated the users\u2019 preferences, showing that, with some reservations, the majority of users does not prefer traditional reuse snippets over paraphrased versions, or vice vers", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The snippets in the result list of a web search engine are built with sentences from the retrieved web pages that match the query\nReusing a web page\u2019s text for snippets has been considered fair use under the copyright laws of most jurisdictions\nAs of recent, notable exceptions from this arrangement include Germany and Spain, where news publishers are entitled to raise claims under a so-called ancillary copyright\nA similar legislation is currently discussed at the European Commission.\nIf this development gains momentum, the reuse of text for snippets will soon incur costs, which in turn will give rise to new solutions for generating truly original snippets\nA key question in this regard is whether the users will accept any new approach for snippet generation, or whether they will prefer the current model of \u201creuse snippets.\nThe paper in hand gives a first answer\nA crowdsourcing experiment along with a statistical analysis reveals that our test users exert no significant preference for either kind of snippet.\nNot with standing the technological difficulty, this result opens the door to a new snippet synthesis paradig"},
{"doc": "contemporary information extraction from text, relation inference in knowledge graphs (kgs), and question answering (qa) are informed by continuous representations of words, entities, types and relations\nfaced with the query \u201cname scientists who played the violin,\u201d and having collected candidate response entities, a qa system will generally want to verify if a candidate is a scientist\n testing if e \\\\epsilon t or t_{1} \\\\subseteq t_{2}, where e is an entity and t , t1, t2 are types, is therefore a critical requirement.\nunlike albert einstein, lesser-known candidates may not be registered in knowledge graphs, and we may need to assign a confidence score of belongingness to a target type\na common recipe for inferring general relations between entities is to fit suitable vectors to each of them, and to train a network to input query vectors and predict presence or absence of the probed relationship\na key question has been whether types merit a special representation, different from the generic devices that represent kg relations, because of their special properties\ntwo types may be disjoint, overlapping, or one may contain the other.\ncontainment is transitive\ncompared to the vast array of entity-relation representations available [2, 10, 12, 16, 19], few proposals exist [5, 17, 18] for representing types to satisfy their specific requirements\nof these, only order embedding (oe) by vendrov et al. [17] directly enforces transitivity by modeling it as elementwise vector dominance\nwe make three contributions\nfirst, we present a significant improvement to the oe loss objective\nsecond, we generalize oe to rectangle embeddings for types: types and entities are represented by (hyper-)rectangles and points respectively.\nideally, type rectangles contain subtype rectangles and entity instance points\nrather than invoke established neural gadgets as black boxes, we introduce constraints and loss functions in a transparent manner, suited to the geometric constraints induced by the task at hand\nthird, we remove a limitation in the training and evaluation protocol of vendrov et al. [17], and propose a sound alternative\nexperiments using synsets from the wordnet noun hierarchy (same as vendrov et al. [17]) show the benefits of our new formulations.\n our code will be availabl", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Beyond word embeddings, continuous representations of knowledge graph (KG) components, such as entities, types and relations, are widely used for entity mention disambiguation, relation inference and deep question answering\nGreat strides have been made in modeling general, asymmetric or antisymmetric KG relations using Gaussian, holographic, and complex embeddings\nNone of these directly enforce transitivity inherent in the is-instance-of and is-subtype-of relations\nA recent proposal, called order embedding (OE), demands that the vector representing a subtype elementwise dominates the vector representing a supertype\nHowever, the manner in which such constraints are asserted and evaluated have some limitations.\nIn this short research note, we make three contributions specific to representing and inferring transitive relations\nFirst, we propose and justify a significant improvement to the OE loss objective\nSecond, we propose a new representation of types as hyperrectangular regions, that generalize and improve on OE.\nThird, we show that some current protocols to evaluate transitive relation inference can be misleading, and offer a sound alternative\nRather than use black-box deep learning modules off-the-shelf, we develop our training networks using elementary geometric consideration"},
{"doc": "ride sharing has become an increasingly popular option for people to meet their travel needs.\nride-sharing platforms such as uber and didi chuxing have been growing rapidly in the past few years, completing more than 4 billion1 and 7 billion2 rides in year 2017 alone.\nthe growing popularity has also led to an increase in the diversity of users\u2019 requests\n while some users are cost-sensitive and prefer the cheapest option, some other users would rather pay a high price for a higher-standard service\nit is also very common for some users to use the ride sharing app to request a taxi pickup\nin light of the diversity in people\u2019s needs, ride sharing companies are providing multiple services for users to choose, which, however, puts an extra burden for users\n sometimes, a user has to swipe several times to find the exact line-of-business (s)he is looking for\nin this paper, we model users\u2019 choice of services, and build classifiers to predict the user\u2019s choice when opening the app\nintuitively, such choices are correlated with several features: (a) the services that the users have chosen in previous trips, and (b) the current time and location of the users\nfor each feature, we first characterize its correlation with the choice of services using mutual information, and then build a predictive model with the feature.\n we also combine all features based on an ensemble model\nfinally, we conduct offline simulation experiments to verify the effectiveness of our models with realworld data\nthe results show that our ensemble model performs very well.\nin addition, although all of our models increase the prediction accuracy from the majority-guess baseline, no particular model outperforms all other models for all user groups\nthe rest of the paper is organized as follows: section 2 briefly reviews the related work\nin section 3, we formalize the prediction task, the evaluation metrics, and the experiment setup.\nin section 4, we describe the prediction models based on sequential features, spatial features, and temporal features\nwe compare the performance of different models in section 5\nwe discuss the limitation and future work in section 6 and conclude in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Ride sharing apps like Uber and Didi Chuxing have played an important role in addressing the users\u2019 transportation needs, which come not only in huge volumes, but also in great variety\nWhile some users prefer low-cost services such as carpooling or hitchhiking, others prefer more pricey options like taxi or premier services\nFurther analyses suggest that such preference may also be associated with different time and location\nIn this paper, we empirically analyze the preferred services and propose a recommender system which provides service recommendation based on temporal, spatial, and behavioral features\nOffline simulations show that our system achieves a high prediction accuracy and reduces the user\u2019s effort in finding the desired service\nSuch a recommender system allows a more precise scheduling for the platform, and enables personalized promotion"},
{"doc": "millions of people increasingly rely on music recommendation and streaming services for extended periods each day.\nthe most popular music services such as pandora, spotify or last.fm, use, among other algorithms, context aware recommendations, which often include demographic context, user location, type of the device, time of day, and others, to improve recommendation quality\ncontextual recommender systems (cars) have been extensively studied [3, 4], and have been successfully incorporated into state-of-the-art recommendation systems [11, 12]\nin addition to context, other dimensions of recommendations have been proposed, such as user emotion, knowledge, skills, or psychological state[11, 12], which can be difficult to detect explicitly\ntherefore, previous research has primarily focused on representing the activity as the main recommendation context (e.g.,[2, 7, 9]), with the underlying assumption that it is sufficient to represent the \"user context to match the users listening intent\nhowever, a recent study has shown this not to be the case: for the same activity, different users often report different music listening intents [13]\nby showing that music listening intent is distinct from the activity context, reference[13] indicated that further improvements to music recommendation may be possible\nfurther empirical validation of the music listening intent is needed, and more importantly, an exploration of whether the listening intent could be operationalized to improve recommendation quality\nfurthermore, user intent, especially in recommender systems, is notoriously difficult to automatically infer, and to incorporate into recommendation.\nto make this problem more tractable, we propose to model the activity context, and the intent of the listener jointly, by first using the (inferred or specified) activity context to restrict the set of the most likely intents, and then use the intent-specific model to prioritize the contextual music recommendations\nour contributions are threefold: first, we estimate the empirical music listening intent distribution of music videos posted on youtube.com, one of the currently most popular music sharing services (section 2.1)\nsecond, we perform initial experiments on representing music listening intent as audio features using another popular music api, from spotify.com (section 2.2)\nthird, we report initial results on using the trained audio intent models to improve activity- and intent-aware music recommendation, and show promising improvements over activity-only music recommendation (section 3.2", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "While activity-aware music recommendation has been shown to improve the listener experience, we posit that modeling the listening intent can further improve recommendation quality.\nIn this paper, we perform initial exploration of the dominant music listening intents associated with common activities, using music retrieved from popular online music services\nWe show that these intents can be approximated through audio features of the music itself, and potentially improve recommendation quality\nOur initial results, based on 10 common activities and 5 popular listening intents associated with these activities, support our hypothesis, and open a promising direction towards intent-aware contextual music recommendatio"},
{"doc": "machine learning has become an essential part of many tasks recently, including information retrieval as in cross domain recommendation [1] and cross network influence maximization [2]\nhowever, methods that work well rely on the assumption that the training set and the test set are drawn from the same feature space and the same distribution, which does not always happen in realistic settings\nhence, to obtain favorable performance on a target dataset whose feature space or distribution is different from the source data, one may need to recollect labeled training data manually and then retrain the models on it.\nhowever, it is prohibitively expensive or even impossible to recollect the training data with label information.\nthus, it is important to develop methods that can borrow prior knowledge to compensate for the unavailable or insufficient labels of the target data\ndomain adaptation is one of the fields that are associated with this problem\ndomain adaptation addresses the case that adapts a model trained on source data to target data from different but related domains.\nthe importance of domain adaptation has been explored in a series of applications, such as natural language processing, computational biology, computer vision and information retrieval\nunder the setting of domain adaptation, despite consisting of the same categories, the source data and the target data are typically distributed differently which is referred to as domain shift.\nfor instance, in the scenario of visual domain adaptation, the distribution can be substantially affected by angle transformation, illumination, or occlusion.\ntherefore, many approaches to domain adaptation focus on bridging different domains by reducing their distribution discrepancy\nto combat the performance degradation on target domain arising from domain shift, previous works have explored approaches in various directions\nthe common approaches include feature augmentation, feature transformation, and domain re-sampling\nalthough these methods have made prominent progress, their shallow architectures prevent them from achieving more desirable performance\nrecently, deep neural networks have been proved having strong ability of learning transferable features, and this depicts the potential of empowering domain adaptation with deep learning\nfeatures in deep neural networks eventually transit from general to specific as the layer goes higher(deeper), which shows that parameters from lower-layers are general enough to suit both source and target tasks, while the transferability gap between tasks grows in higher(deeper) layers\ntherefore, the network pre-trained on source dataset is not likely to be discriminative enough when it is applied to the target dataset directly\ninspired by such characteristic of deep neural networks, some hierarchical approaches have been proposed to boost the generalization performance by reducing the domain discrepancy in higher layers [3] [4]\nhowever, deep neural networks are hard to train when regularizer is integrated into the loss function, which play a role of adversarial training, and there are too many parameters needed to be finetuned very carefully\nthe key idea in this paper is to add samples that are most likely to be correctly predicted from each category to the source domain for more effective transfer.\nsuch process to control the transfer is carried out in an iterative finetuning manner\nfurthermore, the process of selecting target data is conducted for each category simultaneously, which makes the proposed framework efficient and accurate\nin the next section, we review some existing works related to domain adaptation.\nafter that, our proposed method is introduced\n we then evaluate our approach and show the comparative results before concluding the pape", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Domain adaptation refers to the learning scenario where a model learned from the source data is applied on the target data which have the same categories but different distributions.\nIn information retrieval, there exist application scenarios like cross domain recommendation characterized similarly\nIn this paper, by utilizing deep features extracted from the deep networks, we proposed to compute the multi-layer joint kernelized mean distance between the kth target data predicted as the ith category and all the source data of the jth category d_{ij}^{k}\nThen, target data T_{m} that are most likely to belong to the ith category can be found by calculating the relative distance d_{ii}^{k} / \\\\sum j d_{ij}^{k}\nBy iteratively adding T_{m} to the training data, the finetuned deep model can adapt on the target data progressively\nOur results demonstrate that the proposed method can achieve a better performance compared to a number of state-of-the-art method"},
{"doc": "query performance prediction (qpp) is a well studied  problem in information retrieval (ir) due to its potential importance in improving the effectiveness and  efficiency of a wide variety of search tasks [5]\nthe query performance prediction task is defined as predicting the quality of a retrieval model for a given query, when neither explicit nor implicit relevance information is available\naccurate and real-time performance predictors could potentially be used in triggering a specific action in the retrieval system, such as selecting an index traversal algorithm at query time [27], choosing the correct number of documents to process in a cascaded multistage retrieval system [13], choosing the most effective ranking function per query, selecting the best variant from multiple query reformulations, or requesting more information from users in cases of potential poor retrieval performance, particularly in conversational systems\nquery performance prediction models are categorized as pre-retrieval and post-retrieval approaches\npost-retrieval approaches, which are the focus of this paper, analyze the result list returned by the retrieval engine in response to the query\npostretrieval predictors are our focus as they have been proven to be more effective than pre-retrieval predictors [5]\nin this paper, we propose a general framework based on neural networks for the query performance prediction task\nour framework, called neuralqpp, consists of multiple components, each analyzing a distinct aspect useful for performance prediction\neach component learns a high-dimensional dense representation suitable for the qpp task\nthese representations are then aggregated and fed into a prediction sub-network\nthe whole framework is trained in an end-to-end fashion\nwe introduce three neural components for implementing the neuralqpp framework\neach is designed with minimal network engineering for simplicity\nthe first component analyzes the retrieval scores for the top documents retrieved in response to a given query\nthe retrieval score distribution has been previously used in a variety of qpp models [14, 40, 44, 57]\nthe second component analyzes the term distribution for the documents appearing in the result list\na term distribution can be a means for measuring the coherence of the top ranked documents, which has been proven to be highly correlated with query performance [11]\nthe third component analyzes the distributed representation of documents in a semantic space\nthis component is able to measure the semantic coherence and diversity of the result list\nrecently, dehghani et al. [16] and zamani and croft [50] proposed the training of neural ir models with weak supervision\nweak supervision is an unsupervised learning approach where a large set of unlabeled data is labeled with an existing unsupervised model as a weak labeler\nas it is often very difficult to generate high quality training data, we describe an approach to training neuralqpp using multiple weak supervision signals\nto do so, we are able to benefit from three existing predictors that estimate the query performance based on different intuitions and assumptions\nto be exact, our weak labelers include a clarity-based approach by cronen-townsend et al. [11], a score-based approach by shtok et al. [44], and a combining approach by shtok et al. [42]\ntraining a generalized model with multiple weak signals led us to develop a component dropout technique that randomly disables at most k \u22121 (out of k) components of the neuralqpp framework\nthis can be also viewed as a regularization that prevents the models from overfitting\nwe evaluate our models using four standard trec collections, including two newswire test collections (ap and robust) respectively used for the trec 1-3 ad-hoc tracks and the trec 2004 robust track, and two large-scale web collections (gov2 and clueweb) respectively used for the trec 2004-2006 terabyte tracks and the trec 2009-2012 web tracks\nour experiments show that the proposed model significantly outperforms the baselines, in nearly every setting\nwe also empirically study the influence of each component in the neuralqpp framework, and the effectiveness of employing multiple weak signals for training\nthe results demonstrate that neuralqpp performs remarkably well in predicting the performance of various retrieval models\nin summary, this paper introduces the first neural network architecture for query performance prediction\nfurthermore, it not only provides a successful implementation of the weak supervision idea for an additional fundamental ir task, but also provides new insights on how best to learn with multiple weak labelers\nneuralqpp produces state-of-the-art performance on multiple collection", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Predicting the performance of a search engine for a given query is a fundamental and challenging task in information retrieval\n Accurate performance predictors can be used in various ways, such as triggering an action, choosing the most effective ranking function per query, or selecting the best variant from multiple query formulations\nIn this paper, we propose a general end-to-end query performance prediction framework based on neural networks, called NeuralQPP\nOur framework consists of multiple components, each learning a representation suitable for performance prediction\nThese representations are then aggregated and fed into a prediction subnetwork\nWe train our models with multiple weak supervision signals, which is an unsupervised learning approach that uses the existing unsupervised performance predictors using weak labels\nWe also propose a simple yet effective component dropout technique to regularize our model\nOur experiments on four newswire and web collections demonstrate that NeuralQPP significantly outperforms state-of-the-art baselines, in nearly every case\nFurthermore, we thoroughly analyze the effectiveness of each component, each weak supervision signal, and all resulting combinations in our experiment"},
{"doc": "data providers (aka. data vendors) collect and index both public and proprietary data source into a searchable database and serve people in various areas such as finance [2, 7, 16] and academia [1, 5]\nthe key value-add for data providers is data integration, cleaning, updating and offering the structured query interface\n for example, bloomberg [2] users can query real-time financial and economic data by using manual commands or scripting-friendly apis\nit is important for the data providers to protect their data by only allowing users who commit to the fair use of the data\nwhile providers have different definitions of fair use, almost everyone agrees that data reselling is not acceptable use.\nin general, data resellers (drs) pull loads of data through the query api from data providers, and resell them, or offer their own data service\nsuch data reselling activities can cause loss of revenue to the data provider and potential copyright infringement to the data source\nthus it is necessary for the data providers to identify drs and take countermeasures against them\nidentifying data resellers, which we call the anti-data-reselling (adr) problem, is different from the anti-web-crawling (awc) problem\nexisting awc work analyzes the web access patterns to distinguish automatic web crawlers from humans [4, 6, 9, 13, 15, 17, 18] or to detect malicious crawlers for security issues (e.g., defending against ddos attacks) [14, 19]\nhowever, these techniques do not help solve adr problem\na key goal for awc is to distinguish bots from human, while many data providers offer api-based queries to support scripting as a feature, allowing different kinds of bots.\nthus, the goal is to distinguish a specific type of abnormal data retrieval behavior, rather than whether the task is done programmatically\nin some sense, for adr, we need to reason about \u201cwhy the data is taken\u201d rather than \u201chow the data is taken\u201d, making it a hard problem\ntechnically, awc often relies on analyzing very short-term behaviors to identify a bot, while adr focuses on much longer-term patterns\nit is nontrivial to precisely identify data resellers\npeople often use simple rules, such as limiting the query volume\nhowever, modern applications may need lots of data.\nfor example, some automated trading applications use lots of queries to track the price of a set of stocks.\nif the user paid for the query volume, setting a quota hurts the user experience\nthus, we need more sophisticated features and model to distinguish drs from regular heavy data users\nas an additional challenge, there are not many labeled drs for model training, and thus we have to leverage unsupervised learning techniques as much as possible\nwhile people can write rules to capture dr\u2019s patterns, drs can change their patterns to avoid the detection\nwe design our systems based on a set of fundamental behavior patterns that the drs cannot change easily, and we have the following three key insights about drs\u2019 behavior patterns\n1) volume\nwhile heavy data users are not necessarily drs, the other way around is usually true (they may not be the heaviest users, but still heavy)\nthis is because someone retrieving only a small data volume cannot cause much damage anyways\n2) spread.\ndrs need to retrieve a large variety of data items, instead of getting a small set over and over, as legitimate users do.\nthis is because normal users often focus more on certain keys (e.g., a subset of stocks, or a specific academic discipline), while data resellers need more data variety for reselling\nnote that spread by itself does not necessarily lead to dr suspicion\nfor example, legitimate users may conduct a survey that requires retrieving a variety of keys\n3) periodicity\ndrs need to query data periodically to update the database they are reselling, and thus they will send queries periodically over time\nbased on these three insights, we propose a systematic method combining feature engineering and supervised/unsupervised learning techniques to solve the adr problem\nour basic idea includes: 1) feature creation\nbased on the three characteristics above, we create some features covering user behavior patterns including query volume, distribution, time, periodicity and burstiness\nthe goal of this step is to cover the three characteristics from as many aspects as possible and as redundant as possible, to make it difficult for drs to avoid\n2) feature selection and reweighting\nthe key problem is that there is no precise definition of the dr behavior, even from the data providers\nwe take a learn-by-example approach and automatically select a subset of features based on the few observed dr samples\nthe goal here is to \u201clearn\u201d the data provider\u2019s definitions of dr behavior\n3) dr identification\nusing the selected features above, we use unsupervised outlier detection algorithms (and supervised classification, if we have some labels) to identify drs\nwe report the detected drs to the data provider, and it decides what measures to take against each of them\nwithout ground truth labels, we cannot quantitatively evaluate the detection result.\ninstead, we focus on getting interpretable results and insights and getting feedbacks from data security experts at the data provider\nwe perform experiments on a real-world query log containing 9,000+ users\u2019 queries from a large-scale financial data provider\nthere are very limited data labels\nour method can identify a number of drs, achieving much better accuracy than the naive methods\nsecurity experts at the data provider have confirmed our detection results and our system has been deployed at the data provider\nwe define adr problem and focus on solving a general framework\nin summary, our major contributions are\n we define the anti-data-reselling (adr) problem, identify the three key characteristics behaviors of drs and propose expressive features and a systematic method to identify them\nwe apply our methods to a real query log and provide insightful detection result", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Data providers have a profound contribution to many fields such as finance, economy, and academia by serving people with both web-based and API-based query service of specialized data\nAmong the data users, there are data resellers who abuse the query APIs to retrieve and resell the data to make a profit, which harms the data provider\u2019s interests and causes copyright infringement\nIn thiswork, we define the \u201canti-data-reselling\u201d problem and propose a new systematic method that combines feature engineering and machine learning models to provide a solution\nWe apply our method to a real query log of over 9,000 users with limited labels provided by a large financial data provider and get reasonable results, insightful observations, and real deployment"},
{"doc": "recurrent neural networks are supposed to be able to capture sequential pattern successfully from sequence data, such as speech and text [10, 17, 21]\nrecurrent networks retain a state that can represent information from an arbitrarily long context window [10]\nso it can capture time dependencies in each sequence globally\nsequential recommendation is solved by recurrent neural network as follows\nat each time step, it takes one item of the user\u2019s sequence in a sequential order.\nthen, it encodes this item\u2019s information in the context for the next step\nfinally, it utilizes the current item information and the context information at last step to predict the next item\nthrough this process, recurrent neural networks only learn the global information from each sequence alone\nhowever, neither the global method (latent factor model) or the local approach (neighborhood approach) performs the best [9].\nthis poses a great challenge in the application of recurrent neural network to sequential recommendation\nto tackle this challenge, we introduce neighborhood model into recurrent neural networks to capture both the local and global information at the same time\n therefore we propose a novel network structure, k-plet recurrent neural network (inspired by triplet network [7]), to accommodate the query sequence and its k neighbor sequences jointly\nkr network (short for k-plet recurrent neural network) is comprised of k + 1 instances of the same recurrent neural networks\nthere are various ways to model the interaction between the query sequence and its neighbor sequences in the kr network\nthe first and intuitive way is to introduce the interaction into the loss function, denoted as krnn-l.\nspecifically, we model the similarity between the query sequence and its neighbors, and employ this similarity as the regularization of the original loss function, e.g. cross entropy loss\nwe optimize this similarity regularized loss to learn the network parameters\na deeper interaction is built in the prediction function of the query sequence, and we denote this method as krnn-p\nin light of k-nn classification, we predict the next item distribution of the query sequence as the weighted sum of probabilistic outputs of the query sequence and its neighbors\nweights are estimated as the similarity between the query sequence and its neighbors, such as the euclidean distance\nexperimental results on benchmark datasets movielens-1m and amazon movies show that our proposed kr network outperforms state-of-the-art baselines but needs more time for training compared with traditional rnns\nour main contribution lies in the following two aspects\n(1) we propose a k-plet recurrent neural network architecture, referred to as kr networks, to capture both the local and global information in the sequences for better recommendation\n(2) we introduce two distinguished ways to model the interaction between the query sequence and its neighbors in kr networ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Recurrent Neural Networks have been successful in learning meaningful representations from sequence data, such as text and speech\nHowever, recurrent neural networks attempt to model only the overall structure of each sequence independently, which is unsuitable for recommendations\nIn recommendation system, an optimal model should not only capture the global structure, but also the localized relationships\nThis poses a great challenge in the application of recurrent neural networks to the sequence prediction problem.\nTo tackle this challenge, we incorporate the neighbor sequences into recurrent neural networks to help detect local relationships\nThus we propose a K-plet Recurrent Neural Network (Kr Network for short) to accommodate multiple sequences jointly, and then introduce two ways to model their interactions between sequences\nExperimental results on benchmark datasets show that our proposed architecture Kr Network outperforms state-of-the-art baseline methods in terms of generalization, short-term and long term prediction accurac"},
{"doc": "finding proper citations and referring to them appropriately in scientific manuscripts is often a labor-intensive task [7].\ncitation recommendation system aims to ease the process by suggesting reference candidates through a two-step interactive procedure\n first, the user specifies the location in the manuscript where the citation is needed\nsecond, the system ranks the possible candidates from a corpus or bibliographical list\nranking candidate references as the second step of citation recommendation has been extensively studied in the literature [4, 5, 7, 8, 17]\nhowever, minimizing the authors\u2019 effort in the first step is relatively unexplored\ncitation sentences are those where some references to other papers are required\u2014for validating, motivating, or other purposes.\nin this study, we use linguistic features to detect citation sentences in sentence-level granularity.\nto this end, we define the task of evaluating sentences for citation, in short citation worthiness.\nindeed, given a sentence s, the citation worthiness task is to classify the sentence to either \u201ccite\u201d or \u201cnot_cite\u201d class, i.e., a binary classification task\nthe task assumes that no sentence in the input text has signatures of citation sentences (citation placeholders \u201c(author(s), year)\u201d, the author\u2019s name for a cited work, especial phrases like \u201cet al.\u201d)\nthe task we introduce here is similar to the teufel\u2019s argumentative zoning (az) task [18, 19]\nusing simple features (e.g., sentence location, length, whether the sentence contains citation, linguistic features of the sentence, etc.), az aims to identify and classify scientific text into different pre-specified categories\u2014e.g., background, motivation, or contrasting statements\nteufel later introduced a citation function task for predicting the author\u2019s reason for citing a given paper with a linguistically inspired solution [21]\nsome other similar tasks are defined in the literature, e.g., citation sentiment detection [3], argumentation mining [14], rhetorical classification [9, 20], text summarization using citation sentences [11], reference scope identification [1], and citation recognition in public comments [2]\ncontrary to the mentioned studies, citation worthiness does not use any external knowledge bases and does not depend on citation signatures\ntable 1 presents four example sentences for each binary label\nthe objectives for \u201ccite\u201d sentences are based on four main categories presented in [21] and \u201cnot_cite\u201d ones are based on argumentative zones [19]\nthese examples provide an insight into the differences between \u201ccite\u201d and \u201cnot_cite\u201d sentence", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Does this sentence need citation?\nIn this paper, we introduce the task of citation worthiness for scientific texts at a sentence-level granularity\nThe task is to detect whether a sentence in a scientific article needs to be cited or not\nIt can be incorporated into citation recommendation systems to help automate the citation process by marking sentences where needed\nIt may also be useful for publishers to regularize the citation process\nWe construct a dataset using the ACL Anthology Reference Corpus; consisting of over 1.1M \u201cnot_cite\u201d and 85K \u201ccite\u201d sentences.\nWe study the performance of a set of state-of-the-art sentence classifiers for the citation worthiness task and show the practical challenges\nWe also explore sectionwise difficulty of the task and analyze the performance of our best model on a published articl"},
{"doc": "real-time bidding (rtb) is becoming an increasingly popular trading means for advertising inventory that is bought and sold through the public auction of individual impressions in real time, i.e., while a consumer is waiting\nrtb advertising has become an increasingly important approach for enabling advertisers to promote their advertising performance through more accurately targeting online potential users\nin the rtb enabled display advertising ecosystem, publishers (sellers) supply advertising inventory to buyers (advertisers, agencies, etc.) through advertisement exchange systems\nadvertisement exchanges aggregate the advertising inventories of multiple publishers and sell them to a number of buyers via a real-time auction of each impression\nthe display of an advertisement placed on a publisher\u2019s site (web page, application, etc.) is considered an advertisement impression\nwhen a user clicks on a hyperlink to a publisher\u2019s web page, in addition to producing and showing high-quality content to the user, the second main task of the publisher is to sell its advertisement inventory for monetization\nif there is an advertisement space that needs to be sold by rtb, the publisher initiates the rtb trading process by sending a bid request to a number of bidders (buyers) through an advertisement exchange or other supply source\nthe bid request consists of at least advertisement space attributes and user attributes, and optionally additional attributes providing the impression context\nwhen receiving a bid request, the bidders need to use bidding algorithms to instantly decide whether to bid and determine their bidding price, and then return their bidding price to the advertisement exchange in real time\nthe impression is sold to the highest bidder and the winner\u2019s advertisement is placed on the publisher\u2019s website\nat the same time, the buyer must pay the seller for this advertisement impression, which completes this rtb trading process\nadvertisers obtain an unequal return from these impressions according to the subsequent responses of users who visit the web page containing the advertisements\nthe possible responses are divided into three types, as follows\nfirst, if the user takes no action on the impression, as if the advertisement does not exist, this response is defined as non-click\nsecond, if the user clicks on the advertisement and reaches the landing page of the advertiser, but takes no further action, this response is defined as click-only.\nthird, if the user further completes one of the predefined actions on the publisher\u2019swebsite, such as registration, order placement, purchase, or subscription to an email list, we define it as a conversion response\nin general, for performance-based advertising, conversions may bring advertisers multiple times the profits of click-only actions, and click-only events may bring them more profit than non-clicks.\ntherefore, during a certain rtb trading period, buyers aim to purchase more conversion impressions first and then more click-only impressions with their fixed budget\nto achieve this, buyers must accurately estimate the conversion rate (cvr) or click-through rate (ctr) for each advertisement impression before giving the final bid price, which directly affects the bid acceptance probability and the return on their investment\nthis paper is focused on the cvr estimation problem for buy-sides in rtb\na combined regression and tripletwise ranking method (crt) is proposed that jointly examines regression loss and tripletwise ranking loss to estimate the cvr\nthe final goal of buyers is to maximize their return on investment (roi) during an rtb trading period for one or more advertising campaigns, which contains a series of rtb bidding processes for impressions\nto gain higher returns when their budget is fixed, buyers prefer to first purchase more conversion impressions than click-only ones and then more click-only impressions prior to nonclick ones\ntherefore, the predicted cvr (pcvr) or predicted ctr (pctr) should be able provide ranking of conversion impressions above click ones and click impressions above non-click ones\nmeanwhile, to reduce the expense, buyers need to accurately estimate a reasonable bid price, which directly determines the investment quantity and the bidding result\nsince pcvr and pctr both play key roles in buyers\u2019 computation of the bid price, it is critical that the actual cvr or ctr estimation is as accurate as possible to enable efficient bid pricing [18]\nmoreover, it is important for buyers that the pcvr and pctr not only yield good ranking values, but also provide good regression estimates [15, 18]\nthus, methods for estimating the cvr and ctr more accurately constitute a key technology for buyer\u2019s bidding algorithms\nin recent years, cvr estimation has increasingly attracted research attention because of the substantial returns that advertisement conversions yield\nbecause of the more serious sparsity of historical conversion data for buy-sides, in rtb it is more difficult for buyers to predict the cvr than the ctr\nhowever, historical click information is more plentiful than conversion data\nfurthermore, in general impressions with a click event have a greater probability of being converted than those with no click event\ntherefore, appropriate utilization of click information would facilitate improvement in the pcvr accuracy\nin this paper, we focus on the cvr estimation problem for buy-sides in rtb trading and propose a combined regression and tripletwise ranking method, crt\nthe proposed crt method simultaneously utilizes historical click information and conversion information via tripletwise learning optimization to alleviate the sparsity of conversion data available for cvr estimation\nfurthermore, our method also jointly examines regression and ranking loss while estimating the pcvr\ntherefore, crt attempts to rank conversion impressions above click-only ones and click-only impressions above non-click ones (in terms of the area under the receiver operating characteristics (roc) curve (auc) and multi-class auc), as well as to achieve a good regression-based performance (in terms of the squared error)\nthe key contributions of this paper are as follows\nwe propose a tripletwise learning algorithm for three-category ranking\nthis learning algorithm is aimed to determine the correct order of each pair of conversion and click-only events, as well as the correct order of each pair of click-only and non-click events\ntherefore, it simultaneously uses the conversion and click information in the impression history log to alleviate sparsity for predicting the cvr\nwe propose a combined regression and tripletwise ranking algorithm, crt, which jointly examines the regression and ranking while estimating the cvr\nthe crt method attempts to rank conversion impressions above click-only ones and click-only impressions above non-click ones (in terms of the auc and multi-class auc), as well as to achieve a good regression-based performance in terms of the squared error\nwe describe the evaluation of our methods using content datasets\nthe experimental results show that our approach not only shows a promising performance for binary classification as compared to the baseline and some existing combined methods, but also provides a significant advantage for the three-category ranking task\nthis paper is organized as follows\nin section 2, we review existing studies related to ours and discuss their limitations\nthen, we present the problem formulation and preliminaries in section 3\nsection 4 presents details of our motivation and approach for combining regression and tripletwise ranking\nfinally, we provide the results of experiments on real-world datasets conducted to validate our analysis and test the efficacy of our metho", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "In real-time bidding advertising (RTB), the buyers bid for individual advertisement impressions provided by publishers in real time\nThe final goal of the buyers is to maximize the return on their investment\nTo gain higher returns, buyers prefer to first purchase more conversion impressions than click-only ones and then purchase more click-only impressions prior to non-click ones\nSimultaneously, to reduce the expense, they need to accurately estimate a reasonable bid price, the predicted precision of which depends on the precision of the predicted conversion rate (CVR) or predicted click-through rate (CTR)\nTherefore, the predicted CVR or predicted CTR must provide not only good ranking values but also correct regression estimations\nThis paper is focused on the CVR estimation problem for buy-sides in RTB and a combined regression and tripletwise ranking method (CRT) is proposed that jointly considers regression loss and tripletwise ranking loss to estimate the CVR\nThis method attempts to rank conversion impressions above clickonly ones and simultaneously rank click-only impressions above non-click ones\nMeanwhile, through simultaneously utilizing the historical conversion and click information to alleviate sparsity, the CRT method is also aimed to achieve a good two categoryranking performance, as well as a good regression performance for predicting the CV"},
{"doc": "one important goal in many information retrieval tasks involves providing search results that covers a wide range of topics for a search query, called search result diversification [1]\nthe goal of search result diversification can be formalized as selecting a minimal subset of documents from the candidate set to cover as many different subtopics as possible\nsince the novelty of a document depends on the other selected documents, selecting an optimal subset of documents amounts to the problem subset selection and its complexity is in general np-hard\ntypical approaches treat search result diversification as ranking the documents based on their relevance as well as the novelty\ngreedy sequential document selection has been widely adopted to construct the diverse document ranking, that is, the document ranking is constructed step by step\n at each step the ranking model selects one document from the candidate set for the current ranking position\n usually, the document with the maximal amount of additional utility, a.k.a. marginal relevance, is selected\na number of diverse ranking algorithms have been developed under the greedy document selection framework\ndifferent algorithms utilize different criteria to estimate the additional utility a candidate document can provide\nfor example, the representative approach of maximal marginal relevance (mmr) [3] uses the sum of the query-document relevance and the maximal document distance (referred to as marginal relevance) as the utility\nxquad [25] defines the utility so as to explicitly account for the relationship between documents retrieved for the original query and the possible subqueries\nin recent years, machine learning based methods have been proposed for conducting diverse ranking [23, 31, 32, 34, 36, 39]\nthe relational learning to rank (r-ltr) [39] and its variations [31, 32, 34] define the utilities based on the relevance features and the novelty features\nmdp-div adapted the markov decision process (mdp) to model the document ranking process\nthe utility of a document is estimated based on the mdp state, which consists of the query, the preceding documents, and the remaining candidates [33]\nthe greedy sequential document selection simplifies the ranking process and can accelerate the online ranking\nhowever, the rankings produced by greedy document selection are inevitably suboptimal\nat each ranking position, the greedy selection mechanism only considers the possibilities at the current ranking position (i.e., estimates the utility of each candidate document if it were selected)\nthus, greedy document selection will select the locally optimal document at each ranking position\nhowever, a sequence of the locally optimal documents cannot lead to the globally optimal diverse ranking, because the utilities of the documents are not independent\nthe selection of a document at one position will change the utilities of the remaining candidate documents and thereafter aspects the subsequent decisions\nin general the ranking algorithm need to explore the whole ranking space if the optimal ranking is mandatory\nhowever, this is usually infeasible in real applications because of the huge space size: there exist n! different rankings for n documents\ninspired by the success and methodology of the alphago [27] and alphago zero [28] for the game of go, in this paper we propose to enhance the mdp model for diverse ranking [33] with the monte carlo tree search (mcts), for alleviating the suboptimal ranking problem\nthe new ranking model, referred to as m2div (stands for mcts enhanced mdp for diverse ranking), makes use of an mdp to model the sequential document selection process of diverse ranking\nat each time step (corresponding to a ranking position), based on the user query and the preceding document ranking, a recurrent neural network (rnn) is used to produce the policy (a distribution over the candidate documents) for guiding the document selection and the value for estimating the whole document ranking quality (e.g., in terms of \\\\alpha -ndcg@m)\nto alleviate the problem of suboptimal diverse ranking, in stead of greedily selecting a document with the predicted raw policy, m2div conducts an exploratory decision making: an mcts is conducted to explore the possible document rankings at the subsequent positions, resulting a strengthened search policy for conducting the real document selection at current position\nsince it has explored more future possible document rankings, the search policy has higher probability to select a globally optimal document than the predicted raw policy\nmoving to the next iteration, the above process is continued until the candidate set is empty\nreinforcement learning is used to train the model parameters\n in the training phase, at each training iteration and for each training query, an mcts guided by the current policy function and value function is conducted at each ranking position.\nthe mcts produces a search policy for the document selection.\nthen the model parameters are adjusted to minimize the loss function\nthe loss function consists of two terms: 1) the squared error between the predicted value and the final quality of the whole document ranking in terms of \\\\alpha -ndcg@m; and 2) the cross entropy of the predicted raw policy and the search policy for document selection\nstochastic gradient descent is utilized for conducting the optimization\nto evaluate the effectiveness of m2div, we conducted experiments on the basis of trec benchmark datasets\nthe experimental results showed that m2div can significantly outperform the stateof- the-art diverse ranking approaches that using greedy sequential decision making, including the heuristic based diverse ranking methods of mmr and xquad, and the machine learning based diverse ranking methods of pamm and mdp-div\nwe analyzed the results and showed that the exploratory decision-making mechanism in m2div does help to improve the ranking performance", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The goal of search result diversification is to select a subset of documents from the candidate set to satisfy as many different subtopics as possible\nIn general, it is a problem of subset selection and selecting an optimal subset of documents is NP-hard\n  Existing methods usually formalize the problem as ranking the documents with greedy sequential document selection\nAt each of the ranking position the document that can provide the largest amount of additional information is selected.\nIt is obvious that the greedy selections inevitably produce suboptimal rankings\nIn this paper we propose to partially alleviate the problem with a Monte Carlo tree search (MCTS) enhanced Markov decision process (MDP), referred to as M^{2}Div\nIn M^{2}Div, the construction of diverse ranking is formalized as an MDP process where each action corresponds to selecting a document for one ranking position\nGiven an MDP state which consists of the query, selected documents, and candidates, a recurrent neural network is utilized to produce the policy function for guiding the document selection and the value function for predicting the whole ranking quality\nThe produced raw policy and value are then strengthened with MCTS through exploring the possible rankings at the subsequent positions, achieving a better search policy for decision-making\nExperimental results based on the TREC benchmarks showed that M2Div can significantly outperform the state-of-the-art baselines based on greedy sequential document selection, indicating the effectiveness of the exploratory decision-making mechanism in M^{2}Di"},
{"doc": "ranking is a core problem of information retrieval (ir).\nmany ir applications  such as ad-hoc retrieval, summarization and recommendations are ranking problems by nature [23].\namong all the ranking paradigms, learning to rank is the most widely used technology in modern search systems.\nthe idea of learning to rank is to represent each object with a manually designed feature vector and learn a ranking function with machine learning techniques.\nin document retrieval, for example, the ranking objects are querydocument pairs and the vector representation of a query-document pair usually consists of multiple document or query features such as bm25 scores, click through rates, query quality scores etc.\nthe ranking functions are typically learned globally on labeled query-document pairs from a separate training dataset [3, 6, 21, 23, 27]\nsuch a global ranking function, however, may not be optimal for document retrieval as it ignores the differences between feature distributions for each query.\ndepending on the query characteristics and user intents, relevant documents for different queries often have different distributions in feature space.\nconsidering two features such as word matching and freshness, relevant pages for a query like \u201cfriends season 1 online  watch\"  often have high scores on word matching but freshness is a lower priority; relevant documents for a query such as \u201cpolitical  news\", on the other hand, should have high values of freshness but word matching  scores are less important\n no matter how we design the feature vectors, these differences are inevitable and hard to solve with a global ranking function\na better paradigm for learning to rank is to learn a ranking model that can take into account the query-specific feature distributions.\nideally, ranking functions would be constructed  for each query separately [5, 16], but this would lead to unreasonable cost and low generalization ability because the number of possible queries is almost infinite and we do not know the feature distribution of an unseen query in advance.\nas a compromise, a more practical method is to learn a local model for each query on the fly and use it to refine the ranking results.\nfor example, a well-studied  framework is to represent each query with the top retrieved documents, namely the local ranking context\n previous studies [22, 28, 29, 39] have shown that pseudo relevance models learned from the local ranking context can significantly improve the performance of many textbased retrieval models\ngiven previous observations [22, 26, 39], it seems intuitive to assume that the local context information from top ranked documents would benefit the performance of learning-to-rank  systems.\nnonetheless, the utility of this information has not been fully studied\n one of the key challenges is how to develop a ranking model by using the feature representations of top results effectively and efficiently.\non the one hand, there is no trivial solution to extract patterns from a group of feature vectors with hundreds of dimensions (which is common in modern search engines).\ninstead, most previous studies focus on constructing models using the text of documents alone [22, 38] and ignore other ranking signals.\nthose methods usually require an additional feature extraction (e.g. term extractions from top documents) and retrieval  process in order to generate the final ranked list.\non the other hand, re-ranking retrieved documents without considering their inherent structure could be risky.\nglobal information from the initial retrieval, namely the ranking positions of top results, is a strong indicator of document relevance and should be considered when we encode and fine-tune the ranked list for each query\nto tackle these challenges, we propose a deep listwise context model (dlcm) that directly encodes the feature vectors of top retrieved documents to learn a local context  embedding and use it to improve the learning-to-rank systems.\nspecifically, we sequentially feed the original features of the top ranked results from a global learning-to-rank model into a recurrent neural network (rnn)\n the network state and the hidden outputs of the rnn are then used to re-rank the results from the initial retrieval.\nthere are several properties of our model that make it novel compared to previous studies.\nfirst, to the best of our knowledge, our model is the first model that directly incorporates the local ranking context from top results into a learning-to-rank  framework.\nsecond, our model uses the original feature representations and ranked lists from an existing system, which means that it can be directly deployed with most learning-to-rank  models without additional term or feature extraction from the top retrieved documents.\nwe adopt a re-ranking framework and require no additional retrieval process on document corpus after the initial run.\nlast, we propose an attention-based listwise loss for the training of our model.\nmodels trained with our attention-based loss are more efficient and effective than those trained with traditional listwise loss functions such as listmle [36]\n to demonstrate and understand the effectiveness of our model, we conducted empirical experiments on large-scale learning-torank corpora.  \nexperimental results show that our model outperformed the state-of-the-art learning-to-rank algorithms significantly and consistently.\nin addition, our analysis shows that our model was particularly good at finding the best document from a group of results, which potentially makes it useful for ranking scenarios where performance at high ranks is extremely importan", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Learning to rank has been intensively studied and widely applied in information retrieval.\nTypically, a global ranking function is learned from a set of labeled data, which can achieve good performance on average but may be suboptimal for individual queries by ignoring the fact that relevant documents for different queries may have different distributions in the feature  space.\n Inspired  by the idea of pseudo relevance feedback where top ranked documents, which we refer as the local ranking context, can provide important information about the query\u2019s characteristics, we propose to use the inherent feature distributions of the top results to learn a Deep Listwise Context Model that helps us fine tune the initial ranked list.  \nSpecifically, we employ a recurrent neural network to sequentially encode the top results using their feature vectors, learn a local context model and use it to re-rank the top results.\nThere are three merits with our model: (1) Our model can capture the local ranking context based on the complex interactions between top results using a deep neural network; (2) Our model can be built upon existing learning-to-rank methods by directly using their extracted feature vectors; (3) Our model is trained with an attention-based loss function, which is more effective and efficient than many existing listwise methods.\nExperimental results show that the proposed model can significantly improve the state-of-the-art learning to rank methods on benchmark retrieval corpor"},
{"doc": "the goal of learning to rank is to optimize a parameterized ranking function such that documents that are more relevant to a user\u2019s query are ranked at higher positions [16]\na trained ranker combines hundreds of ranking features to recognize the relevance quality of a document to a query, and shows several advantages over the manually crafted ranking algorithms [4]\ntraditionally, such a ranker is optimized in an offline manner over a manually curated search corpus.\nthis learning scheme, however, becomes a main obstacle hampering the application of learning to rank algorithms for a few reasons: 1) it is expensive and time-consuming to obtain reliable annotations in large-scale retrieval systems; 2) editors\u2019 annotations do not necessarily align with actual users\u2019 preferences [20]; and 3) it is difficult for an offline-trained model to reflect or capture ever-changing users\u2019 information needs in an online environment [21]\nto overcome these limitations, recent research has focused on learning the rankers on the fly, by directly exploiting implicit feedback from users via their interactions with the system [5, 10, 27]\nfundamentally, online learning to rank (ol2r) algorithms operate in an iterative manner: in every iteration, the algorithm examines one or more exploratory directions, and updates the ranker if a proposed one is preferred by the users via an interleaved test [9, 23, 29, 30]\nthe essence of this type of ol2r algorithms is to estimate the gradient of an unknown objective function with low bias, such that online gradient descent can be used for optimization with low regret [6]\nfor example, one eventually finds a close to optimal ranker and seldom shows clearly bad results in the process\nin the web search scenario, the objective function is usually considered to be the utility of search results, which can be depicted by ordinal comparisons in user feedback, such as clicks [20]\nhowever, to maintain an unbiased estimation of the gradient, uniform sampling of random vectors in the entire parameter space is performed in these algorithms\n as a result, the newly proposed exploratory rankers are independent from not only the past interactions with users, but also the current query being served\nthis inevitably leads to slow convergence and large variance of ranking quality during the online learning process\nseveral lines of works have been proposed to improve the algorithms\u2019 online learning efficiency\nhofmann et al. [9] suggested to reduce the step size in gradient descent for better empirical performance\nin their follow-up work [8], historical interactions were collected to supplement the interleaved test in the current query and pre-select the candidate rankers\nschuth et al. [23] proposed to explore multiple gradient directions in one multi-interleaved test [24] so as to reduce the number of comparisons needed to evaluate the rankers\nzhao et al. [30] introduced the idea of using two uniformly sampled random vectors with opposite directions as the exploratory directions, with the hope that when they are not orthogonal to the optimal gradient, one of them should be a more effective direction than a simplely uniformly sampled direction.\nthey also developed a contextual interleaving method, which considers historical explorations when interleaving the proposed rankers for comparison, to reduce the noise from multi-interleaving\nnevertheless, all aforementioned solutions still uniformly sample from the entire parameter space for gradient exploration\nthis results in independent and isolated rankers for comparison\ntherefore, less promising directions might be repeatedly tested, as historical interactions are largely ignored when proposing the new rankers\nmore seriously, as the exploratory rankers are independently proposed for the current query, they might give the same ranking order of the candidate documents for interleaving (this happens when the difference in the feature weight vectors between two rankers are orthogonal to the feature vectors in those candidate documents)\nin this scenario, no click feedback can differentiate the ranking quality of those rankers in this query\nwhen the interleaved test cannot recognize the best ranker from ordinal comparison in a query, tie will be arbitrarily broken [23, 29]\nthis again leads to large variance and slow convergence of ranking quality in these types of algorithms\nwe propose improving the learning convergence of ol2r algorithms by carefully exploring the gradient space.\nfirst, instead of uniformly sampling from the entire parameter space for gradient estimation, we maintain a collection of recently explored gradients that performed relatively poorly in their interleaved tests\nwe sample proposal directions from the null space of these gradients to avoid repeatedly exploring poorly performing directions\nsecond, we use the candidate ranking documents associated with the current query to preselect the proposed rankers, with a focus on those that give different ranking orders over the documents\nthis ensures that the resulting interleaved test will have a better chance of recognizing the difference between those rankers\n third, when an interleaved test fails to recognize the best ranker for a query, e.g., two or more rankers tie, we compare the tied rankers on the most recent worst performing queries (i.e., the difficult queries) with the recorded clicks to differentiate their ranking quality.\nwe name the resulting algorithm null space gradient descent, or nsgd for short, and extensively compare it with four state-of-the-art algorithms on five public benchmarks\nthe results confirm greatly improved learning efficiency in nsgd, with a remarkably fast and stable convergence rate at the early stage of the interactive learning process.\nthis means systems equipped with nsgd can provide users with better search results much earlier, which is crucial for any interactive syste", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Online learning to rank (OL2R) optimizes the utility of returned search results based on implicit feedback gathered directly from users.\nTo improve the estimates, OL2R algorithms examine one or more exploratory gradient directions and update the current ranker if a proposed one is preferred by users via an interleaved test\nOur algorithm, named as Null Space Gradient Descent, reduces the exploration space to only the null space of recent poorly performing gradients\n This prevents the algorithm from repeatedly exploring directions that have been discouraged by the most recent interactions with users\nTo improve sensitivity of the resulting interleaved test, we selectively construct candidate rankers to maximize the chance that they can be differentiated by candidate ranking documents in the current query; and we use historically difficult queries to identify the best ranker when tie occurs in comparing the rankers\nExtensive experimental comparisons with the state-of-the-art OL2R algorithms on several public benchmarks confirmed the effectiveness of our proposal algorithm, especially in its fast learning convergence and promising ranking quality at an early stag"},
{"doc": "searching videos of interests from large collections has long been an open problem in the field of multimedia information retrieval [36]\nsince this task needs to answer queries by relevant videos only, most prior efforts cast it as a matching problem [33] by estimating the relevance score between a video and the given query\nsuch direct video-query matching works well for judging whether the description query occurs in an entire video that depicts simple scenes solely\nhowever, in some real-world scenarios (e.g., robotic  navigation, autonomous driving, and surveillance), the untrimmed videos usually contain complex scenes and involve a large number of objects, attributes, actions, and interactions, whereby only some parts of the complex scene convey the desired cues or match the description\nfor a prepared surveillance video lasting for several minutes, as figure 1 shows, one may only have interest in the moment, \u201ca girl in orange first walks by the camera\u201d, where the start and end points are at the 24s and the 30s, respectively\ntherefore, localizing temporal moments of interest within a video is more useful yet challenging, as compared to simply retrieving an entire video\nin this paper, we focus on the task of moment retrieval, aiming to identify the specific start and end points within a video to precisely respond to the given query\nin our work, a desired moment refers to a query-aware temporal segment whose content is in accordance with the given query\nin general, automatic moment retrieval from a video requires two components, namely, finegrained moment candidates localization and relevance estimation\nthe key challenges are, first, different moments in a video have varying durations and diverse spatial-temporal characteristics; thereby uncovering the underlying moments is already highly challenging, not to mention the estimation of moment-query relevance\nto generate the moment candidates, a direct way is to densely sample sliding windows at different scales\nhowever, such moment derivation methods are limited, not only for the expensive computational costs, but also the exponential search space\nsecond, the relevance estimation is a typical cross-modal retrieval problem\na viable solution as employed in [2] is to first project the visual features of the moment candidates and textual features of the query into a common latent space and then calculate the relevance based on their similarity\nnevertheless, such workflow overlooks the spatial-temporal information inside the moment and the query\ntaking the query of \u201ca girl in orange first walks by the camera\u201d as an example, the term \u201cfirst\u201d is relative and requires temporal context for proper comprehension\nto address the aforementioned problems, we develop an attentive cross-modal retrieval network, dubbed as acrn, for the task of moment retrieval\nfor moment derivation, we propose a temporal memory attention network to explore the attentive contextual visual features of the moments\nfor each pre-segmented moment, its surrounding context, consisting of pre- and postmoments, encodes consistent signal to imply the continuous scenes [10]\ninspired by this, we utilize a memory network to memorize the contextual information for each moment, and treat the natural language query as the input to an attention network to adaptively assign weights to the memory representation\nin the light of this, we obtain the augmented moment representation\nthereafter, we introduce a cross-modal fusion network to enhance the moment-query representation\nit is built on the inter- and intra-modal embedding interactions\nthe former aims to explicitly model the interactions between the visual and textual embeddings, and the latter targets at exploring the embedding interactions within each individual modality\nfinally, we feed the moment-query representation into a boundary regression model to predict the relevance scores and moment offsets\nthe key contributions of this work are three-fold\nwe present a novel attentive cross-modal retrieval network, which jointly characterizes the attentive contextual visual feature and the cross-modal feature representation\nto the best of our knowledge, the existing studies either consider only one of the above models or not integrate them within a unified model\nfor the purpose of accurately localizing moments in a video with natural language, we are the first to introduce a temporal memory attention network to memorize the contextual information for each moment, and treat the natural language query as the input of an attention network to adaptively assign weights to the memory representation\nwe perform extensive experiments on two benchmark datasets to demonstrate the performance improvement\nas a side contribution, we released the data and codes\nthe rest of the paper is organized as follows\nthe related work is briefly introduced in section 2\nsection 3 details the proposed approach\nwe present experiment results in section 4\nfinally, section 5 concludes the work and points out the future direction", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "In the past few years, language-based video retrieval has attracted a lot of attention.\nHowever, as a natural extension, localizing the specific video moments within a video given a description query is seldom explored.\nAlthough these two tasks look similar, the latter is more challenging due to two main reasons:\n1. The former task only needs to judge whether the query occurs in a video and returns an entire video, but the latter is expected to judge which moment within a video matches the query and accurately returns the start and end points of the moment.\nDue to the fact that different moments in a video have varying durations and diverse spatialtemporal characteristics, uncovering the underlying moments is highly challenging\n2. As for the key component of relevance estimation, the former usually embeds a video and the query into a common space to compute the relevance score\nHowever, the later task concerns moment localization where not only the features of a specific moment matter, but the context information of the moment also contributes a lot\nFor example, the query may contain temporal constraint words, such as first, therefore temporal context is required to properly comprehend them\nTo address these issues, we develop an Attentive Cross-Modal Retrieval Network\nIn particular, we design a memory attention mechanism to emphasize the visual features mentioned in the query and simultaneously incorporate their context.\nIn the light of this, we obtain an augmented moment representation\nMeanwhile, a cross-modal fusion sub-network learns both the intra-modality and inter-modality dynamics, which can enhance the learning of moment-query representation\nWe evaluate our method on two datasets: DiDeMo and TACoS.\nExtensive experiments show the effectiveness of our model as compared to state-of-the-art method"},
{"doc": "in the last ten years several effective machine learning solutions explicitly tailored for ranking problems have been designed, giving rise to a new research field called learning-to-rank (ltr)\nweb search is one of the most important applications of these techniques, as complex ranking models learned from huge gold standard datasets are necessarily adopted to effectively identify the documents that are relevant for a given user query among the billions of documents indexed.\ngiven a gold standard dataset where each query-document example is modeled by hundreds of features and a label assessing the relevance of the document for the query, a ltr algorithm learns how to exploit the features to provide a query-document scoring model that optimizes a metric of ranking effectiveness, such as ndcg, map, err, etc. [19]\nin a large-scale web search system a user query can match thousands or millions of documents, but only a few of them are actually relevant for the user [23].\ntherefore, learning effective ranking functions in this scenario requires large gold standard datasets where each training query is associated with a few relevant documents (positive examples) and a large amount of irrelevant ones (negative examples)\nindeed, several studies confirm that a number of examples in the order of thousands per query is required [4, 5, 18]\nresearch in this field has focused on designing efficient [6] and effective algorithms that improve the state of the art, or on engineering new classes of features allowing to better model the relevance of a document to a query\n less effort has been spent in understanding how to deal with the unbalanced classes of positive and negative examples in the gold standard so as to maximize the effectiveness and robustness of the learned ranking model\n this aspect has not been fully investigated mainly because publicly available datasets contain a relatively low number of negative examples per query, thus preventing in-depth studies on the impact of class imbalance on ltr algorithms\nto investigate the issue of class imbalance in real-world ltr datasets, in this paper we contribute and study a new dataset with about 2.7k examples per query on average\nwe first investigate how the volume of negative examples impacts on a state-of-the-art ltr algorithm such as \u03bb-mart [21]\nexperimental results confirm that a large number of negative examples is required to train effective models\nwe also show that \u03bb-mart reaches a plateau, where increasing class imbalance neither harms or improves the accuracy achieved\nhowever we observe that not all the negative examples are equally important for the training process, and that it is hard for an algorithm do properly identify and exploit the most informative negative instances\nwe thus present a novel ltr algorithm, named selective gradient boosting (selgb) focusing during learning on the negative examples that are likely to be the most useful to improve the model learned so far\nto this purpose, we introduce a novel negative selection phase within a gradient boosting learning process.\nspecifically, selgb is designed as a variant of the \u03bb-mart algorithm that at each iteration limits the training set used to grow the tree ensemble to all the positive examples and to a sample of negative ones\nthe negative examples chosen at each step are the most informative ones, those that are most useful to reduce the misranking risk, i.e., the probability that a method ranks two randomly drawn instances incorrectly\nresults of the exhaustive experimental assessment conducted confirm that selgb is able to train models that significantly improve ndcg@10 over the ones generated by the reference \u03bb-mart algorithm\nin summary, we improve the state of the art in the ltr field with the following contributions\nwe propose selgb, a new gradient boosting algorithm designed as a variant of \u03bb-mart that iteratively selects the most \u201cinformative\u201d negative examples from the golden standard dataset\nthe proposed technique allows the selgb algorithms to focus during training on those negative examples that are likely to be the most useful to reduce the ranking risk of the scoring model learned so far and adjust the model correspondingly\n we provide a comprehensive experimental comparison showing that selgb outperforms the reference \u03bb-mart algorithm by up to +3.2% in terms of ndcg@10\nwe release the selgb source code and a new public ltr dataset to foster the research in this field and to allow the reproducibility of our results\nthe dataset is made up of 26,791,447 query-document pairs, produced starting from 10,000 queries sampled from a query log of a real-world search engine\non average, the dataset contains 2,679 documents per query\nto the best of our knowledge this is the largest public ltr dataset ever released, in terms of number of documents per query\nthe rest of the paper is structured as follow: section 2 discusses the related work while section 3 introduces the notation and the preliminaries needed to present the selective gradient boosting algorithm in section 4\nwe provide a comprehensive evaluation of selective gradient boosting against state-of-the-art competitors in section 5\nfinally, section 6 concludes the work and outlines future wor", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Learning an effective ranking function from a large number of query-document examples is a challenging task.\nIndeed, training sets where queries are associated with a few relevant documents and a large number of irrelevant ones are required to model real scenarios of Web search production systems, where a query can possibly retrieve thousands of matching documents, but only a few of them are actually relevant\nIn this paper, we propose Selective Gradient Boosting (SelGB), an algorithm addressing the Learning-to-Rank task by focusing on those irrelevant documents that are most likely to be mis-ranked, thus severely hindering the quality of the learned model\nSelGB exploits a novel technique minimizing the mis-ranking risk, i.e., the probability that two randomly drawn instances are ranked incorrectly, within a gradient boosting process that iteratively generates an additive ensemble of decision trees\nSpecifically, at every iteration and on a per query basis, SelGB selectively chooses among the training instances a small sample of negative examples enhancing the discriminative power of the learned model\nReproducible and comprehensive experiments conducted on a publicly available dataset show that SelGB exploits the diversity and variety of the negative examples selected to train tree ensembles that outperform models generated by state-of-the-art algorithms by achieving improvements of NDCG@10 up to 3.2"},
{"doc": "extensive research effort has been devoted to improving the effectiveness of recommendation algorithms [13, 17, 22, 29]; but one fundamental  question of \u201chow a system should explain those recommendations to its users\u201d has not received enough attention [38]\nthe lack of transparency [31] leaves users in a dilemma: a user can only assess the recommendation quality by taking the suggested actions, e.g., purchase the topranked items; however, in order for him/her to adopt the system\u2019s customized results, he/she needs to first build trust over the system\n explaining the automatically generated recommendations would bridge the gap\n arguably, the most important contribution of explanations is not to convince users to accept the customized results (i.e., promotion), but to allow them to make more informed and accurate decisions about which results to utilize (i.e., satisfaction) [5]\nexisting recommendation algorithms emphasize end-to-end optimization of performance metrics, such as root-mean-square error and normalized discounted cumulative gain, which are defined on numerical ratings or ranking orders reflecting a user\u2019s overall preference over a set of items\nvarious algorithms such as collaborative filtering [4, 18, 28, 29, 37] and factorization based methods [18, 22, 27] have been proposed to optimize those metrics\n  however, it is known that humans are complex autonomous systems: a click/purchase decision is usually a composition of multiple factors\nthe end-to-end learning scheme can hardly realize the underlying reasons behind a user\u2019s decision making process\nas a result, although such algorithms achieve great success in quantitative evaluations, they are still computerized oracles that merely give advice, but cannot be questioned, especially when a new or wrong recommendation has been made\nthis greatly limits the practical value of such recommendation algorithms\nwe argue that a good recommendation algorithm should consist of companion learning tasks focusing on different aspects of users\u2019 decisions over the recommended items, such that the observed final decisions (e.g., clicks or ratings) can be mutually explained by the associated observations\nin this work, we focus on opinionated review text that users provide in addition to their overall assessments of the recommended items, and aim to exploit such information to enhance and explain the recommendations\nfigure 1 illustrates how opinionated content in a user\u2019s review reflects the composition of his/her overall assessment (rating) of the item\nfor this given fourstar overall rating, three positively commented features contribute to his/her positive assessment, and one negatively commented feature explains his/her negative judgment\nif an algorithm could learn to predict not only the user\u2019s overall rating, but also the opinionated comment he/she would provide on this item (e.g., how would he/she endorse the features of this item), the recommendation will become self-explanatory\nand clearly these two predictions are not independent: the predicted overall assessment has to be supported by the predicted opinionated comments\ntherefore, the additional information introduced by the companion content modeling task would help improve the quality of recommendation task\nconsiderable effort has been devoted to utilizing user-generated opinionated content for providing text-based explanations.\none typical solution is to combine rating prediction with topic models [23, 36, 39].\nhowever, they fail to recognize the detailed opinions on each specific feature or topic, which inevitably leads to biased recommendations or wrong explanations, when a user criticizes a particular feature in an overall positive review\nanother type of solutions leverage phrase-level sentiment analysis [25, 38], which zoom into users\u2019 detailed feature-level opinions to explain the recommendations\nbut these solutions simply map feature-level comments into numeric ratings, a and thus ignore the detailed reason that the user likes/dislikes the feature of a particular item\nfor example, in figure 1 the user endorses the mobile phone\u2019s screen because of its large size and high definition; but the phrase-level sentiment analysis only discloses the user is in favor of this phone\u2019s screen feature\nit is impossible for such type of algorithms to explain how exactly the highlighted features of their recommendations match the user\u2019s specific preference\nwe focus on explaining factorization-based recommendation algorithms [4, 16] by taking a holistic view of item recommendation and sentiment analysis\nwe develop a joint tensor factorization solution to integrate two complementary tasks of user preference modeling for recommendation and opinionated content modeling for explanation, i.e., a multi-task learning approach [3, 10, 20]\nthe task of item recommendation is modeled by a three-way tensor over user, item and feature, to describe users\u2019 preferences on individual items\u2019 features, constructed by feature-level sentiment analysis in opinionated review content\nthe companion task of opinionated content analysis is modeled by another two three-way tensors, one over user, feature, opinionated phrase, and another over item, feature, opinionated phrase, both of which are constructed from user-generated review content\namong these two tensors, the first one specifies what kind of text descriptions a user usually provides to depict a particular feature of a given item, and the second describes what kind of opinionated comments an item often receives on its features.\nvia a joint factorization over these three tensors, we map users, items, features and opinionated phrases to a shared latent representation space.\nitem recommendation can be performed by projecting items onto the space spanned by the user factors; and explanations at different levels of granularity can be generated by projecting the features and opinionated phrases onto the space jointly spanned by the user and item factors\ncursed by the high dimensionality, sparsity is a serious issue in both learning tasks; our joint factorization method alleviates this challenge by exploiting relatedness between these two companion tasks\nextensive experimental comparisons between our proposed solution and existing explainable recommendation algorithms demonstrate the effectiveness of our solution in both item recommendation and explanation generation tasks in two different application scenarios, i.e., product recommendation based on amazon reviews and restaurant recommendation based on yelp reviews\nin particular, we perform serious user studies to investigate the utility of our explainable recommendation in practice\npositive user feedback further validates the value of our proposed solutio", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Explaining automatically generated recommendations allows users to make more informed and accurate decisions about which results to utilize, and therefore improves their satisfaction\nIn this work, we develop a multi-task learning solution for explainable recommendation\nTwo companion learning tasks of user preference modeling for recommendation and opinionated content modeling for explanation are integrated via a joint tensor factorization.\nAs a result, the algorithm predicts not only a user\u2019s preference over a list of items, i.e., recommendation, but also how the user would appreciate a particular item at the feature level, i.e., opinionated textual explanation\nExtensive experiments on two large collections of Amazon and Yelp reviews confirmed the effectiveness of our solution in both recommendation and explanation tasks, compared with several existing recommendation algorithms.\nAnd our extensive user study clearly demonstrates the practical value of the explainable recommendations generated by our algorith"},
{"doc": "nowadays, with the rapid development of scientific research, more and more scholarly papers have emerged\nat the same time, one of the best way to evaluate those papers is peer review (also known as refereeing) [3]\nfor most journals and conferences in the computer science field, peer review is used to help decide whether a paper submission should be accepted or rejected\nusually, a professional reviewer writes a detailed review text to introduce both the pros and the cons of a submitted paper and then gives an overall score to recommend a decision status (typically accept or reject)\nhowever, it is still unknown whether the review texts and the recommendation scores are consistent with each other or not\nif not, the review submission system should warn the reviewer if there is any potential mistake in review scores\nmoreover, if we can automatically highlight the reviewer\u2019s controversies (e.g., the major pros and cons in the review text) on the submitted paper, it will not only help the chair to write a comprehensive meta-review, but also be convenient for authors to further improve their paper\nin order to address the above problems, we investigate the sentiment analysis task in the new domain of peer review texts for scholarly papers, which makes an important step in the area of sentiment analysis and artificial intelligence\nas a first step in this research direction, the task investigated in this paper is simply defined as predicting the overall recommendation status (accept, reject, or sometimes borderline) and identifying the sentences with positive and negative sentiment polarities from the peer review text\nfor illustration, the above block in table 1 is an example review (the original text of this review text contains 399 words, due to space limit, we have to intercept a small fragment here as an example)\nin the review text, the contribution of the paper is first summarized, and then the reviewer expresses his opinions about the paper, including both positive aspects (see the underlined text) and negative aspects (see the italic text), we hope to be able to automatically determine the overall recommendation status (i.e., accept) of the reviewer and discovering reviewer\u2019s opinions from this review text (see the below block in table 1)\nsentiment analysis is a hot topic in the researches of natural language processing and has been investigated for a long time\nsome researchers have applied sentiment classification to a wide range of text domains such as product reviews [4, 9, 20, 24, 39, 41, 51], movie reviews [20, 27, 33, 41], tweets [10, 14, 18, 32], news articles [15], etc., and they have achieved some significant advances in these domains\nbut there is very few research on sentiment classification of the peer review texts, not only because the peer review corpus is hard to obtain but also because of the following main challenges\nlong length\nin our evaluation datasets constructed from the iclr open reviews, the average length of the review text is about 299 words, and longer text makes it more difficult to capture the overall sentiment polarity\nmixture of non-opinionated and opinionated texts\na review text contains a summary of the contributions or content in the paper\nin many reviews, the points regarding the writing are also contained, e.g., typos and grammatical errors\nthe reviewer\u2019s opinionated text is mixed with the non-opinionated text, and it is not easy to separate them\nmixture of pros and cons\nin a review, the reviewer usually talks about both merits and shortcomings of the paper\na paper usually has a few merits and a few shortcomings, so it is difficult to capture the key points of the reviewer as evidence for overall recommendation\nin this study, we propose a multiple instance learning network with a novel abstract-based memory mechanism (milam) to address this challenging task\nour neural network model is elaborately designed based on multiple instance learning network (mil) [2]\nmoreover, by considering the abstract information of a scholarly paper as memories, it leverages the relevant memories to control the representation of each sentence in the review text\nwe collect all peer review texts of iclr-20171, iclr-20182 (international conference on learning representations) as evaluation datasets which are publicly accessible on the openreview website\nevaluation results show that our proposed neural network model outperforms several baseline methods in different experimental settings, including some of the state-of-art neural network models for text classification and sentiment classification\nthe predicted recommendations for all reviews of a paper can be aggregated to predict the final decision (accept or reject) of the paper\nmoreover, our model can well identify opinionated sentences with polarity scores from the review text and the reviewer\u2019s opinions can be used to help authors to further improve their paper\nwe also find that the accuracy of overall recommendation prediction (accept and reject) achieved by our model can be improved by about 10 points if the borderline reviews are removed\nempirical analysis shows that the borderline reviews are hard to be classified\nthe results indicate the generally good consistency between the review texts and the recommended decisions, except for the borderline reviews\nthe major contributions of this paper are summarized as follows\n1) to the best of our knowledge, we are the first to investigate the sentiment analysis task in the domain of peer review texts for scholarly papers\nmoreover, we built two evaluation datasets by using all peer reviews of iclr-2017 and iclr-2018\n2) we propose a multiple instance learning network with a novel abstract-based memory mechanism (milam) to address this challenging task\n3) extensive experiments are conducted and evaluation results demonstrate the efficacy of our proposed model and show the great helpfulness of using abstract as memory\nwe also draw some interesting conclusions by empirical analysis\nin the rest of this paper, we will first introduce related work, and then describe our proposed model and discuss the evaluation results\nafter that, we conclude this pape", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Sentiment analysis has been widely explored in many text domains, including product reviews, movie reviews, tweets, and so on\nHowever, there are very fewstudies trying to perform sentiment analysis in the domain of peer reviews for scholarly papers, which are usually long and introducing both pros and cons of a paper submission\n In this paper, we for the first time investigate the task of automatically predicting the overall recommendation/decision (accept, reject, or sometimes borderline) and further identifying the sentences with positive and negative sentiment polarities from a peer review text written by a reviewer for a paper submission\nWe propose a multiple instance learning network with a novel abstract-based memory mechanism (MILAM) to address this challenging task\n  Two evaluation datasets are constructed from the ICLR open reviews and evaluation results verified the efficacy of our proposed model\nOur model much outperforms a few existing models in different experimental settings\n  We also find the generally good consistency between the review texts and the recommended decisions, except for the borderline review"},
{"doc": "collaborative filtering is one of the most successful ways to build recommender systems and has received significant success in the past decades [1, 21]\nspecifically, it infers each user\u2019s interests by collecting the user-item interaction history without any content information\namong all models of cf, latent factor based models have received great success in both academia and industry\n these models try to characterize both users and items in a same low latent space inferred from the historical user-item interaction matrix [25, 27]\nthen, the predicted preference of a user to an item could be reduced to comparing users and items in the low latent space\n despite the huge success, in the real-world systems, a user usually rates or experiences a small set of items in these applications, the data sparsity issue remains a key challenge for enhancing recommendation performance [1]. \nluckily, the emergence of online social networks greatly improves users\u2019 initiative on the internet, such as facebook, twitter, online social product review platform epinions, and location based social network gowalla\nin these social applications, users like to spread their preferences for items to their social connections (e.g., friends in a undirected social network and followers in a directed social network), and social influence effect is well recognized as a main factor in these platforms [14]\nthe social influence theory argues that, users are influenced by the social connections in the social network, leading to the homophily effect of similar preferences among social neighbors [3, 4]\nthus, by leveraging the social influence among users, social recommendation has become a popular way to tackle the data sparsity issue in traditional recommender systems and has been extensively studied [16, 17, 24, 34]\ne.g., jamali et al. proposed a social influence propagation based latent factor models for social recommendation [16], and ma et al. designed a latent factor based model with the social regularization of users\u2019 interests [24]\nin summary, these works focused on how to push the social influence theory among users in the recommendation process\nusually, the social influence strength was set equally for the social connections [16] or relied on a predefined static function [17, 24]\nsuccessful as they are, these social recommender systems assumed a general static assumption of users\u2019 interests over time\n in the real world social recommendation scenarios, users\u2019 preferences for items are not always stationary but evolve over time\nin fact, time has been recognized as an important type of information for modeling the dynamics of users\u2019 preferences in traditional recommender systems [9, 40, 44]\nresearchers proposed tensor factorization or temporal user latent factor based models to capture users\u2019 dynamic interests over time.\ninstead of capturing users\u2019 temporal interests, other researchers argued that each user\u2019s preference is composed of two parts: a general static interest that is stationary and does not evolve over time, and a complex dynamic interest that is easily influenced by the external environments.\nthus, some models have been proposed to tackle the temporal recommendation problem by combining static and dynamic interest modeling [20, 42]\nthese models showed better performance by simultaneously modeling users\u2019 static interests for temporal recommendation\nto summarize, all these temporal models relied on the variants of latent factor based models to capture users\u2019 dynamic interest.\nnevertheless, the inherent reasons for users\u2019 preference evolution are complex and non-linear, which are hard to be captured by these linear shallow latent factor based models\ntherefore, the performance of those approaches are still not satisfactory\nrecently, recurrent neural network (rnn) based approaches have shown promising potentials for capturing complex temporal patterns for time series tasks, such as sentence generation [19], and acoustic modeling [30]\nsome pioneering works attempted to introduce rnns for temporal recommendation [26, 38, 47].\n these rnn based recommendation models modeled the latent structure of each user at each time with a hidden state\nthese hidden states over time are learned from the recurrent neural networks to model the complex temporal patterns, which advance previous shallow temporal recommendation models\ndespite the success of applying rnns for temporal recommendation, to the best of our knowledge, few research works have attempted to tackle the temporal social recommendation task with rnns\nindeed, it is a non-trivial task due to the following two key challenges in this process\nfirst, as the rnn based models are good at modeling the complex dynamic user interests, how to design a model that unifies both users\u2019 dynamic interests and the static interests? second, in both the dynamic user interest and static user interest modeling part, different social connections would have different social influence strengths on users\nthis social influence strength issue is more challenging in the dynamic user interest modeling process due to the interplay between users\u2019 dynamic interests and the social influence\nthe dynamics of the social influence strength would affect the connected users\u2019 preferences, and users\u2019 evolving preferences would also affect their influence strengths to their social connections\nthus, how to model the complex interplay between social influence and users\u2019 interests over time becomes another challenge. \nto solve the above technical challenges, in this paper, we present an attentive recurrent network based approach for the temporal social recommendation task\nin the proposed approach, we model users\u2019 complex dynamic and general static preferences over time by leveraging social influence among users with two attention networks\nspecifically, we use a recurrent neural network model as the base model for dynamic interest modeling, where each user\u2019s dynamic preference over time could be introduced as a hidden temporal state in the neural network structure\n to capture the interplay between social influence and user dynamic interest over time, we build a dynamic social attention module in each hidden state to measure the temporal social influence strength\nthe dynamic attention network could selectively choose the social connections that have large influence for each user at each time, and a social aware recurrent neural network is proposed to capture users\u2019 complex latent interests over time\nin the general static preference modeling process, we augment each user\u2019s static interest part by introducing a static social attention module to model the stationary social influence among users\nthus, both users\u2019 complex dynamic interests and general static interests are fused in a unified framework with the attentive social modeling networks\nwe summarize the contributions of this paper as follows\nwe propose the problem of temporal social recommendation\nwe argue that it is important to take users\u2019 complex dynamic interest and general static interest into consideration, where both the dynamic interest modeling part and the general interest modeling part needs to leverage the social influence in social networks\nwe propose rnn based structure to capture users\u2019 complex dynamic interest and design a dynamic social attention network to measure the dynamic social influence of social connections over time\nwe also extend the static user interest modeling part with a static social attention network to measure the stationary social influence among users\nthese two parts are fused together for the temporal social recommendation task\nwe conduct extensive experimental results on two real-world datasets\nthe experimental results clearly demonstrate the superiority of our proposed model compared to the baseline", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Collaborative filtering (CF) is one of the most popular techniques for building recommender systems\n To alleviate the data sparsity issue in CF, social recommendation has emerged by leveraging social influence among users for better recommendation performance\n In these systems, uses\u2019 preferences over time are determined by their temporal dynamic interests as well as the general static interests\nIn the meantime, the complex interplay between users\u2019 internal interests and the social influence from the social network drives the evolution of users\u2019 preferences over time.\nNevertheless, traditional approaches either neglected the social network structure for temporal recommendation or assumed a static social influence strength for static social recommendation\n Thus, the problem of how to leverage social influence to enhance temporal social recommendation performance remains pretty much open\n To this end, in this paper, we present an attentive recurrent network based approach for temporal social recommendation\nIn the proposed approach, we model users\u2019 complex dynamic and general static preferences over time by fusing social influence among users with two attention networks\nSpecifically, in the dynamic preference modeling process, we design a dynamic social aware recurrent neural network to capture users\u2019 complex latent interests over time, where a temporal attention network is proposed to learn the temporal social influence over time\nIn the general static preference modeling process, we characterize each user\u2019s static interest by introducing a static social attention network to model the stationary social influence among users\nThe output of the dynamic preferences and the static preferences are combined together in a unified end-to-end framework for the temporal social recommendation task\nFinally, experimental results on two real-world datasets clearly show the superiority of our proposed model compared to the baseline"},
{"doc": "twitter-like social media are some of the most popular and influential platforms for information generation and diffusion\naccording to the definition by twitter, a tweet that contains @username is called a mention\nin addition, if a tweet includes multiple @username, all of those people will see it in their own notification tabs\nhence, twitter-like microblogging users would like to mention their friends or celebrities to report new events, promote products, share experiences, or participate in discussions\nwhen an appropriate mention is recommended, a user could increase their exposure, promote their reputation, attract more followers, and accelerate the dissemination of information across the platform\naccording to the quarterly report released by twitter, it had 330 million active users monthly, and the average number of followers per user was 482\nhence, it would be beneficial to have a small number of candidates when users want to mention others in a specific tweet. \nprevious works have studied various aspects of the mention recommendation problem\nvarious supervised methods with manually constructed features like tag similarity and text similarity have been proposed to perform this task and promote tweet diffusion [21, 33]\nlinguistic topic models [20, 25] and support vector machine models [31] have also been used to perform this task\ninstead of trying to expand the diffusion of tweets, some works have focused on recommending a similar interest person [35\u201337, 40]\nbecause the post history of users plays quite an important role in the mention recommendation task, different kinds of resources have also been taken into consideration [5, 11, 12]\nchen et al. [5] incorporated the user\u2019s own tweet history, their retweet history, and the social relations between users to capture personal interests\n gong et al. [11] treated the recommendation task as a topical translation problem with the addition of tweet content and user histories\nin addition to feature engineering for machine learning models, huang et al. [12] proposed a neural network-based method combined with the external memory of users\u2019 history\nmoreover, neural network-based methods have achieved better performances than other kinds of methods\nalthough some research has been done on the mention recommendation task, most of the previous methods only focused on the use of textual information\n however, according to the statistics, more than 42% of tweets include more than one image\nmoreover, tweets with images are 150% more likely to get retweets than text-only tweets3\nhence, processing these multimodal tweets has become an important task\nfigure 1 gives a multimodal tweet example\nafter reading the tweet content \u201cmy first mac purchase,\u201d we probably think the user bought a computer or laptop by apple, which usually called a \u201cmac.\nhowever, in this tweet, \u201cmac\u201d should be a lipstick of the makeup brand \u201cmac.\nwith only textual information, it may be difficult to determine what \u201cmac\u201d is\nto address this issue, we present a novel multimodal model to combine textual and visual information\n some previous works simply combine the text feature vector and image feature [1].\nhowever, the correct entities or other meaningful content are often only related to a small part of the image or text\nunder these conditions, using a vector to represent the image or text may lead to an incorrect final prediction as a result of the noise made by the irrelevant or unimportant part of the image or text\nmotivated by work on the visual question answering task [38] and the generation of image descriptions [13], we incorporated an attention mechanism to process the textual information and visual information of a multimodal tweet\nwith the help of the proposed attention mechanism, our model can focus on important parts of the visual and textual information of the tweet, which can represent almost the complete meaning of the multimodal tweet\nmore specifically, the proposed network architecture is a neural memory network combined with a cross attention mechanism.\nthis model can take the content of a tweet, history of its author, and history interests of candidate users into consideration, simultaneously\nmeanwhile, the model can make good use of visual information by treating the image content as an assist information\nfinally, predictions are calculated based on the similarity features extracted from the multimodal information of tweets, the users\u2019 histories and the candidate users\u2019 interests\nto demonstrate the effectiveness of our model, we performed experiments on a large data set collected from twitter\nthe experimental results showed that the proposed method could achieve better performance than state-of-the-art methods using textual information only\nthe main contributions of our work can be summarized as follows\nthe mention recommendation task for multimodal tweets is novel and has not been carefully studied in previous methods\nin this paper, we defined the problem and evaluated several methods for this task\nwe propose a novel cross-attention memory network that incorporates tweet-guided visual attention\nit takes the content of a tweet, interests of the user, and interests of the author into consideration\nexperimental results using a dataset constructed by us from twitter demonstrated that our model could achieve significantly better performance than current state-of-the-art method", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The users of Twitter-like social media normally use the \u201c@\u201d sign to select a suitable person to mention\n It is a significant role in promoting the user experience and information propagation.\nTo help users easily find the usernames they want to mention, the mention recommendation task has received considerable attention in recent years\nPrevious methods only incorporated textual information when performing this task\nHowever, many users not only post texts on social media but also the corresponding images\nThese images can provide additional information that is not included in the text, which could be helpful in improving the accuracy of a mention recommendation\nTo make full use of textual and visual information, we propose a novel cross-attention memory network to perform the mention recommendation task for multimodal tweets\nWe incorporate the interests of users with external memory and use the cross-attention mechanism to extract both textual and visual information\nExperimental results on a dataset collected from Twitter demonstrated that the proposed method can achieve better performance than state-of-the-art methods that use textual information onl"},
{"doc": "uncovering user interests and expertise is a vital component of search and recommendation systems [14, 16, 30, 37, 38]\nfor example, knowing a user is interested in tennis and proficient in python can be used to augment newsfeed ranking algorithms to surface high-quality content, improve item-based recommenders by leveraging the topical expertise of knowledgeable users [3], and enhance personalized web search [26, 30] and targeted ads [2]\nwhile many existing user topical profiles focus on a global view of each user, there are important geo-social factors that are critical to consider: (i) first, individual users may be perceived differently in different places; and (ii) users with similar topical profiles can have very different geo-spatial impact\nwithout careful consideration of each of these factors, user topical profiles will lead to error-prone recommendation (e.g., recommending content generated by local foodies to people interested in food but living far away), low-quality advertising (e.g., placing online ads of a new product by a tennis equipment brand company in areas little known by local people) and so on\nto illustrate these two geo-social factors, figure 2 shows the heat maps of the locations of twitter users who have labeled michael moore (@mmflint) and roy blunt (@royblunt) using twitter lists\nthis aggregate crowd-labeling of twitter users has been used previously to derive user topical profiles [4, 9, 11]\nfor factor (i), we see that @mmflint, as a filmmaker, is mainly known in new york and la, while he has a much broader impact for politics in regions such as san francisco and d.c\nfor factor (ii), @mmflint and @royblunt are both known for politics, but @royblunt is known mainly in missouri and d.c. while @mmflint is known in a much broader geo-scope\nhence, these observations motivate the need for a careful study of the impact of social-spatial properties on the creation of highquality user topical profiles\nwe propose in this paper to complement traditional user topical profiles with new fine-grained user geo-topic profiles\nthese profiles are designed to capture the variations of user popularity for topics across geo-locations; essentially, geo-topic profile is a multi-dimensional concept to describe and model a user, and is expected to capture the pair-wise interactions involving geo-locations and users\u2019 topical profiles (see figure 1)\nmodeling user geo-topic profiles faces two major challenges, however: (i) they are often overdispersed\nunlike the ratings or content studied in many previous works [6, 12, 13], the popularity counts in geo-topic profiles are of great variability in terms of variance which is caused by user heterogeneity (some users are much more popular than others); and (ii) they are often extremely sparse due to multi-dimensionality, with many users often known for very few topics at certain locations\ngiven these challenges, we first propose a multi-layered (twolayered in our case) bayesian hierarchical user factorization which generalizes poisson gamma belief network [40] from modeling two dimensional non-negative counts to multi-dimensional heterogeneous counts\nthe extra layer of user factorization learns a more expressive user model than that of single-layered factorization by allowing a larger variance-to-mean ratio on user geo-topic profiles, thus making it better equipped at handling overdispersion and user heterogeneity\nto alleviate the sparsity issue, we investigate how user\u2019s contexts, specifically, geo-location and social ties, correlate with one\u2019s geo-topic profile, and then propose to integrate them into the two-layered hierarchical model to learn better representation of user\u2019s geo-topic preference by others\ndue to the non-conjugacy of the multi-layered factorization scheme, we exploit a data augmentation scheme for negative binomial (nb) distribution and develop an efficient closed-form gibbs sampling formula for scalable inference\nwe summarize the contributions as follows\nfirst, we introduce multi-dimensional geo-topic profile to capture the pair-wise interactions between geo-location and user\u2019s topical profile, and formulate the problem of learning user\u2019s geotopic profile to infer user\u2019s location-sensitive topical profile\nsecond, to overcome overdispersed popularity counts caused by user heterogeneity, we propose a two-layered bayesian hierarchical user factorization (bhuf) generative framework, which can be easily generalized to deep user factorization\nthird, to alleviate sparsity, we investigate the impact of user\u2019s contexts, specifically, user\u2019s geo-location and social ties, on user\u2019s geo-topic profile, and propose an enhanced model (bhuf+) by exploiting user\u2019s contexts\nwe then develop an efficient closedform gibbs sampling scheme for inference using data augmentation schemes for nb distribution\nfinally, we evaluate bhuf and bhuf+ against several baselines over gps-tagged twitter datasets, and observe that bhuf gives about 5%~13% improvement in precision and recall over the best alternative one-layered baseline, and an additional 6%~11% improvement with user\u2019s geo-location and social contex", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Understanding user interests and expertise is a vital component toward creating rich user models for information personalization in social media, recommender systems and web search\n To capture the pair-wise interactions between geo-location and user\u2019s topical profile in social-spatial systems, we propose the modeling of fine-grained and multi-dimensional user geo-topic profiles.\nWe then propose a two-layered Bayesian hierarchical user factorization generative framework to overcome user heterogeneity and another enhanced model integrated with user\u2019s contextual information to alleviate multi-dimensional sparsity\nThrough extensive experiments, we find the proposed model leads to a 5~13% improvement in precision and recall over the alternative baselines and an additional 6~11% improvement with the integration of user\u2019s context"},
{"doc": "recent years have witnessed a rapid growth in the use of mobile devices, enabling people to access the internet in various contexts\nmore than 77% of americans now own a smartphone, with an increasing trend in terms of the time people spend on their phones\nas of 2016, the average u.s. user spends 5 hours on mobile devices per day, with just 8% of it spent in the phone\u2019s browser.\nin fact, people spend most of their time (72%) using apps that have their own search feature\nmoreover, google play store now features more than 3.5 million apps and users install an average of 35 mobile apps on their phones, using half of them regularly\nmore recently, with the release of intelligent assistants, such as google assistant and apple siri, people are experiencing mobile search through a single voice-based interface\nthese systems introduce several research challenges\ngiven that people spend most of their times in apps and, as a consequence, most of their search interactions would be with apps (rather than a browser), one limitation is that users are unable to use a conversational system to search within many apps\nthis suggests the need for a unified search framework that replaces all the search boxes in the apps, with a single search box.\nwith such a framework, the user can submit a query through this system which will identify the target app(s) for the issued query\nthe query is then routed to the identified target apps and the results are displayed in a unified interface\nin this work, we are particularly interested in taking the first step towards developing a unified search framework for mobile devices by introducing and studying the task of target apps selection, which is defined as identifying the target app(s) for a given query\nto this end, we built a collection of cross-app search queries through crowdsourcing, which is released for research purposes\nour crowdsourcing experiment consists of two parts: we initially asked crowdworkers to explain their latest search experience on their smartphones and used them to define various realistic mobile search tasks\nthen, we asked another set of workers to select the apps they would choose to complete the tasks as well as the query they would submit\nwe investigate various aspects of user behaviors while completing a search task\nfor instance, we show that users choose to complete most of the search tasks using two apps\n in addition, we demonstrate that for the majority of the search tasks, most of the users prefer not to use google search\nfrom the lessons learned from our data analysis, we propose two simple yet efficient neural target apps selection models\nour first model looks at the problem as a ranking task and produces a score for a given query-app pair\nwe study two different training settings for this model\nour second framework, on the other hand, casts the problem as a multi-label classification task\n both neural approaches, called ntas, learn a high-dimensional representation for each app\nour experiments demonstrate that our model significantly outperforms a set of state-of-the-art models in this task\nin summary, the main contributions of this paper include\ndesigning and conducting two crowdsourcing tasks for collecting cross-app search queries for real-life search tasks\nthe tasks and queries are publicly available for research purposes\n presenting the first study of user behaviors while searching with different apps as well as their search queries\nin particular, we study the attributes of the search queries that are submitted to different apps and user behaviors in terms of the apps they chose to complete a search task\nproposing two neural models for target apps selection\nevaluating the performance of state-of-the-art retrieval models for this task and comparing them against the proposed method\nour analyses and experiments suggest specific future directions in this research are", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "With the recent growth of conversational systems and intelligent assistants such as Apple Siri and Google Assistant, mobile devices are becoming even more pervasive in our lives\nAs a consequence, users are getting engaged with the mobile apps and frequently search for an information need in their apps\nHowever, users cannot search within their apps through their intelligent assistants\nThis requires a unified mobile search framework that identifies the target app(s) for the user\u2019s query, submits the query to the app(s), and presents the results to the user\nIn this paper, we take the first step forward towards developing unified mobile search\nIn more detail, we introduce and study the task of target apps selection, which has various potential real-world applications\nTo this aim, we analyze attributes of search queries as well as user behaviors, while searching with different mobile apps\nThe analyses are done based on thousands of queries that we collected through crowdsourcing\nWe finally study the performance of state-of-the-art retrieval models for this task and propose two simple yet effective neural models that significantly outperform the baselines\nOur neural approaches are based on learning high-dimensional representations for mobile apps\nOur analyses and experiments suggest specific future directions in this research are"},
{"doc": "dialogue act recognition (dar) is an essential problem in modeling and detecting conversation structure\nthe goal of dar is to attach semantic labels to each utterance in a conversation and recognize the speaker\u2019s intention, which can be regarded as a sequence labeling task\nmany applications have benefited from the use of automatic dialogue act recognition such as dialogue systems, machine translation, automatic speech recognition, topic identification and talking avatars [20] [14].\none of the primary applications of dar is to support task-oriented conversation agent system.\n knowing the past utterances of da can help ease the prediction of the current da state, thus help to narrow the range of utterance generation topics for the current turn\nfor instance, the \"greeting\" and \"farewell\" acts are often followed with another same type utterances, the \"answer\" act often responds to the former \"question\" type utterance\nthus if we can correctly recognize the current dialogue act, we can easily predict the following utterance act and generate a corresponding response\ntable 1 demonstrates a snippet of the kind of conversation structure in which we are interested\nthe essential problem of dar lies in predicting the utterance\u2019s act by referring to contextual utterances with act labels\nmost of the existing models adopt handcrafted features and formulate the dar as a multi-classification problem\nhowever, these methods which adopt feature engineering process and multi-classification algorithms reveal fatal weakness from two aspects: first, they are labor intensive and cannot scale up well across different datasets.\nfurthermore, they abandon the useful correlation information among contextual utterances\ntypical multi-classification algorithms like svm, naive bayes [13] [2] [38] can not account for the contextual dependencies and classify the da label in isolation\nit is evident that during a conversation, the speaker\u2019s intent is influenced by the former utterance such as the previous \"greeting\" and \"farewell\" examples\nto tackle these two problems, some works have turned to structured prediction algorithm along with deep learning tactics such as drlm-conditional [17], lstm-softmax [20] and rcnn [19]\nhowever, most of them fail to utilize the empirical effectiveness of attention in the structured graphical networks and rely completely on the hidden layers of the network, which may cause the structural bias\n a further limitation is that although these works claim they have considered the contextual correlations, in fact they view the whole conversation as a flat sequence and neglect the dual dependencies in the utterance level and act level [4] [16] [30]\nuntil now, the achieved performances in dar field are still far behind human annotator\u2019s accuracy\nin this paper, we present the problem of dar from the viewpoint of extending richer crf-attentive structured dependencies along with deep neural network\n for simplicity, we call the framework as crf-asn (crf-attentive structured network)\nthe main idea is inspired by the long range contextual capturing method proposed by liu et al. [28] and the structural attention proposed by kim et al. [21].\nspecifically, we utilize the hierarchical semantic inference integrated with memory mechanism on the utterance modeling\nthe memory mechanism is adopted to enable the model to look beyond localized features and have access to the entire sequence\nthe hierarchical semantic modeling learns different levels of granularity including word level, utterance level and conversation level\nwe then develop internal structured attention network on the linear-chain conditional random field (crf) to specify structural dependencies in a soft manner\nthis approach generalizes the soft-selection attention on the structural crf dependencies and takes into account the contextual dependencies on the nearing utterances\nit is notable that the whole process is differentiable thus can be trained in an end-to-end manner\nthe main contributions of this paper are as follows\nunlike the previous studies, we study dialogue act recognition from the viewpoint of extending rich crf-attentive structured dependencies.\nthe proposed crf structural attention on the dar problem provides an alternative approach to encode the internal utterance inference with dialogue acts\nwe propose the hierarchical deep recurrent neural network with memory enhanced mechanism to adequately model the utterance semantic representations\nthe proposed framework can be trained end-to-end from scratch and can be easily extended across different dialogue tasks\nwe conduct extensive experiments on two popular datasets swda and mrda to show that our method outperform several state-of-the-art solutions on the problem\nit is worth noting that our approach has achieved nearly close to the human annotator\u2019s performance on swda within 2% gap, which is very convincin", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Dialogue Act Recognition (DAR) is a challenging problem in dialogue interpretation, which aims to associate semantic labels to utterances and characterize the speaker\u2019s intention\nCurrently, many existing approaches formulate theDAR problem ranging from multiclassification to structured prediction, which suffer from handcrafted feature extensions and attentive contextual dependencies\nIn this paper, we tackle the problem of DAR from the viewpoint of extending richer Conditional Random Field (CRF) structured dependencies without abandoning end-to-end training\nWe incorporate hierarchical semantic inference with memory mechanism on the utterance modeling at multiple levels\nWe then utilize the structured attention network on the linear-chain CRF to dynamically separate the utterances into cliques\nThe extensive experiments on two primary benchmark datasets Switchboard Dialogue Act (SWDA) and Meeting Recorder Dialogue Act (MRDA) datasets show that our method achieves better performance than other state-of-the-art solutions to the proble"},
{"doc": "as intelligent assistants such as siri (apple), facebook messenger, amazon alexa, google assistant, enter the daily life of users, research on conversational information systems is becoming increasingly important\n there are mainly three kinds of conversational systems (i.e. dialogue systems): chit-chat, informational chat and task oriented chat\nchit-chat systems are focusing on information social chat and try to interact with human-like reasonable or interesting responses [8][25]\ninformational chatbots try to help user find information or directly answer user questions.\ntask oriented chatbots try to help users finish a specific task, such as booking a flight or canceling a trip\nand they are usually built for a close domain\n this paper is related to both informational chat and task oriented chat. \ndue to the big commercial potential, there are quite some activities on task oriented conversational chatbots that can interact with users to help them find products/services\ncompanies like amazon, google, ebay, alibaba are all rolling out these chatbots\nmost of existing works are focusing on natural language processing or semantic rich search solutions for dialogue systems\nthe most notable recent related work is [5], which focuses on enabling user to query knowledge base interactively\non the other hand, researchers have demonstrated the importance of recommender systems in e-commerce websites and applications\nto improve the success or conversion rate of a shopping/sales chatbot, we argue that one should integrate recommendation techniques into conversational systems.\nintuitively, this can benefit both recommender systems and dialog systems\nfor dialogue systems, good recommendations based on users\u2019 previous purchasing or rating history can better fulfill user\u2019s information need, and create more business opportunities\nfor recommender systems, dialogue systems can provide more detailed information about user intentions, such as user preferred price range or the location of a restaurant, by interactively soliciting and identifying user intentions based on multi-round natural language conversation\nthis motivates us to study how to build a conversational recommender system\nthis paper tries to integrate search and recommendation techniques with conversational systems seamlessly.\nwe build a chat agent that can assist users to find items interactively\nwith the recent breakthrough of deep learning technologies and a better understanding of search and recommendation, we can approach this problem with a new perspective and a set of enabling technologies\nsimilar to other dialog systems, our system has three major components\nfirst, a natural language understanding (nlu) module for analyzing each user utterance, keeping track of the user\u2019s dialogue history and constantly updating the user\u2019s intention\nthis nlu module focuses on extracting item specific meta data\nsecond, we propose a dialogue management (dm) module that decides which action to take given the current state\nthis dm module has an action space defined specifically for this task\nit is well integrated with an external recommender system\nthe third component is a natural language generation module to generate response to the user\nthis framework enables us to build a conversational search and recommender system that can decide when and how to gather information from users and make recommendations based on a user\u2019s past purchasing history and context information in the current session\nfor the nlu module, we train a deep belief tracker to analyze a user\u2019s current utterance based on context and extract the facet values of the targeted item from the user utterance\nits output is used to update the current user intention, which is represented as a user query that is a set of facet-value pairs about the target\nthe user query will be used by both the dialogue manager and the recommender system.\nfor the dm module, we train a deep policy network that decides which machine action to take at each turn given the current user query and long term user preferences learned by the recommender system\nthe action could be asking the user for information about a particular facet or recommending a list of products\nthe deep policy network selects an action that maximizes the expected reward in the entire conversation session\nwhen the user query collected so far is sufficient to identify the user\u2019s information need, the optimal action usually is recommending a list of items that is personalized for the user\nwhen the user query collected is not sufficient, the optimal action usually is asking for more informatio", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "A personalized conversational sales agent could have much commercial potential\nE-commerce companies such as Amazon, eBay, JD, Alibaba etc. are piloting such kind of agents with their users\nHowever, the research on this topic is very limited and existing solutions are either based on single round adhoc search engine or traditional multi round dialog system\nThey usually only utilize user inputs in the current session, ignoring users\u2019 long term preferences\nOn the other hand, it is well known that sales conversion rate can be greatly improved based on recommender systems, which learn user preferences based on past purchasing behavior and optimize business oriented metrics such as conversion rate or expected revenue\nIn this work, we propose to integrate research in dialog systems and recommender systems into a novel and unified deep reinforcement learning framework to build a personalized conversational recommendation agent that optimizes a per session based utility function\nIn particular, we propose to represent a user conversation history as a semi-structured user query with facet-value pairs\n This query is generated and updated by belief tracker that analyzes natural language utterances of user at each step\nWe propose a set of machine actions tailored for recommendation agents and train a deep policy network to decide which action (i.e. asking for the value of a facet or making a recommendation) the agent should take at each step\nWe train a personalized recommendation model that uses both the user\u2019s past ratings and user query collected in the current conversational session when making rating predictions and generating recommendations\nSuch a conversational system often tries to collect user preferences by asking questions\nOnce enough user preference is collected, it makes personalized recommendations to the user\nWe perform both simulation experiments and real online user studies to demonstrate the effectiveness of the proposed framewor"},
{"doc": "people at the scene of a disaster post information about the disaster on microblogs in real time\nemergency responses during natural or man-made disasters use information available on social media platforms such as twitter\nvolunteers and other support personnel generate reports or summaries of the relevant tweets posted via twitter that the responders can then use to address the needs of people located in disaster areas [11].\nsuch manual intervention may not scale given the volume of data produced within a short time interval during a disaster\nmoreover, different stakeholders and responders need information at varying levels of granularities\nsome stakeholders may want to obtain overall situational updates for a given day as a short summary or report (high-level information need) or specific updates for a particular class, such as \u2018infrastructure damage\u2019, \u2018shelter needs\u2019 etc. (class-specific information need)\nseveral works on disaster-specific summarization [13, 17, 21, 22] in recent times proposed algorithms that mostly provide a general summary of the whole event\nhowever, different stakeholders [29] like rescue workers, government agencies, field experts, common people, etc. have different information needs\ntweets posted during disasters contain information about various classes like \u2018infrastructure damage\u2019, \u2018missing or found people\u2019\nthey can be classified using classification systems, e.g., aidr [12]\nto address the multi-dimensional needs of different stakeholders, we propose a perspective-based tweet summarization technique using an integer linear programming (ilp) framework.\nthe framework provides flexibility to add constraints that capture the information needs of end-users (see section 5)\nfurthermore, existing works hardly leverage any disaster-specific trait to generate summaries\nevery disaster witnesses a series of small-scale emergencies such as \u2018a bridge collapsing\u2019, \u2018airport getting shut\u2019, \u2018medical aid reaching an area\u2019, \u2018family members being stranded\u2019 etc - a summary on disaster may get enriched by including information of such sub-events\nwhile interacting with experts at the united nations office for the coordination of humanitarian affairs (ocha), we realized that a summary can be enriched, if we include a representative but diverse sample of the important sub-events related to the disaster\ngiven the volume of the streaming data on a microblog like twitter, the sub-event identification has to be fast (and thereby simple) but effective\nwe use a dependency parser to identify noun-verb pairs representing sub-events\nfurther, we rank them based on the frequency of co-occurrence of their constituent nouns and verbs in the corpus\nsubsequent to the identification and ranking, maximization of high-ranked sub-events becomes one of the objectives of the linear program driving our tweet summarization framework\nour summarizer also utilizes multiple criteria where the constraints capture desirable properties of a good summary, and uses a fast ilp solver to generate summaries efficiently\nthe simplicity as well as the robustness of our approach makes it successful; we show empirically that despite being simple, this method works very well in practice\nwe evaluate the efficacy of our (a) sub-event identification, and (b) tweet summarization algorithms against recently developed baselines [13, 17, 21, 22].\nwe evaluate our proposed methods on 1.87m, 0.49m, and 0.24m tweets collected using the aidr platform [12] corresponding to the 2015 nepal earthquake, the 2014 typhoon hagupit, and the 2014 pakistan flood respectively using both traditional ir metrics and crowdsourcing\nour sub-event identification algorithm outperforms many state-of-the-art techniques [1, 2, 20, 30]\nwe find that elaborate clustering and dependency parsing based techniques perform poorly on \u2018noisy\u2019 tweet data\nour proposed tweet summarization method performs 6-30% better in terms of rouge-1 f-score than existing methods\nwe crowdsource the evaluation of the quality of summaries and show that our method generates summaries that are rated to be significantly superior for post-disaster situation assessment compared to prior approaches [13, 17, 21, 22] in terms of information coverage, diversity, and readability (see section 6)\nwhen we present our results, to increase readability, we highlight the sub-event keyphrases in the summary\nan overwhelming majority of crowd-workers opined that highlighting helps them to grasp the situation summarized quickly\nas a final contribution, we have made the codes and datasets publicly available at http: //www.cnergres.iitkgp.ac.in/subeventsummarizer/dataset.ht", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "In recent times, humanitarian organizations increasingly rely on social media to search for information useful for disaster response\nThese organizations have varying information needs ranging from general situational awareness (i.e., to understand a bigger picture) to focused information needs e.g., about infrastructure damage, urgent needs of affected people.\nThis research proposes a novel approach to help crisis responders fulfill their information needs at different levels of granularities\nSpecifically, the proposed approach presents simple algorithms to identify sub-events and generate summaries of big volume of messages around those events using an Integer Linear Programming (ILP) technique\nExtensive evaluation on a large set of real world Twitter dataset shows (a). our algorithm can identify important sub-events with high recall (b). the summarization scheme shows (6\u201430%) higher accuracy of our system compared to many other state-of-the-art techniques\nThe simplicity of the algorithms ensures that the entire task is done in real time which is needed for practical deployment of the syste"},
{"doc": "personalization techniques in modern information retrieval systems are a double-edged sword\nsearch engines trace, analyze and exploit their users\u2019 personal information [29, 47] and behavior signals [18, 32, 45, 46] to infer what the users are looking for, when, and in what context, so as to deliver more relevant and customized content or advertising [13, 50]\nalthough personalization techniques largely increase the utility of provided services on a per-user basis [17], however, it is this very dimension that raises serious public concerns about privacy infringement [12, 48]\naccording to [36], almost threequarters of search engine users in the u.s. are not okay with their personal information being tracked and used to personalize their future search\ncurrent solutions for privacy protection in online systems mostly focus on the identifiability aspect of privacy, i.e., who issued the query, via providing secured communication [15, 37], encrypted data storage [10, 35] and releasing [19, 24]\nhowever, another important aspect of privacy, linkability, i.e., determining the interests of an individual from their observed behaviors, has not received enough attention\nspecifically, linkability is what enables a service provider to link multiple queries to the same user, and thereby learn detailed information about a user\u2019s interests\naccording to jones et al.\u2019s study in [28], a simple supervised classifier based on the textual query content recorded in search engine logs can link a sequence of queries to a set of candidate users with known gender, age and location; and this set is 300-600 times smaller than a random chance would allow\nthis leaves the users with little control to avoid \u201ccurious\u201d systems abusing their personal information, e.g., targeted advertising [22] and digital discrimination [34]\nin this work,we develop intent-aware query obfuscation solution to protect the linkability aspect of privacy in a personalized web search system\nin particular, we focus on search log based personalization techniques [17, 41, 46, 47], which have been extensively studied in literature and widely used in practical systems\nin such type of personalization methods, search engines profile users with their historical queries and clicks and use such profiles to customize future system output, such as the search results\nat a high level, the basic idea behind our solution is to hide a user\u2019s true search intents among a set of randomly generated but loosely related cover queries and clicks.\nin order to compensate the degenerated search quality that is caused by the injected noise, on the client-side, we maintain a noise-free user profile to re-rank the received search results\nour proposed solution is totally client-centered, with no support required from the search engine side, which makes it general and adaptable to many personalized application scenarios, e.g., recommender systems\nthe key challenge in this research is to ensure the plausibility of the generated cover queries, not only with respect to the user\u2019s current query but also to the sequence of his/her previous queries and clicks in the same search task [27, 49].\nexisting studies in query obfuscation only focus on the first aspect of plausibility\nfor example, latent semantic analysis [33] and topic models [2] have been used to cluster queries into semantically coherent groups; every time, cover queries are randomly generated from qualitatively similar but different query groups with respect to the true user query\nhowever, users\u2019 true search intents usually span a wide spectrum in practice; for complex needs like health diagnosis, they often submit multiple queries and even continue the search for several days [1]\nas a result, the strong dependency between a user\u2019s sequential search behaviors leave the genuine query sequence distinctive, as the cover queries are independently generated for each query in the same task from those conventional query obfuscation methods\nwe address the challenge by generating sequences of related cover queries to form cover tasks with respect to a user\u2019s gradually developed search tasks\nto ensure semantic relatedness within the sequence of cover queries, we sample queries from a set of hierarchically organized language models based on the topic ontology defined in the open directory project (odp) [44]\nto make the sampled queries comparable to the genuine ones in plausibility, we adopt the rejection sampling method [21] to ensure the entropy of sampled queries is close to that of the enuine query\nadditionally, as query term matching still plays a very important role in modern search engine [17, 18], we submit the genuine user query together with the cover queries and only return the search results for genuine queries to the user to ensure utility of the search results\nas a result, there is a trade-off between privacy protection our solution provides and the utility of search results; and the number of generated cover queries controls such trade-off\nquantitatively evaluating the effectiveness of privacy protection is also challenging\nprevious studies have shown that people do not necessarily reveal their true privacy needs in laboratory experiments and/or questionnaires [14]\nand it is even more challenging (if not impossible) for third-parties to judge privacy needs from the logged search history because different users hold different criteria about privacy [5, 12]\nin this work, we assume all user queries are sensitive and need privacy protection; and we measure the change of entropy between the prior and posterior distributions over the search intents inferred for a user after a genuine search task is finished\nan algorithm is considered as a good privacy protection solution if it creates similar changes of entropy in the cover queries to those in the genuine queries, i.e., bayes-optimal privacy [30]\nin our empirical evaluations, promising improvement in privacy protection and search quality measured by a distinct set of performance metrics confirm the effectiveness of our proposed solutio", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Modern web search engines exploit users\u2019 search history to personalize search results, with a goal of improving their service utility on a per-user basis\nBut it is this very dimension that leads to the risk of privacy infringement and raises serious public concerns\nIn this work, we propose a client-centered intent-aware query obfuscation solution for protecting user privacy in a personalized web search scenario\nIn our solution, each user query is submitted with l additional cover queries and corresponding clicks, which act as decoys to mask users\u2019 genuine search intent from a search engine\nThe cover queries are sequentially sampled from a set of hierarchically organized language models to ensure the coherency of fake search intents in a cover search task\nOur approach emphasizes the plausibility of generated cover queries, not only to the current genuine query but also to previous queries in the same task, to increase the complexity for a search engine to identify a user\u2019s true intent\nWe also develop two new metrics from an information theoretic perspective to evaluate the effectiveness of provided privacy protection\nComprehensive experiment comparisons with state-of-the-art query obfuscation techniques are performed on the public AOL search log, and the propitious results substantiate the effectiveness of our solutio"},
{"doc": "for centuries, text has been used to convey information between human beings through books, letters, newspapers and magazines\nwith the advent of the digital age, more and more textual data is being processed and analyzed by machines\ntypical tasks include text classification, which is used in particular for spam filtering [36] and automated email routing [6], document retrieval [38], where indexed documents are retrieved and ranked according to search queries, sentiment analysis [23], and a wide variety of other tasks in the information retrieval (ir) and text mining domains\nin many cases, it is desirable for an author that his writings stay anonymous\nthis could be the case if the textual data contains sensitive information about the author, for instance in search queries\nnegative feedback from customer surveys might negatively impact business relations if the author or his company is known, and critical news or blog articles about a company (or government) might have severe (or fatal) consequences for the author of the article.\nin other areas, anonymity is required for compliance or legal reasons, e.g. in the selection of job candidates to eliminate discrimination\nfurthermore, without anonymity people and data owners might feel reluctant to participate in surveys or to release their data\noffering anonymity might be a means to convince them to share their data in an anonymized form, which could then be used to perform evaluations and as training data for machine learning models\ntraditional sanitization approaches for free text include removing parts containing personally identifiable information (pii) such as the author\u2019s name, or replacing it with a pseudonym\nhowever, these methods are insufficient to protect the author\u2019s identity\nas the famous netflix de-anonymization attack [31] and other studies [9, 16, 35, 42] have shown, the originator of data can be reidentified from the data itself\nwe illustrate this in the case of the aol search data release [5], where search queries of over 650,000 users were released for research purposes in 2006\nthe search logs were \u201canonymized\u201d by linking the queries to a numerical identifier instead of the actual user name\nafter some investigation in the data, the new york times eventually learned enough information about user 4417749 so they could re-identify her as thelma arnold, a 62-year-old widow from lilburn, a city in georgia\nthe task of attributing authorship of an anonymous or disputed document to its respective author is called authorship attribution\nsuch methods usually make use of stylistic features to identify or discriminate authors, as has been done with the statistic techniques in [30] to resolve the dispute of the federalist papers\nrecently, more sophisticated methods have evolved that use statistical analysis and machine learning to tackle the problem\na survey of modern authorship attribution methods is given by stamatatos [40], and examples include the jgaap [18] and jstylo [26] frameworks\n while these powerful methods are useful in the literary world and in forensics, they can often pose a threat to the privacy and integrity of authors of documents with potentially sensitive content\ncontributions\nmany ir and text mining algorithms rely on the vector space model (vsm) [38] where documents are represented as vectors containing, for instance, their term frequency (tf) or term frequency\u2013inverse document frequency (tf\u2013idf) values\nmore precisely, we make the following contributions\nin section 3, we propose \u201csyntf\u201d, a differentially private method to compute anonymized, synthetic term frequency vectors for textual data that can be used as feature vectors for common ir and text mining tasks such as text classification\nin section 3.4, we give theoretical results on the differential privacy properties of our method\nwe derive improved bounds for the privacy loss of our method and give a heuristic argument that differential privacy on large (discrete) output spaces demands a large privacy loss if the result should fulfill a minimum usefulness requirement\nin section 4, we experimentally verify our method on a corpus of newsgroups postings\na benign, well-intended analyst wants to classify the documents into certain topics, whereas a malicious attacker tries to re-identify the author of these documents using authorship attribution techniques\nthe results show that our method has a much stronger impact on the attacker\u2019s than on the analyst\u2019s task\nbased on our motivation and results, we presume that the synthetic term frequency vectors (syntf vectors) can be used in a multitude of text mining and ir tasks where the semantic similarity of documents is decisive\non the other hand, our method obliterates stylistic features that could otherwise reveal the identity and other privacysensitive information about the writer such as age or gende", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Text mining and information retrieval techniques have been developed to assist us with analyzing, organizing and retrieving documents with the help of computers\nIn many cases, it is desirable that the authors of such documents remain anonymous: Search logs can reveal sensitive details about a user, critical articles or messages about a company or government might have severe or fatal consequences for a critic, and negative feedback in customer surveys might negatively impact business relations if they are identified\nSimply removing personally identifying information from a document is, however, insufficient to protect the writer\u2019s identity: Given some reference texts of suspect authors, so-called authorship attribution methods can reidentfy the author from the text itself\nOne of the most prominent models to represent documents in many common text mining and information retrieval tasks is the vector space model where each document is represented as a vector, typically containing its term frequencies or related quantities\nWe therefore propose an automated text anonymization approach that produces synthetic term frequency vectors for the input documents that can be used in lieu of the original vectors\nWe evaluate our method on an exemplary text classification task and demonstrate that it only has a low impact on its accuracy\nIn contrast, we show that our method strongly affects authorship attribution techniques to the level that they become infeasible with a much stronger decline in accuracy\nOther than previous authorship obfuscation methods, our approach is the first that fulfills differential privacy and hence comes with a provable plausible deniability guarante"},
{"doc": "as sensitive information is increasingly stored on the cloud, privacy concerns have been a critical factor for users to adopt cloud-based information services [53]\nin offering a service, a server can observe the client-initiated query processing flow, and reason about client\u2019s data even it is encrypted\nleaking unencrypted feature values or statistic information about index can lead to privacy attacks [10, 27, 44]\nto address the emerging need of privacy on the cloud, searchable encryption [11\u201314, 18, 27, 31\u201333, 50] has been proposed to identify documents that match a user query from the encrypted index, but has not considered ranking\nrecent research has proposed privacy-aware ranking with linear additive scoring in [2, 9, 51, 56], but the work of [9, 51, 56] can only be applicable for small datasets while the solution in [2] is targeted for modest-sized datasets unless client-server internet connection is fast\nit is an open problem to develop a server-side privacy-aware ranking solution for a large dataset, especially using nonlinear tree ensembles\nensemble trees derived with machine learning techniques such as gradient boosted regression trees (gbrt)[22, 39], lambdamart [8] are popular for feature vector classification and have been proven to be very effective, outperforming a linear additive formula for ranking with web-scale document collections in handling dynamic queries\nfor example, in the yahoo! learning-to-rank challenge [15], all winners have used some forms of tree ensembles\nrecently, random forests by bagging with gbrt and lambdamart [26] have been shown to be competitive\nthe main challenge in developing privacy-aware techniques for server-side nonlinear tree-based ranking is that such computation involves not only query-dependent arithmetic calculations in organizing features, but also numerical comparison to navigate decision trees\nwhile fully homomorphic encryption [23] can support server-side addition and multiplication over encrypted data without decrypting such data, such a scheme is not computationally scalable\nfor example, a fast implementation of fully homomorphic encryption [25] takes more than 320 seconds for adding 1024 encrypted integers\npartially homomorphic encryption can improve time efficiency in a limited degree, and still has other limitations, for example, results computed are not comparable at the server side [41]\norder-preserving encryption techniques [1, 5, 42, 43] let a server compare the encrypted results but do not support arithmetic computation on encrypted numbers\nthe contribution of this paper is an efficient privacy-aware server-side ranking scheme with tree ensembles and to the best of our knowledge, this is the first effort to address such a problem by exploiting a relevance and privacy trade-off\nour key idea is to reduce the dependence of tree ensembles on composite features that require dynamic query-based calculation as a trade-off (e.g. bm25), and rely more on raw features to train tree ensembles (e.g. document term frequency)\nto justify the above approach, we show that decision trees with certain composite features can be transformed into ones with raw features, without increasing regression or entropy-based loss\nbased on such a setting, we propose comparison-preserving mapping that hides document feature values while supporting minimum and maximum feature composition\nwith participation of more raw features, the model training becomes hybrid with a query length specific selection of the base algorithms and feature mixing\nthe rest of the paper is organized as follows\nsection 2 defines the problems and section 3 reviews the related work\nsection 4 gives an overview of our design considerations and approach\nsection 5 analyzes the change of learning accuracy loss when transforming a tree with composite features\nsection 6 presents a mapping to hide document features and tree thresholds, and provides the leakage profile and privacy properties\nsection 7 presents the evaluation results\nsection 8 concludes our paper\n all proofs are listed in appendix ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Tree-based ensembles are widely used for document ranking but supporting such a method efficiently under a privacy-preserving constraint on the cloud is an open research problem\nThe main challenge is that letting the cloud server perform ranking computation may unsafely reveal privacy-sensitive information\nTo address privacy with tree-based server-side ranking, this paper proposes to reduce the learning-to-rank model dependence on composite features as a trade-off, and develops comparison-preserving mapping to hide feature values and tree thresholds\nTo justify the above approach, the presented analysis shows that a decision tree with simplifiable composite features can be transformed into another tree using raw features without increasing the training accuracy loss\nThis paper analyzes the privacy properties of the proposed scheme, and compares the relevance of gradient boosting regression trees, LambdaMART, and random forests using raw features for several test data sets under the privacy consideration, and assesses the competitiveness of a hybrid model based on these algorithm"},
{"doc": "answer selection (as) is an important subtask of question answering (qa) that enables selecting the most suitable answer from a list of candidate answers in regard to the input question\none main challenge of this task lies in the complex and versatile semantic relations that can be observed between questions and answers\n while for factoid qa the task of answer selection may be largely cast as a textual entailment problem, for non-factoid qa what makes an answer better than another often depends on many factors\ndifferent from many other matching tasks, the linguistic similarities between questions and answers may or may not be indicative for the good answers; depending on what the question is looking for, a good answer may come in different forms\nsometimes a correct answer completes the question precisely with the missing information, and in other scenarios, good answers need to elaborate part of the question to rationalize it, and so on\nin other cases, the best answers can also be noisy and include extraneous information irrelevant to the question\nin addition, while a good answer must relate to the question, they might not share common lexical units\nfor example, in the question in table 1, \u201ccompanies\u201d is not directly mentioned in the answer\nthis issue may confuse simple word-matching systems\nthese challenges consequently make the traditional models which are commonly based on lexical features [29, 30, 33] less effective compared to deep learning based methods [8, 28]\nthe neural models often follow the two step procedure\nfirstly, representations of questions and answers are learned via a neural encoder such as long short-term memory (lstm) networks or convolutional neural networks (cnn);\nsecondly, these representations of questions and answers are composed by an interaction function to produce an overall matching score\nin the first step, each word in a question or an answer sequence is first represented with a hidden vector and then all the hidden vectors are aggregated for sequence representations\nthese models have shown very successful results in the as task, however they still suffer from an important issue\nthe answers can be very long and contain lots of words that are not related to the question at hand, especially in non-factoid qa; consequently, the resulting representation might be distracted by non-useful information\nrecent years, attention-based models are proposed to deal with this challenge and have shown great success in many tasks such as machine translation [2, 23], machine reading comprehension [11] and textual entailments [19]\nin the as task, attention-based approaches aim to focus on segments within the candidate answer that are most related to the question [24, 27]\nthe segments with a stronger focus are treated as more important and have more influence on the resulting representations\nfor example, in attention-based lstm models [24] as shown in figure 2, a weight is automatically generated for each word in the answer via an attention model, and the answer is represented as the weighted sum of the hidden vectors\nvarious attention mechanisms have been proposed in previous studies in which additive attention [2] and multiplicative attention [20] are the two most commonly used\nwhile additive attention is associated with a multi-layer perceptron for computing attention weights, multiplicative attention uses inner product for the weight estimations\nthough these attention mechanisms have shown promising results in answer selection, they do not make use of surrounding word context when calculating attention weights, which has been proved to enhance the performance of lstm based qa [5]\nto address this issue, we adopt another attention mechanism, i.e. sequential attention [3] in which an additional lstm is added to compute a context-aware weight for each hidden vector\nthis mechanism helps generate more accurate answer representation regarding to the question\na common characteristic of the previous attention-based approaches is that the question is represented by one feature vector and a round of attention is applied for learning the representation of the answer\n however, in many cases different segments of the answer can be related to different parts of the question\nfor example, in table 1 the segment \u201cthe only requirements when it comes to invoicing have to do with sales tax\u201d is relevant to \u201cprovide invoices\u201d mentioned in the question, while \u201cthere is no obligation under californian law\u201d answers \u201ccalifornia obliged\u201d stated in the question\nconsequently, using one feature vector pair for question answer matching may be not capable of capturing the complex semantic relations between questions and answers\nthis can lead to suboptimal results\nclearly, it is expected that the more aspects an answer covers the better the answer is\na good system should reflect this expectation\nin this paper, we propose multihop attention networks (mans) to deal with this problem\nmans locate, via multiple steps of attention, answer segments that are relevant to different aspects of the question\nas illustrated in figure 1(b), the man first uses the question vector to deduce the answer vector in the first attention layer, then the question vector is refined in the next step to learn the answer vector in the second attention layer.\neach attention step gives a different attention distribution focusing on the segments that are relevant to one aspect of the question\nfinally, we sum up the matching score in each step for scoring the answer\nwe perform experiments on both factoid qa and non-factoid qa datasets.\nexperimental results show that our proposed models obtain highly competitive results and outperform state-of-the-art approaches\nthe main contributions of our paper can be summarized as follows\nwe are the first to investigate the effectiveness of sequential attention mechanism for answers\u2019 attentive representations in answer selection\nwe propose multihop attention networks which represent questions by multiple vectors and use multiple steps of attention for learning the representation of answers\nby doing this, mans can capture different semantic relations between questions and answers\nwe provide extensive experimental evidence of the effectiveness of our model on both factoid question answering and community-based question answering on different domains\nour proposed approach outperforms many other neural architectures on these dataset", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Attention based neural network models have been successfully applied in answer selection, which is an important subtask of question answering (QA)\nThese models often represent a question by a single vector and find its corresponding matches by attending to candidate answers\nHowever, questions and answers might be related to each other in complicated ways which cannot be captured by single-vector representations.\nIn this paper, we propose Multihop Attention Networks (MAN) which aim to uncover these complex relations for ranking question and answer pairs\nUnlike previous models, we do not collapse the question into a single vector, instead we use multiple vectors which focus on different parts of the question for its overall semantic representation and apply multiple steps of attention to learn representations for the candidate answers\nFor each attention step, in addition to common attention mechanisms, we adopt sequential attention which utilizes context information for computing context-aware attention weights.\nVia extensive experiments, we show that MAN outperforms state-ofthe- art approaches on popular benchmark QA datasets\nEmpirical studies confirm the effectiveness of sequential attention over other attention mechanism"},
{"doc": "it has long been thought that combining document-level and passage level evidence is an effective retrieval approach [8, 46]\nbendersky and kurland [4], for example, showed that combining evidence from the best-matching passage in retrieved documents leads to increased retrieval effectiveness\ndifferent types of passages have been examined\ntombros and sanderson [43] proposed so-called query biased summaries for use in search result pages\nlater work provided evidence supporting the use of summaries as a passage representation to improve ad hoc retrieval [14, 22, 40]\nsuch summaries are created based on the degree of query-term matching, rather than document relevance\nit remains to be seen if more effective passages can be found\nwe investigate whether passages can be biased towards selecting text fragments that are more likely to bear answers to the query, and whether this new approach would give a better indication of underlying document relevance\nthe induced representation would tend to cover a richer set of text evidence rather than just the given query terms\nwe call these fragments answer passages\nwe create answer passages by exploiting content in a specialized resource where high quality, human-curated question-answer structures are abundant\ncommunity question answering (cqa) services\nthe text content on such services is utilized in a specific wa\nnot to reuse or synthesize answers, but to provide an indication as to which text fragments in a document are likely to be part of an accepted answer\nthis answer-bearingness property can serve as a valuable document ranking signal\nwhile exploiting information from external resources to improve ranking is common [5, 12], to the best of our knowledge, no past work has studied using an external resource to improve the relevance estimate of document passages for ad hoc retrieval\nour main contributions are:\n(1) we develop a new approach for representing passage-level evidence for ad hoc retrieval via a novel use of cqa data\n(2) the new approach provides a strong indication of document relevance, and is able to outperform many previous passage-based methods\ncombining text quality with evidence derived from the new representation leads to further improvements\nour experiments show that incorporating the new evidence significantly improves over state-of-the-art ranking models, including quality-biased ranking (qsdm), external expansion (ee), and a combination of both\nthe remainder of this paper is organized as follows\nsection 2 presents related work, followed by the motivation of this work in section 3\nsection 4 details our framework of passage extraction using external resources and document re-ranking using quality features derived from the answer passages\nsections 5 and 6 describe the experiment and the results\ndiscussion and concluding remarks are given in sections 7 and ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Evidence derived from passages that closely represent likely answers to a posed query can be useful input to the ranking process\nBased on a novel use of Community Question Answering data, we present an approach for the creation of such passages\nA general framework for extracting answer passages and estimating their quality is proposed, and this evidence is integrated into ranking models\nOur experiments on two web collections show that such quality estimates from answer passages provide a strong indication of document relevance and compare favorably to previous passage-based methods.\nCombining such evidence can significantly improve over a set of state-of-the-art ranking models, including Quality-Biased Ranking, External Expansion, and a combination of both\nA final ranking model that incorporates all quality estimates achieves further improvements on both collection"},
{"doc": "email is one of the most popular online activities and remains the major tool for communication and collaboration.\nin 2017, the total number of business and consumer emails sent and received per day was estimated to be 269 billion, and is expected to continue to grow at an average annual rate of 4.4% over the next four years, reaching 319.6 billion by the end of 2021 [1]\nemail is particularly popular for work related communications with 86% of professionals naming email as their favorite mode of communication [1]\na recent survey shows that reading and answering emails takes up to 28% of enterprise workers\u2019 time, which is more than searching and gathering information (19%), communication and collaboration internally (14%), and second only to role specific tasks (39%) [11]\ndabbish et al. [16] developed a conceptual model of the main purpose email serves in an organizational context\nthey identified four distinct uses of email that have been previously studied in literature: task management, social communication, scheduling and information exchange\nthey conducted a survey with 124 participants to characterize different angles of email usage\nthey report that 36% of emails contained some information or attachment and 18% contained information requests\nthis shows that information exchange is one of the main uses of email.\nemail is often used to ask questions and respond to information requests.\nthe ability to archive such information was shown to be one of the primary reasons users save messages [53, 54]\nprevious research also shows that emails with information requests or responses are less likely to be deleted by the user, and more likely to be left in the inbox or filed [16]\nprevious research studied how people go back to information in their mailboxes\nsome tend to create more structure in their mailboxes (especially with business email), where they organize their emails into categories and folders, while others rely on search to find emails [9]\nwe hypothesize that people are more likely to need to get back to email threads containing questions and answers and that providing adequate support for this category could assist users with retrieving this information or with sharing it with other people\nlet\u2019s consider the example in figure 1\nmany users who have domain knowledge in specific areas tend to receive the same questions over and over (see section 3 for details).\nthe ability to extract question and answer pairs from threaded human-to-human communication could enable scenarios to assist users with composing responses to such messages\nadditional experiences could allow users to directly ask questions and get answers if they think the answer is available in their mailboxes\nthis could also help unlock a lot of the organizational knowledge that would otherwise remain trapped inside people\u2019s mailboxes\nin this paper, we build on previous work by studying information exchange via question and answers in enterprise email\nwe present a detailed analysis of question and answer exchanges in a publicly available collection of almost 1, 000, 000 emails from a defunct information technology company\nwe complement the analysis with a survey of almost 1, 000 information workers to gain more insights into the motivation for the observed email use.\nmotivated by the study of how people leverage email for information exchange, we propose a novel method for extracting question/answer pairs from email threads\nwe also perform a crowd-sourcing user annotation study to annotate pairs of question and candidate answers.\nwe use this dataset to evaluate the proposed approach and show that we can efficiently identify question/answer pairs with reasonable performance outperforming multiple baselines\nfinally, we study how we can leverage external question/answer datasets from community question answering forums to improve the overall performance of our approach\nour contributions can be summarized as follows\n(1) we present detailed analysis of information exchange in enterprise email focusing on question and answer pairs in email thread\n(2) we conduct a survey with close to 1, 000 information workers to gain insights on how they use email for information exchange\nwe study several aspects ranging from types of question and answers, when and how do people get back to this information, how long does the information remain relevant, etc\nwe define the task of extracting question and answer pairs from threaded email conversations\nwe present a novel approach for extracting such pairs and study the effect of leveraging external data to boost the performance.\nthe proposed method outperforms multiple baseline methods\nthe reminder of this paper will proceed as follows\nwe will start by discussing related work and contextually positioning the proposed work relative to the literature in section 2\nwe present the analysis for characterizing information exchange in enterprise email in section 3\nwe describe the datasets and the task definition in section 4\nsection 5 describes the method we propose for extracting question and answer pairs from threaded conversation and section 6 describes our experiments and results\nwe conclude and discuss future work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Email is one of the most important means of online communication\nPeople spend a significant amount of time sending, reading, searching and responding to email to manage tasks, exchange information, etc\nIn this paper, we focus on information exchange over enterprise email in the form of questions and answers.\nWe study a large scale publicly available email dataset to characterize information exchange via questions and answers in enterprise email\nWe augment our analysis with a survey to gain insights on the types of questions exchanged, when and how do people get back to them and whether this behavior is adequately supported by existing email management and search functionality.\nWe leverage this understanding to define the task of extracting question/answer pairs from threaded email conversations.\nWe propose a neural network based approach that matches the question to the answer considering comparisons at different levels of granularity\nWe also show that we can improve the performance by leveraging external data of question and answer pairs\nWe test our approach using a manually labeled email data collected using a crowd-sourcing annotation study\nOur findings have implications for designing email clients and intelligent agents that support question answering and information lookup in emai"},
{"doc": "designing powerful tools that support cooking activities has become an attractive research field in recent years due to the growing interest of users to eat home-made food and share recipes on social platforms [34]\nthese massive amounts of data shared on devoted sites, such as all recipes1, allow gathering food-related data including text recipes, images, videos, and/or user preferences\nconsequently, novel applications are rising, such as ingredient classification [7], recipe recognition [38] or recipe recommendation [34]\nhowever, solving these tasks is challenging since it requires taking into consideration 1) the heterogeneity of data in terms of format (text, image, video, ...) or structure (e.g., list of items for ingredients, short verbal sentence for instructions, or verbose text for users\\ reviews); and 2) the cultural factor behind each recipe since the vocabulary, the quantity measurement, and the flavor perception is culturally intrinsic; preventing the homogeneous semantics of recipes\none recent approach emerging from the deep learning community aims at learning the semantics of objects in a latent space using the distributional hypothesis [14] that constrains object with similar meanings to be represented similarly\nfirst used for learning image representations (also called embeddings), this approach has been derived to text-based applications, and recently some researchers investigate the potential of representing multi-modal evidence sources (e.g., texts and images) in a shared latent space [20, 30]\nthis research direction is particularly interesting for grounding language with common sense information extracted from images, or viceversa\nin the context of computer-aided cooking, we believe that this multi-modal representation learning approach would contribute to solving the heterogeneity challenge, since they would promote a better understanding of each domain-specific word/image/video\nin practice, a typical approach consists in aligning text and image representations in a shared latent space in which they can be compared [6, 8, 20, 21, 24, 33]\none direct application in the cooking context is to perform cross-modal retrieval where the goal is to retrieve images similar to a text recipe query or conversely text recipes similar to a image query.\nhowever, salvador et al. [33] highlight that this solution based on aligning matching pairs can lead to poor retrieval performances in a large scale framework\ntraining the latent space by only matching pairs of the exact same dish is not particularly effective at mapping similar dishes close together, which induces a lack of generalization to newer items (recipes or images)\nto alleviate this problem, [33] proposes to use additional data (namely, categories of meals) to train a classifier with the aim of regularizing the latent space embeddings (figure 1a)\ntheir approach involves adding an extra layer to a deep neural network, specialized in the classification task\nhowever, we believe that such classification scheme is under-effective for two main reasons\nfirst, the classifier adds many parameters that are discarded at the end of the training phase, since classification is not a goal of the system\nsecond, we hypothesize that given its huge number of parameters, the classifier can be trained with high accuracy without changing much of the structure of the underlying latent space, which completely defeats the original purpose of adding classification information\nto solve these issues, we propose a unified learning framework in which we simultaneously leverage retrieval and class-guided features in a shared latent space (figure 1b).\nour contributions are three-fold\nwe formulate a joint objective function with cross-modal retrieval and classification loss to structure the latent space\nour intuition is that directly injecting the class-based evidence sources in the representation learning process is more effective at enforcing a high-level structure to the latent space as shown in figure 1b\nwe propose a double-triplet scheme to express jointly 1) the retrieval loss (e.g., corresponding picture and recipe of a pizza should be closer in the latent space than any other picture - see blue arrows in figure 1b) and 2) the class-based one (e.g., any 2 pizzas should be closer in the latent space than a pizza and another item from any other class, like salad)\nthis double-loss is capable of taking into consideration both the fine-grained and the high-level semantic information underlying recipe items\ncontrary to the proposal of [33], our class-based loss acts directly on the feature space, instead of adding a classification layer to the model\nwe introduce a new scheme to tune the gradient update in the stochastic gradient descent and back-propagation algorithms used for training our model\nmore specifically, we improve over the usual gradient averaging update by performing an adaptive mining of the most significant triplets, which leads to better embeddings\nwe instantiate these contributions through a dual deep neural network.\nwe thoroughly evaluate our model on recipe1m [33], the only english large-scale cooking dataset available, and show its superior performances compared to the state of the art models\nthe remaining of this paper is organized as follows\nin section 2, we present previous work related to computer-aided cooking and cross-modal retrieval\nthen, we present in section 3 our joint retrieval and classification model as well as our adaptive triplet mining scheme to train the model\nwe introduce the experimental protocol in section 4\nin section 5 we experimentally validate our hypotheses and present quantitative and qualitative results highlighting our model effectiveness\nfinally, we conclude and discuss perspective", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Designing powerful tools that support cooking activities has rapidly gained popularity due to the massive amounts of available data, as well as recent advances in machine learning that are capable of analyzing them\nIn this paper, we propose a cross-modal retrieval model aligning visual and textual data (like pictures of dishes and their recipes) in a shared representation space\nWe describe an effective learning scheme, capable of tackling large-scale problems, and validate it on the Recipe1M dataset containing nearly 1 million picture-recipe pairs\nWe show the effectiveness of our approach regarding previous state-of-the-art models and present qualitative results over computational cooking use case"},
{"doc": "machine learning techniques for information retrieval (ir) become widely used in both academic research and commercial search engines [19]\nalthough there have been studies that use unsupervised data or pseudo supervision for learning-to-rank models [2, 8], the best retrieval system is typically constructed based on supervised learning\nmany of the state-of-the-art retrieval systems today make use of deep models [11, 22], which require large amounts of labeled data\ndespite the development of crowdsourcing systems [9, 18], obtaining large-scale and high quality human annotations (e.g. trec-style relevance judgments) is still expensive, if not impossible\ntherefore, implicit feedback such as clicks are still the most attractive data source for the training of ranking systems\ndirectly training a ranking model to optimize click data, however, is infeasible because click data are heavily biased [14, 15, 17, 38]\nin particular, the order of documents in a search engine result page (serp) has a strong influence on where users click [14].\nstudies of position bias show that users tend to examine and click results on the top of a serp while ignoring those on the bottom.\na naive method that treats click/non-click signals as positive/negative feedback will lead to a ranking model that optimizes the order of a search result page but not the relevance of documents\nto leverage the full power of click data for learning to rank, ir researchers have attempted to debias click data before training ranking models\n  one such effort is the development of click models\nthe basic idea of click model is to make hypotheses about user browsing behaviors and estimate true relevance feedback by optimizing the likelihood of the observed user clicks\nsuch methods work well on head queries in web search but not on tail queries or other retrieval applications where multiple observations of same query-document pairs may not be available (e.g. personal search [34])\n also, the construction of click models is separated from the learning of ranking models.\nclick models are usually optimized for the likelihood of observed clicks but not the ranking performance of the overall system.\ntheir parameters need to be re-estimated whenever there are changes in user behaviors\nanother effort to debias click data is result interleaving [4, 25, 28, 30, 31, 37]\nby collecting clicks on swapped results from the same result list, we can obtain unbiased pair preferences for documents and use them to train learning-to-rank models in an online manner.\nthis paradigm, however, introduces non-deterministic ranking functions into the product system [16].\nit may hurt the user experience by putting more irrelevant documents on the top of serps\nbased on these problems, a new research direction emerged recently that focuses on directly training ranking models with biased click data, which is often referred to as unbiased learning to rank [16, 34]\nthe unbiased learning-to-rank framework treats click bias as a counterfactual effect and debiases user feedback by weighting each click with their inverse propensity weights [27]. \n it uses a propensity model to quantify click biases and does not explicitly estimate the query-document relevance with training data\nas theoretically proven by joachims et al. [16], given the correct bias estimation, ranking models trained with click data under this framework will converge to the same model trained with true relevance signals. \ndespite their advantages, existing unbiased learning-to-rank algorithms share a common drawback with click models as they need a separate experiment to estimate click bias\n one of the most popular methods for click bias estimation is result randomization [16, 34], which randomizes the order of documents so that the collected user clicks on randomized serps can reflect the examination bias of users on each result position\nthis paradigm is similar to result interleaving as they both can negatively affect user experience.\nadditionally, because result randomization needs to be conducted on a proportion of search engine traffic separately, existing unbiased learning-to-rank models cannot adapt to changes in user behavior automatically\nin this paper, we introduce a new framework for automatic unbiased learning to rank\nmost limitations of existing unbiased learning-to-rank models are caused by their additional user experiments for propensity estimation\nas wang et al. [34] and joachims et al. [16] observed that unbiased rankers can be directly learned from user clicks with the help of propensity models, we observed that click propensity can be automatically estimated with click data given an unbiased ranking model.\nwe formulate the problem of automatically estimating a propensity model from user clicks as unbiased propensity estimation and propose a dual learning algorithm (dla) for unbiased learning to rank\ndla jointly learns propensity models and ranking models based on raw click data\nsince it doesn\u2019t rely on any result randomization or offline experiments, dla should be preferable in production systems and applicable to online learning to rank\nfurthermore, we theoretically prove that models trained with dla will converge to their global optima under certain circumstances\nto evaluate the effectiveness of dla in practice, we conducted both simulation and real-world experiments. \nempirical experimental results show that models trained with dla are adaptive to changes in user behavior and significantly outperformed the models trained with click model signals and existing unbiased learning-to-rank frameworks\nthe contributions of this paper are summarized as follows\nwe formulate a problem of unbiased propensity estimation and discuss its relationship with unbiased learning to rank\nwe propose a dual learning algorithm that automatically and jointly learns unbiased propensity models and ranking models from raw click data\nwe conduct both theoretical analysis and empirical experiments to understand the effect of the joint learning process used by dla\nthe rest of the paper is organized as follows\nin section 2, we review previous work on learning to rank with click data\nwe introduce existing unbiased learning-to-rank frameworks and the dual learning algorithm in section 3-4\nour experiment setup and results are described in section 5-6.\n finally, we conclude this paper and discuss future work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Learning to rank with biased click data is a well-known challenge\n A variety of methods has been explored to debias click data for learning to rank such as click models, result interleaving and, more recently, the unbiased learning-to-rank framework based on inverse propensity weighting\nDespite their differences, most existing studies separate the estimation of click bias (namely the propensity model) from the learning of ranking algorithms.\nTo estimate click propensities, they either conduct online result randomization, which can negatively affect the user experience, or offline parameter estimation, which has special requirements for click data and is optimized for objectives (e.g. click likelihood) that are not directly related to the ranking performance of the system\nIn this work, we address those problems by unifying the learning of propensity models and ranking models\nWe find that the problem of estimating a propensity model from click data is a dual problem of unbiased learning to rank.\nBased on this observation, we propose a Dual Learning Algorithm (DLA) that jointly learns an unbiased ranker and an unbiased propensity model.\nDLA is an automatic unbiased learning-to-rank framework as it directly learns unbiased ranking models from biased click data without any preprocessing\nIt can adapt to the change of bias distributions and is applicable to online learning\nOur empirical experiments with synthetic and realworld data show that the models trained with DLA significantly outperformed the unbiased learning-to-rank algorithms based on result randomization and the models trained with relevance signals extracted by click models."},
{"doc": "in adversarial retrieval settings (e.g., the web) there is an on-going ranking competition for many queries: authors of some web documents manipulate them so as to have them ranked high.\nthe ranking competition can have various undesirable effects.\nfirst, ranking effectiveness can degrade due to adversarial changes of documents that result in having them ranked higher than they should; i.e., black-hat search engine optimization (seo) [11]\nfurthermore, rankings can potentially (rapidly) change due to small document perturbations that might be indiscernible\nmotivated by the ranking competitions that take place in the adversarial web retrieval setting, and the growing body of work on robust classification \u2014 specifically, with respect to adversarial manipulations [4, 6\u201310, 14, 20, 21, 25], we present the first (to the best of our knowledge) theoretical and empirical study of the robustness of document relevance-ranking functions to document manipulations\n our focus is learning-to-rank-based functions [13] where a document-query pair is represented as a feature vector\nwe start by adapting a basic classifier-robustness notion used in recent work on robust classification [7, 20] to the case of a document ranking function\nwe formally analyze implications of applying this notion and highlight a notable drawback: the treatment of documents independently of each other \u2014 i.e., this is a pointwise robustness perspective\nhowever, ranking depends on the relative retrieval scores of documents.\nhence, we formulate a definition of pairwise robustness that addresses the effect of small document changes on the relative ranking of pairs of documents.\nwe formally analyze the implications of applying our pairwise robustness definition and the connections with pointwise robustness. \n   the different definitions of robustness that we propose are based on a worst-case scenario; namely, quantifying the minimal document change needed to change a ranking.\nusing the definitions to compare the robustness of different ranking functions can be quite difficult\nthus, we explore an additional aspect of robustness which we term stability (cf. [20]): changes of retrieval scores and relative ranking with respect to a given fixed change of a document\nwe then establish formal connections for linear (in features) ranking functions between the extent of their regularization and their stability\nmotivated by these formal findings, we state a variance conjecture: the higher the variance of a learned ranking function, the less robust the rankings it induces\na ranking is considered robust if it does not significantly change due to small documents\u2019 changes\nwe propose a few methods of measuring ranking robustness\nwhile our motivation is to address documents\u2019 changes introduced by incentivized authors, our formal analysis makes no assumptions on the cause and nature of documents\u2019 changes\nthus, the analysis constitutes a general treatment of ranking robustness under document manipulations\nsince, in practice, documents\u2019 changes in competitive retrieval settings are often incentivized (adversarial), we use for evaluation a recently published dataset of document ranking competitions held between students [17]\nthe mere motivation to change documents in the competitions was to have them ranked high as possible. \nthe analysis of the ranking competitions provides support to the formal analysis and the variance conjecture\n first, increased regularization of a linear ranking function (ranksvm [12]), which leads to reduced variance, results in improved ranking robustness\nsecond, increased regularization of a non-linear ranking function, namely, the state-of-the-art lambdamart method [27], also results in improved ranking robustness\nthird, lambdamart induces rankings that are less robust than those induced by ranksvm; the former has higher variance than the latter. \n  our contributions can be summarized as follows\nwe present the first formal and empirical analysis of the robustness of learning-to-rank-based relevance ranking functions to (adversarial) manipulations of documents\nwe formally demonstrate, and provide empirical support to, the connection between regularization of linear learning-to-rank functions and ranking robustness\nmotivated by our formal findings, we post a conjecture about the connection between the variance of a ranking function and ranking robustness, and provide empirical support.", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "For many queries in the Web retrieval setting there is an on-going ranking competition: authors manipulate their documents so as to promote them in rankings\nSuch competitions can have unwarranted effects not only in terms of retrieval effectiveness, but also in terms of ranking robustness\nA case in point, rankings can (rapidly) change due to small indiscernible perturbations of documents\n While there has been a recent growing interest in analyzing the robustness of classifiers to adversarial manipulations, there has not yet been a study of the robustness of relevance-ranking functions\nWe address this challenge by formally analyzing different definitions and aspects of the robustness of learning-to-rank-based ranking functions.\nFor example, we formally show that increased regularization of linear ranking functions increases ranking robustness\nThis finding leads us to conjecture that decreased variance of any ranking function results in increased robustness\nWe propose several measures for quantifying ranking robustness and use them to analyze ranking competitions between documents\u2019 authors.\n The empirical findings support our formal analysis and conjecture for both RankSVM and LambdaMAR"},
{"doc": "interaction behavior data provides implicit but abundant user feedback and can be collected at very low cost [37]\nthus, it has become a popular source for improving the performance of search engines. \nin particular, it has been successfully adopted to improve general web search in result ranking [1, 25], query auto-completion [20, 22], query recommendation [4, 38], optimizing presentations [36], etc\nweb image search results, however, are displayed in a markedly different way from general web search engine results, which leads to different interaction mechanisms and differences in user behavior\ntake the search engine result page (serp) in fig. 1, for example\nan image search engine typically places results in a grid-based interface rather than a sequential result list\nusers can view results not only in a vertical direction but also in a horizontal direction\ninstead of a query-dependent summary of the landing page, the image snapshot is shown together with some metadata about the image; see the example highlighted with a red box in fig. 1\nthis type of metadata is usually only visible when the user hovers their cursor over the result\nalso, while typically available on web image search result pages, where users can view results by scrolling up and down without having to click on the \u201cnext page\u201d button, pagination is usually not (explicitly) supported on general web serps. \n furthermore, image search results are self-contained, in the sense that users do not have to click the results to view the landing page as in general web search, which leads to sparse click data.\nit has been demonstrated that the probability of an examined and relevant result being clicked is very low in web image search [40]\n  due to users\u2019 different and unique interactions with web image search engines when compared with general web search engines, it is hard to apply user behavior models that have been proven useful for general web search to image search without adaption\ntake click models [7], for example\nthey can alleviate user behavior bias and generate a reasonable estimation of result relevance in general web search [5, 10, 13]\nhowever, they tend to follow the sequential examination hypothesis [8].\nalso, the sparsity of clicks in image search generates a challenge to training these click-based models\nin this paper, we conduct both a qualitative and quantitative analysis using data from a lab-based user study and data from a commercial search log to obtain a deeper understanding of user interactions with web image search engines\nthese analyses then inform our modeling of web image searchers\nwe find that users\u2019 unique and rich interactions are highly informative about their preferences\nspecifically, we demonstrate that \u201chovering\u201d over an image is highly correlated with users\u2019 examination behavior and result relevance\nas image search interfaces elicit 8 to 10 times more hover interactions than clicks [28], cursor hovering could be a strong additional signal for relevance.\nalso, we find that examinations of images between two interaction signals (click and hover) usually follow one direction, but with possible skips\nthis observation shows that some of the assumptions used in previously proposed position-based models (e.g., the sequential examination assumption) are reasonable in a web image search scenario, but only when restricted to the interval between two adjacent interaction signals\nmotivated by these and other observations, we build a web image search interaction model to capture user behavior\ncompared with existing interaction models in image search, our model learns to rank results based on user behavior without incorporating text and visual contents\nalso, the training process does not require manually labeled data. \nto summarize, the main contributions of this work are as follows\nby conducting a comprehensive analysis, using both a qualitative and a quantitative approach, using user study data and commercial search log data, we demonstrate the different user interactions with web image search engines. \nwe propose a novel interaction behavior model named grid-based user browsing model (gubm).\nas this model is based on features of user interactions on serps, it can easily be transferred to other scenarios that have a two-dimensional interface such as video search engines. \nwe conduct extensive experiments to test the performance of the proposed gubm model using a commercial search log data. \nthe raw log data with image relevance and quality labels will be made publicly available (after the paper review process)\nthe experimental results are promising in terms of behavior prediction, topical relevance, and image quality\nwe outline observations from our user study and query log analysis in section 2.\nin section 3, we formally introduce gubm\nwe report on experiments using gubm and compare the results with results of existing models in section 4\nsection 5 reviews related work\nfinally, conclusions and future work are discussed in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "User interaction behavior is a valuable source of implicit relevance feedback\nIn Web image search a different type of search result presentation is used than in general Web search, which leads to different interaction mechanisms and user behavior\nFor example, image search results are self-contained, so that users do not need to click the results to view the landing page as in general Web search, which generates sparse click data\nAlso, two-dimensional result placement instead of a linear result list makes browsing behaviors more complex\nThus, it is hard to apply standard user behavior models (e.g., click models) developed for general Web search to Web image search\nIn this paper, we conduct a comprehensive image search user behavior analysis using data from a lab-based user study as well as data from a commercial search log\nWe then propose a novel interaction behavior model, called grid-based user browsing model (GUBM), whose design is motivated by observations from our data analysis\nGUBM can both capture users\u2019 interaction behavior, including cursor hovering, and alleviate position bias\nThe advantages of GUBM are two-fold\n (1) It is based on an unsupervised learning method and does not need manually annotated data for training\n(2) It is based on user interaction features on search engine result pages (SERPs) and is easily transferable to other scenarios that have a grid-based interface such as video search engines\nWe conduct extensive experiments to test the performance of our model using a large-scale commercial image search log\nExperimental results show that in terms of behavior prediction (perplexity), and topical relevance and image quality (normalized discounted cumulative gain (NDCG)), GUBM outperforms state-of-the-art baseline models as well as the original ranking\nWe make the implementation of GUBM and related datasets publicly available for future studie"},
{"doc": "learning from user behaviors is a general approach used in online interactive information systems\nclicking on an item has been commonly used as positive implicit feedback of user preference to design systems and evaluate their performance\nfor example, most recommender systems use click as implicit preference feedback and take click-through rate (ctr) as the optimization target and the online evaluation metric\nalthough user click provides implicit information about item level user preference, it may not represent true preference\non the one hand, besides preference, click behavior may be affected by many other factors, like position [18], trust [34] and presentation [32].\nto address these issues, researchers have proposed a number of click models to describe user click behavior in web search scenario. [5, 10]\non the other hand, click signal does not capture post-click user experience\nuser may have clicked an item because of the titlebait, but dislike it after reading its content.\nfor example, a user clicks an item because attracted by its title,\"you won\u2019t believe what this guys does after his set...\", but turn out to be disappointed with its poor content and result in a negative preference for it.\nin such cases, using click as positive preference indicator will mislead system in modeling user interest.\nsome previous works have realized this issue and have incorporated some other user behaviors to reduce the gap between click and user experience\nfor example, dwell time (i.e., the time that user spends on a clicked item) has been found well correlated with item level user satisfaction in information retrieval (ir)\nclick followed by a long dwell time has traditionally been seen as satisfied click and been successfully used in a number of retrieval applications [11]\nbesides dwell time, recent work also tries to learn from other behaviors, like mouse movement [27], scroll information [23], and gaze [1, 25]\nhowever, as for the subjective user experience, it still lacks a thorough understanding of how and why click is not aligned with user preference\nin the above example, the user may be interested and expect to like the item when he sees its title, but after reading the news, his/her preference for the item has changed because of the low content quality\n thus, it is necessary to model user preference in different phases\nin this paper, we conduct an in-depth user study in online news reading scenario in the mobile environment, in which we collect user behavior logs as well as user explicit preference feedback for the news (section 3).\nthrough comparing the click signal and the item-level user preference, it is observed that click signal are not always aligned with user preference\nmore than half of the clicked news is disliked by user (section 4)\nin addition, we propose user preferences in three phases\nthe first one is before-read preference collected right after users click but before reading the content, the second one is after-read preference collected right after users finishing reading the content, the third one is post-task preference which is context-independently collected after users finishing the task\nthe statistic analysis of the differences between these multiphase preferences shows that the changes of preference are highly related to the quality of news and the context of user interactions (section 5)\nfurthermore, we investigate how user behaviors, like viewport time, dwell time and scroll patterns correlate with preference (section 6)\nwe find that different behaviors represent preference in different phases\n    for example, the viewport time is correlated with user before-read preference, while dwell time is more correlated with user after-read preference\nbased on these observations, by incorporating various kind of user behaviors, news quality, and interaction context information, we build an effective model to predict user\u2019s actual preference for the clicked item\nfinally, we replace binary click signals to predicted preferences in common click-based online metrics, like ctr, and obtain a significant improvement for estimation of user list-level satisfaction (section 7)\nto sum up, we have made the following contributions\nto the best of our knowledge, this is the first work which conducts an in-depth user study about the gap between user click and preference in online news reading and recommendation scenarios\nwe demonstrate that click is not always aligned with user actual preference, and the gap is related to the change of preferences in different phases. \nthe analysis on different reading phases show that the news quality and the user interaction context are related to the change of user preferences\nfurthermore, different user behaviors, like viewport time, dwell time and read length, are found reflecting preference in different phases\nbased on these findings, a novel preference prediction model is proposed to predict user actual preferences of the clicked items, in which various user behaviors, news quality, and interaction context are taken into account\nfurthermore, a significant improvement of measuring user list-level satisfaction is achieved by incorporating the predicted preference in traditional click-based online metrics, such as ct", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Click signal has been widely used for designing and evaluating interactive information systems, which is taken as the indicator of user preference\n However, click signal does not capture post-click user experience\nVery commonly, the user first clicked an item and then found it is not what he wanted after reading its content, which shows there is a gap between user click and user actual preference\nPrevious studies on web search have incorporated other user behaviors, such as dwell time, to reduce the gap\nUnfortunately, for other scenarios such as recommendation and online news reading, there still lacks a thorough understanding of the relationship between click and user preference, and the corresponding reasons which are the focus of this work\nBased on an in-depth laboratory user study of online news reading scenario in the mobile environment, we show that click signal does not align with user preference.\nBesides, we find that user preference changes frequently, hence preferences in three phases are proposed: \nBefore-Read Preference, After-Read Preference and Post-task Preference\nIn addition, the statistic analysis shows that the changes are highly related to news quality and the context of user interactions.\nMeanwhile, many other user behaviors, like viewport time, dwell time, and read speed, are found reflecting user preference in different phases\nFurthermore, with the help of various kinds of user behaviors, news quality, and interaction context, we build an effective model to predict whether the user actually likes the clicked news.\nFinally, we replace binary click signals of traditional click-based evaluation metrics, like Click-Through Rate, with the predicted item-level preference, and significant improvements are achieved in estimating the user\u2019s list-level satisfaction. \nOur work sheds light on the understanding of user click behaviors and provides a method for better estimating user interest and satisfaction\n The proposed model could also be helpful to various recommendation tasks in mobile scenario"},
{"doc": "search engines play an important role in our everyday lives\none way to improve them is by getting a better understanding of user search behavior, such as clicks, dwell times, mouse movements, etc\nso far, models of user behavior have focused on modeling and predicting single events, e.g., clicks [9] and mouse movements [22], and properties of these events, e.g., time between clicks [4]\nin this paper for the first time we focus on modeling and predicting sequences of information interaction events and, in particular, sequences of clicks\nalthough people tend to make only one (or sometimes no) click on a search engine result page (serp), multi-click query sessions constitute a significant part of search traffic\nfor example, about 23% of the query sessions in the yandex relevance prediction challenge dataset contain multiple clicks (see 4.1 for details)\nit is commonly assumed that users traverse search results from top to bottom, which leads to the assumption that clicks are ordered by the position of search results\nhowever, it was shown that in practice this assumption does not always hold, and that up to 27.9%\u201330.4% of multi-click sequences, depending on the dataset, are not ordered by position [43]\nwe aim to create tools that help us understand, model and predict sequences of clicks on search engine results, which is important because it provides an opportunity for improving the user search experience\nfor example, knowing that a user is likely to click on many results or that there are high chances that the user will interact with the results in an order other than the one in which the results are presented on a serp can be used by a search engine to proactively show an advice or make a change in the ranking\nwe propose a click sequence model (csm) that predicts a probability distribution over click sequences\nat the core of our model is a neural network with encoder-decoder architecture\nwe implement the encoder as a bidirectional recurrent neural network (bi-rnn) that goes over the search engine result page from top to bottom and from bottom to top and outputs contextual embeddings of the results\nwe implement the decoder as a recurrent neural network (rnn) with an attention mechanism\nthe decoder is initialized with the final states of the forward and backwardrnns of the encoder\nit is used to predict the sequence of positions of the clicked results\nthe whole network is trained by maximizing the likelihood of the observed click sequences\nwe evaluate our proposedcsmusing a publicly available click log and showthat csm provides good means to generate a short list ofk click sequences that contains the observed click sequence with a high probability\n we present an analysis of the performance of csm for query sessions with different numbers of clicks and query sessions in whichclicks are ordered/not orderedbyposition\nwe measure the performance of csm on two new tasks: predicting the number of clicks and predicting ordered/unordered sequences of clicks\nadditionally, we show that csm achieves state-of-the-art results on the standard click prediction task, which allows us to compare csm to traditional click models that model and predict single events, namely clicks\noverall, we make the following contributions\nwe formulate a novel problem of predicting click sequences\nto solve this problem, we propose a click sequence model (csm) based on neural networks\nwe evaluate csm on a range of prediction tasks, namely predicting click sequences, predicting the number of clicks, predicting ordered/unordered sequences of clicks and, finally, predicting clicks themselves\nas to the potential impact of the proposed csm model, we believe it can be used to predict that (i) a user will click on more than one result, which may indicate that a user has a complex information need [30]; or that (ii) a user will interact with the results not in the order in which these results are presented on the serp, which may indicate that a user is struggling and there are problems in the ranking of the results [36]\ncsm can help us identify queries for which there is a room for improvement (in terms of user experience) and it can serve as a quick analysis tool to interpret how a particular change in the ranking of the results will influence user click behavior\nthe rest of the paper is structured as follows\nin section 2 we provide a precise statement of the click sequence modeling and prediction problems thatwe are tackling\nsection 3 introduces our neural network based model for predicting click sequences\nin section 4 we describe the setup of our experiments and section 5 presents the results of those experiments\nwe describe related work in section 6 and conclude in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Getting a better understanding of user behavior is important for advancing information retrieval systems\nExisting work focuses on modeling and predicting single interaction events, such as clicks\nIn this paper, we for the first time focus on modeling and predicting sequences of interaction events\nAnd in particular, sequences of clicks\nWe formulate the problem of click sequence prediction and propose a click sequence model (CSM) that aims to predict the order in which a user will interact with search engine results\nCSM is based on a neural network that follows the encoder-decoder architecture\nThe encoder computes contextual embeddings of the results\nThe decoder predicts the sequence of positions of the clicked results\nIt uses an attention mechanism to extract necessary information about the results at each timestep\nWe optimize the parameters of CSM by maximizing the likelihood of observed click sequences\nWe test the effectiveness ofCSMon three newtasks: (i) predicting click sequences, (ii) predicting the number of clicks, and (iii) predicting whether or not a user will interact with the results in the order these results are presented on a search engine result page (SERP)\nAlso, we show that CSM achieves state-of-the-art results on a standard click prediction task, where the goal is to predict an unordered set of results a user will click o"},
{"doc": "systematic review (sr) in evidence-based medicine is a literature survey which provides a conclusion or answer to a specific clinical question\nan example clinical question is\ncan laparoscopy (a diagnostic test) assess the resectability of pancreatic cancer?\nstudies reported in research papers relevant to the question are identified and combined to draw the overall conclusion\nsystematic reviews also show evidence of the derived conclusion and appraisal results of the relevant literature.\nfor the better understanding of the task, we describe the steps and challenges in conducting srs in evidence based medicine\noverview of systematic review\na sr is a literature survey which presents the up-to-date conclusion or answer to a clinical question based on relevant studies\nsince the conclusions in srs are considered as gold standard, srs are strictly conducted by following systematic steps\nthe first step is to formulate a clinical question (or sr topic) that needs to be answered\nto determine a clinical question, sr experts carefully check clinical literature and existing srs.\nat the end of this step, the specific clinical question is defined and sr experts become knowing one or two relevant documents for the defined clinical question\nrelevance conditions, namely eligibility criteria in sr community, are also defined in this step\nrelevance conditions are initially defined by using multiple key clinical terms often following the pico model\nthe conditions are then further elaborated from the terms and finally contain details to assess diverse aspects of documents\nonce a clinical question and relevance conditions are set, the next step is to search potential relevant documents from databases e.g., pubmed, using keyword queries\nthe aim of the retrieval step is to collect all candidate documents which are possibly relevant to sr. \nto achieve high recall, various combinations of keyword queries are formulated for retrieval\nthe list of keyword queries are often published together with sr to ensure reproducibility\nthe collection of candidate documents is usually large, with low precision and high recall\nrecently, there are studies aiming to increase precision while maintaining recall using pico [20]. \nthe process of identifying relevant documents from candidate documents is called screening in sr\n in this process, a document is evaluated regarding many aspects defined in the relevance conditions, such as patient information, and experimental design\neach aspect in the relevance conditions contains elaborated details including medical history, symptoms, and ages of patients, to name a few\naccording to the explicit relevance conditions, if a document satisfies all of the conditions, it is a relevant document\ndocuments with partial or non satisfaction are labeled as irrelevant documents\n the screening process is performed in two steps: abstract screening and full text screening\nabstract screening is to judge relevance based on title and abstract of a candidate document\nfull text screening is then to confirm relevance based on the entire text of the document\ntable 1 lists the percentage of relevant and candidate documents for five sample srs taken from the clef 17 ehealth task 2 dataset\nthe minimum, maximum, and median percentage of 50 srs in the dataset are also reported\ntypically, fewer than 1% of candidate documents are relevant\nthe last step in sr is to assess qualities of relevant documents\nsince it is conducted on only relevant documents, this step is relatively less resource-intensive\ntarget information such as patient information, experiment methodologies, and results, are collected and synthesized by sr experts to reach an overall conclusion of the clinical question\nimproving the process of collecting data in this step is also an active research area [9, 17, 24]\nchallenges in screening\nthe screening process is one of the most expensive steps in conducting. sr\n multiple sr experts need to manually and thoroughly examine every candidate document to find all relevant documents\nas reported in table 1, fewer than one percent of candidate documents are determined as relevant in most srs in the clef17 dataset\nin another study [10, 21], it is reported that the number of relevant documents accounts for only 1.2 percent of the candidate documents on average in 93 srs\nthough many approaches have been proposed to improve the costly screening step, manual screening remains necessary to find all relevant documents in candidate documents, concluded in a recent survey on improving screening process using text mining techniques [19]\nbesides, the survey suggests that ranking candidate documents, or screening prioritization, is safe for use in practice to assist manual screening\nscreening prioritization aims to rank candidate documents so that it enables sr experts to screen the relevant documents as early as possible\nwith the same goal, a new task named \u201ctechnologically assisted reviews in empirical medicine\u201d was introduced in clef2017 ehealth competition\n in this study, we use the dataset released by this task\noverview of seed-driven document ranking\nto define a clinical question and relevance conditions, sr experts carefully go through literature and existing srs\nit is natural that sr experts know the existence of one or two relevant documents after defining the clinical question, i.e., before the screening process\nin our proposed solution, we assume that one relevant document is known, which we call the seed document\nnow, the research question becomes\n\u201cgiven a relevant document as a seed, how to rank the candidate documents such that the (remaining) relevant documents appear at the top of the ranking?\nat first glance, this is a problem of query by example document\na common approach is to identify key phrases from the example document as queries to rank documents\nhowever, the relevance conditions defined in sr cover diverse aspects of the documents, and their details e.g., gender/age of subjects in clinical trial and randomized experiment design\nto this end, we conduct a thorough analysis of relevant documents and irrelevant documents in candidate documents\nbased on the analysis we propose to represent the seed document and candidate documents using \u2018bag of clinical terms\u2019\nthe clinical terms are identified by using external thesaurus i.e., umls medical thesaurus\nnote that, this is different from common approaches in query by example where the key phrases are identified based on statistical measures\nmore importantly, we propose a method to estimate the weight of a term by leveraging high similarities between relevant documents in srs\nrelevant documents have higher intra-similarities than irrelevant documents, because all the relevant documents must meet all the relevance conditions\ntherefore, we estimate the importance of a term based on its distribution in the candidate documents and their similarities to the seed document\nin this regard, the proposed weighting scheme finds relevant documents without spending considerable effort identifying explicit relevance conditions\n in sdr, we adopt and modify the query likelihood retrieval model to estimate the relevance\nrecently, there are many studies on using distributed representation of words, or word embeddings, for different tasks in information retrieval\nin our setting, a document can be easily represented based on the embeddings of its contained words\nusing cosine similarity, one can get a ranking towards the seed document\n in our experiments, we evaluate different document representations including bag of clinical terms, bag of words, and word embeddings and different retrieval models including bm25 and query likelihood model\nour experimental results show that sdr outperforms all baseline methods and beat the best performance (without using relevance feedback) reported in clef 2017\nmore interestingly, we observe that relevance estimated based on word embeddings well complements sdr\nthe best ranking is achieved by combining the relevances estimated by sdr and by word embeddings.\nwe also simulate the manual screening process and it demonstrates the efficacy of the proposed approach when multiple relevant documents are known.\nour contributions\nin summary, we make the following contributions in this work\nwe propose a new approach sdr for screening prioritization, to assist conducting systematic reviews, with an assumption that sr experts know one relevant document\nwe conducted a detailed analysis of relevant documents in srs and propose to use \u2018bag of clinical terms\u2019 to represent documents\nabove all, we estimate the importance of the clinical terms based on their distribution in candidate documents and the similarities to the seed documents\nthrough extensive experiments, we demonstrate the effectiveness of sdr against baseline methods\n we provide a comprehensive analysis of the ranking results and simulate the manual screening process where more relevant documents are identified through the screening proces", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Systematic review (SR) in evidence-based medicine is a literature review which provides a conclusion to a specific clinical question\nTo assure credible and reproducible conclusions, SRs are conducted by well-defined steps. One of the key steps, the screening step, is to identify relevant documents from a pool of candidate documents\nTypically about 2000 candidate documents will be retrieved from databases using keyword queries for a SR\nFrom which, about 20 relevant documents are manually identified by SR experts, based on detailed relevance conditions or eligibility criteria\n Recent studies show that document ranking, or screening prioritization, is a promising way to improve the manual screening process\n   In this paper, we propose a seed-driven document ranking (SDR) model for effective screening, with the assumption that one relevant document is known, i.e., the seed document\nBased on a detailed analysis of characteristics of relevant documents, SDR represents documents using \u2018bag of clinical terms\u2019, rather than the commonly used bag of words\nMore importantly, we propose a method to estimate the importance of the clinical terms based on their distribution in candidate documents.\n On benchmark dataset released by CLEF\u201917 eHealth Task 2, we show that the proposed SDR outperforms stateof-the-art solutions\n Interestingly, we also observe that ranking based on word embedding representation of documents well complements SDR.\n The best ranking is achieved by combining the relevances estimated by SDR and by word embedding\nAdditionally, we report results of simulating the manual screening process with SD"},
{"doc": "the goal of due diligence in mergers and acquisitions (m a) law [14, 30, 31] is to identify all passages in a set of legal documents, often contracts, that would pose major liability or risk should the transaction occur\nwhile this process, like other legal retrieval tasks [8, 35, 38], has historically been a manual one, recent due diligence blunders [32], in particular hp\u2019s estimated $8b loss after acquiring autonomy [5], have prompted an increased desire for solutions using automated techniques, and correspondingly, a large number of software providers delivering such solutions [1]. \nin due diligence tasks, lawyers seek to find a set of standard passage-types called provisions, that typically correspond to an increased risk in a transaction\nthese range from what happens to a contract when one party is acquired (\"change of control\"), to what parties are involved in the contracts,1 to what happens when a contract is terminated early\none might naively think, as we did, that this ought to be an easily solved problem as lawyers will generally write contracts in a similar manner to one another.\nhowever, there is a surprising amount of variation in contracts as they typically evolve over the negotiation process and incorporate feedback from the involved parties [20]\nthis complexity is compounded by differences in how jurisdictions phrase and handle provisions as well as more technical problems, such as errors introduced during digitization\naccordingly, we are interested in finding the best approach to identify these passages across a wide variety of legal documents and jurisdictions. \nto determine the best approach, a dataset is needed\nto the best of our knowledge, no such publicly available collection exists\n a fact that we struggled with when developing our own proprietary due diligence platform\nto help correct this situation and foster additional experimentation, this paper describes a subset of our own internal dataset that we are releasing with relevant annotations for academic use\nthis subset spans 50 topics and includes 4,412 manually annotated legal documents totalling over 15 million sentences, collected from various public sources (section 3). \nusing this dataset, we test several possible approaches to identifying desired provisions for extraction\ninspired natural language processing (\"nlp\") community [11, 15], we treat documents as sequences of labelled and unlabelled sentences and investigate how different sequence-based (conditional random fields and hidden markov models) methods compare to more traditional methods (e.g., logistic regression)\nusing sentence-level and annotation-level (see section 4.3) effectiveness measures, we find that crfs significantly and substantially outperform the other tested methods\nwe present an additional examination of the possible degenerate cases that can arise when treating documents as sequences of sentences.\nwe find that crfs have the lowest incidence rate among the methods tested for all degenerate cases\nfinally, we conclude with a discussion of the limitations of this work and potential avenues of further investigatio", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We present and formalize the due diligence problem, where lawyers extract data from legal documents to assess risk in a potential merger or acquisition, as an information retrieval task\n Furthermore, we describe the creation and annotation of a document collection for the due diligence problem that will foster research in this area. \nThis dataset comprises 50 topics over 4,412 documents and ~15 million sentences and is a subset of our own internal training data\nUsing this dataset, we present what we have found to be the state of the art for information extraction in the due diligence problem. \nIn particular, we find that when treating documents as sequences of labelled and unlabelled sentences, Conditional Random Fields significantly and substantially outperform other techniques for sequence-based (Hidden Markov Models) and non-sequence based machine learning (logistic regression)\nIncluded in this is an analysis of what we perceive to be the major failure cases when extraction is performed based upon sentence label"},
{"doc": "a systematic review is a type of literature review that appraises and synthesises the work of primary research studies (called citations below) to answer one or more research questions\nsystematic reviews play a key role in evidence based medicine, informing practice and policy\n for example, in order for medical specialists to diagnose and recommend treatment for a patient accurately, they rely on the most up-to-date clinical evidence\nsimilarly systematic reviews are used by government agencies to decide upon health management and policies [15]\nsystematic reviews provide this evidence through a comprehensive literature review\nhowever, the compilation of systematic reviews can take significant time and resources, hampering their effectiveness\nwith an increasing number of medical studies submitted to databases such as pubmed, it is becoming ever more difficult and laborious for systematic review authors to retrieve relevant studies for inclusion in the review, while minimising the amount of non-relevant citations they need to assess or appraise\ntsafnat et al. report that it can take several years to complete and publish a systematic review [26]\nwhen systematic reviews take such significant time to complete, they can be out-of-date even at the time of publishing\nretrieval of literature from a medical database for systematic reviews is performed using complex boolean queries\nboolean queries (and retrieval) are the accepted standard for searching citations when compiling a systematic review [8]\nan example of one such query is visible in figure 1\nthese queries contain several boolean operators such as or, and, and not, as well as advanced operators such as adj (which matches search terms within a certain range of each other), field restrictions, nesting, and the explosion of terms in an ontology (mesh).\n the query in figure 1 is composed of 19 clauses\neach is represented as a line in the query (to which a number is assigned) and may contain one or more operators. \nquery formulation is an important step in the definition of the search strategy of a systematic review\na poorly formulated query may retrieve only a subset of the relevant citations to the review study or, conversely, may retrieve an extremely large number of citations, while there may only be few relevant citations\nin particular, the retrieval of a large number of citations is often a problem for the compilation of systematic reviews because all of the retrieved citations need to be screened for inclusion in the systematic review (akin to performing relevance assessment).\nthis appraisal phase, commonly performed by two reviewers, is expensive and time consuming, often requiring several person-months to complete [10], thus adding to the costs (both monetary and in terms of time) required for the compilation of a systematic review\n previous work has in fact reported that it can take experienced reviewers between 30 seconds and several minutes [27] to screen a single study (title, abstract and metadata).\nthe effect this has on the timeliness of reviews is highlighted by some notable examples\nfor example in shemilt et al.\u2019s scoping review, 1.8 million studies were screened, of which only about 4,000 were found to be potentially eligible [24]\nto ensure the queries used by systematic reviews are of high quality,reviews often receive the support of information specialists to assist in the query formulation process, and queries are often submitted to an expert panel for review (along with the protocol of the systematic review)\nhowever, when formulating queries, little is known about their retrieval performance when applied to answer the questions posed in systematic reviews [1]\npast research has found that only 30% of citations are retrieved using the boolean queries defined in the protocol of the systematic review [6] (51% of citations were discovered by pursuing references of references, and 24% by personal knowledge or contacts) \u2013 a recall problem\non the other hand, past research has also shown that queries may retrieve an overly large set of citations compared to those that were relevant, as it was for shemilt et al.\u2019s study where only 0.22% of the retrieved citations were relevant [24] \u2013 a precision problem\nin this paper, we question whether the boolean queries used in systematic reviews are the most effective possible (highest recall/- precision), or whether more effective queries are possible and how these can be obtained\nspecifically, we seek to answer the following research questions\nrq1: is it possible to formulate boolean queries that are more effective than those originally used within search strategies of systematic reviews\nwe investigate this with respect to recall, precision, f\u03b2 (f0.5, f1, f3), and work saved over sampling (w ss) as target effectiveness measures\nrq2: if the answer to rq1 is positive, then: can alternative, more effective boolean queries, generated from the original systematic review queries, be automatically selected? \nto answer rq1, we devise a set of transformations that can be applied to clauses of boolean queries for generating alternative valid queries to be issued for retrieval\ntransformations are applied at the level of clauses and consist in changing the operators used in the original queries\nin particular, no new terms are added to the original queries, nor original terms are removed\nthrough the empirical evaluation of alternative queries generated using the transformations, we show that better boolean queries are possible, thus answering rq1 positively\nto answer rq2 we cast the problem of formulating boolean queries that are more effective than the original, into two machine learning problems\n(1) predicting whether a boolean query, generated from the original query by applying a chain of query transformations, is more effective than the original query, and (2) ranking the boolean queries generated from the original query, so that the queries that are better than the original are ranked at the top of the suggested alternative queries\n  the two problems are tackled by training classification and learning to rank algorithms, respectively\nwe empirically show that effective classifiers and rankers can be built, and these can be tailored to optimise different evaluation measures\nspecifically, we find that the classifiers identify queries that on average outperform the original queries by 147.72% in precision, 185.13% in f0.5, 99.29% in f1, and 40.45% in f3; while the rankers identify queries that on average outperform the original queries by 358.47% in precision, 247.79% in f0.5, 149.91% in f1, and 42.90% in f3.\nhowever, neither the classification approach nor the learning to rank approach were able to outperform the original queries (baseline) for recall and work saved over sampling (wss", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Systematic reviews form the cornerstone of evidence based medicine, aiming to answer complex medical questions based on all evidence currently available.\nKey to the effectiveness of a systematic review is an (often large) Boolean query used to search large publication repositories\nThese Boolean queries are carefully crafted by researchers and information specialists, and often reviewed by a panel of experts.\nHowever, little is known about the effectiveness of the Boolean queries at the time of formulation\nIn this paper we investigate whether a better Boolean query than that defined in the protocol of a systematic review, can be created, and we develop methods for the transformation of a given Boolean query into a more effective one\nOur approach involves defining possible transformations of Boolean queries and their clauses.\nIt also involves casting the problem of identifying a transformed query that is better than the original into:\n(i) a classification problem; and (ii) a learning to rank problem\nEmpirical experiments are conducted on a real set of systematic reviews\nAnalysis of results shows that query transformations that are better than the original queries do exist, and that our approaches are able to select more effective queries from the set of possible transformed queries so as to maximise different target effectiveness measure"},
{"doc": "multi-armed bandit algorithms provide a principled solution to the explore exploit dilemma [2, 3, 14], which exists in many important real-world applications such as display advertisement [21], recommender systems [18], and online learning to rank [27]\nintuitively, bandit algorithms adaptively designate a small amount of traffic to collect user feedback in each round while improving their model estimation quality on the fly.\nin recent years, contextual bandit algorithms [9, 17, 18] have gained increasing attention due to their capability of leveraging contextual information to deliver better personalized online services\nthey assume the expected reward of each action is determined by a conjecture of unknown bandit parameters and given context, which give them advantages when the space of recommendation is large but the rewards are interrelated\nmost existing stochastic contextual bandit algorithms assume a fixed yet unknown reward mapping function [9, 11, 18, 20, 25]\nin practice, this translates to the assumption that users\u2019 preferences remain static over time\nhowever, this assumption rarely holds in reality as users\u2019 preferences can be influenced by various internal or external factors [7].\nfor example, when a sports season ends after a championship, seasonal fans might jump over to following a different sport and not have much interest in the off-season\nmore importantly, such changes are often not observable to the learners\nif a learning algorithm fails to model or recognize the possible changes of the environment, it would constantly make suboptimal choices, e.g., keep making out-of-date recommendations to users.\nin this work, moving beyond a restrictive stationary environment assumption, we study a more sophisticate but realistic environment setting where the reward mapping function becomes stochastic over time\nmore specifically, we focus on the setting where there are abrupt changes in terms of user preferences (e.g., user interest in a recommender system) and those changes are not observable to the learner beforehand\nbetween consecutive change points, the reward distribution remains stationary yet unknown, i.e., piecewise stationary.\nunder such a non-stationary environment assumption, we propose a two-level hierarchical bandit algorithm, which automatically detects and adapts to changes in the environment by maintaining a suite of contextual bandit models during identified stationary periods based on its interactions with the environment\nat the lower level of our hierarchical bandit algorithm, a set of contextual bandit models, referred to as slave bandits, are maintained to estimate the reward distribution in the current environment (i.e., a particular user) since the last detected change point\nat the upper level, a master bandit model monitors the \u2018badness\u2019 of each slave bandit by examining whether its reward prediction error exceeds its confidence bound\nif the environment has not changed, i.e., being stationary since the last change, the probability of observing a large residual from a bandit model learned from that environment is bounded [1, 9].\nthus the \u2018badness\u2019 of slave bandit models reflects possible changes of the environment.\nonce a change is detected with high confidence, the master bandit discards the out-of-date slave bandits and creates new ones to fit the new environment\n consequentially, the active slave bandit models form an admissible arm set for the master bandit to choose from\n at each time, the master bandit algorithm chooses one of the active slave bandits to interact with the user, based on its estimated \u2018badness\u2019, and distributes user feedback to all active slave bandit models attached with this user for model updating\nthe master bandit model maintains its estimation confidence of the \u2018badness\u2019 of those slavebandits so as to recognize the out-of-date ones as early as possible\nwe rigorously prove the upper regret bound of our non-stationary contextual bandit algorithm is o(\\\\tau _{t}\\\\sqrt{s_{max}}logs_{max}), in which \\\\tau _{t} is the total number of ground-truth environment changes up to time t and s_{max} is the longest stationary period till time t \nthis arguably is the lowest upper regret bound any bandit algorithm can achieve in such a non-stationary environment without further assumptions\nspecifically, the best one can do in such an environment is to discard the old model and estimate a new one at each true change point, as no assumption about the change should be made\nthis leads to the same upper regret bound achieved in our algorithm.\nhowever, as the change points are unknown to the algorithm ahead of time, any early or late detection of the changes can only result in an increased regret\nmore importantly, we prove that if an algorithm fails to model the changes a linear regret is inevitable\nextensive empirical evaluations on both a synthetic dataset and three real-world datasets for content recommendation confirmed the improved utility of the proposed algorithm, compared with both state-of-the-art stationary and non-stationary bandit algorithm", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": " Multi-armed bandit algorithms have become a reference solution for handling the explore/exploit dilemma in recommender systems, and many other important real-world problems, such as display advertisement.\n However, such algorithms usually assume a stationary reward distribution, which hardly holds in practice as users\u2019 preferences are dynamic.\n This inevitably costs a recommender system consistent suboptimal performance.\n In this paper, we consider the situation where the underlying distribution of reward remains unchanged over (possibly short) epochs and shifts at unknown time instants.\n In accordance, we propose a contextual bandit algorithm that detects possible changes of environment based on its reward estimation confidence and updates its arm selection strategy respectively.\n Rigorous upper regret bound analysis of the proposed algorithm demonstrates its learning effectiveness in such a non-trivial environment.\n Extensive empirical evaluations on both synthetic and real-world datasets for recommendation confirm its practical utility in a changing environment"},
{"doc": "according to the goldman sachs, the 2016 online retail market of china for fashion products, including apparel, footwear, and accessories, has reached 187.5 billion us dollars, which demonstrates people\u2019s great demand for clothing\nin fact, clothing plays a pivotal role in people\u2019s daily life, as a proper outfit (e.g., a top with a bottom) can empower one\u2019s favorable impression\nin a sense, how to make suitable outfits has become the daily headache of many people, especially those who do not have a good sense of clothing matching\nfortunately, recent years have witnessed the proliferation of many online fashion communities, such as polyvore and chictopia, where a great number of outfits composed by fashion experts have been made publicly available, as shown in figure 1\nbased on such rich real-world data, several researchers have attempted to intelligently aid people in clothing matching\nin fact, most existing researches mainly rely on the deep neural networks to extract the effective representations for fashion items to tackle the clothing matching problem, due to their impressive advances in various research domains, including the image classification, speech recognition and machine translation\nhowever, as pure data-driven methods, neural networks not only suffer from the poor interpretability but also overlook the value of human knowledge\nespecially, as an essential aspect of people\u2019s daily life, clothing matching domain has accumulated various valuable knowledge, i.e., the matching rules\nalthough they may be of high subjectivity, certain matching rules have been widely accepted by the public as common sense\nfor example, tank tops would go better with shorts instead of the dress, while silk tops better avoid the knit bottoms\ntherefore, it is highly desired to devise an effective model to seamlessly incorporate such domain knowledge into the pure data-driven learning methods and hence boost the matching performance\nin this work, we aim to investigate the practical fashion problem of clothing matching by leveraging both the deep neural networks and the rich human knowledge in fashion domain\nin fact, the problem we pose here can be cast as the compatibility modeling between the complementary fashion items, such as tops and bottoms\nhowever, comprehensively model the compatibility between fashion items from both the data-driven and knowledgedriven perspectives is non-trivial due to the following challenges\n1) the human knowledge pertaining to fashion is usually implicitly conveyed by the compositions of fashion experts, which makes the domain knowledge unstructured and fuzzy\ntherefore, how to construct a set of structured knowledge rules for the clothing matching constitutes a tough challenge.\n 2) how to seamlessly encode such knowledge rules into the pure data-driven learning framework and enable the model to learn from not only the specific data but also the general rules poses another challenge for us\nand 3) for different samples, knowledge rules may present different levels of confidence and hence provide different levels of guidance\nfor example, as can be seen from figure 2, both compositions satisfy the rule \u201cstripe tops can go with stripe bottoms\u201d according to their contextual metadata.\nhowever, obviously, the given rule should impose more regularization towards the example of figure 2(a) and deserve higher rule confidence as compared to that of figure 2(b)\naccordingly, how to effectively assign the rule confidence is a crucial challenge\nto address the aforementioned challenges, we present a compatibility modeling scheme with attentive knowledge distillation, dubbed as akd-dbpr, as shown in figure 3, which is able to learn from both the specific data samples and the general domain knowledge\nin particular, we adopt the teacher-student scheme [18] to incorporate the domain knowledge (as a teacher) and enhance the performance of neural networks (as a student)\nas a pure data-driven learning, the student network aims to learn a latent compatibility space to unify the fashion items from heterogenous spaces with dual-path neural networks\nto comprehensively model the compatibility and the semantic relation between different modalities, the student network seamlessly integrates the visual and contextual modalities of fashion items by imposing hidden layers over the concatenated vectors of visual and contextual representations\nmoreover, to better characterize the relative compatibility between fashion items, we investigate the pairwise preference between complementary fashion items by building our student network based on the bayesian personalized ranking (bpr) framework [33]\nmeanwhile,we encode the domain knowledge with a set of flexible structured logic rules and encode these knowledge rules into the teacher network with regularizers, whereby we introduce the attention mechanism to attentively assign the rule confidence\nultimately, the student network is encouraged to not only achieve good performance of the compatibility modeling but also emulate the rule-regularized teacher network well\nour main contributions can be summarized in threefold\nwe present an attentive knowledge distillation scheme, which is able to encode the fashion domain knowledge to the traditional neural networks\nto the best of our knowledge, this is the first to incorporate fashion domain knowledge to boost the compatibility modeling performance in the context of clothing matching\nconsidering that different knowledge rules may have different confidence levels in the knowledge distillation procedure, we introduce the attention mechanism to the proposed scheme to flexibly assign the rule confidence\nextensive experiments conducted on the real-world dataset demonstrate the superiority of the proposed scheme over the state-of-the-art methods\nas a byproduct, we released the codes, and involved parameters to benefit other researchers\nthe remainder of this paper is structured as follows\nsection 2 briefly reviews the related work\nthe proposed akd-dbpr is introduced in section 3\nsection 4 presents the experimental results and analyses, followed by our concluding remarks and future work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Recently, the booming fashion sector and its huge potential benefits have attracted tremendous attention from many research communities\nIn particular, increasing research efforts have been dedicated to the complementary clothing matching as matching clothes to make a suitable outfit has become a daily headache for many people, especially those who do not have the sense of aesthetics\nThanks to the remarkable success of neural networks in various applications such as the image classification and speech recognition, the researchers are enabled to adopt the data-driven learning methods to analyze fashion items.\nNevertheless, existing studies overlook the rich valuable knowledge (rules) accumulated in fashion domain, especially the rules regarding clothing matching\nTowards this end, in this work, we shed light on the complementary clothing matching by integrating the advanced deep neural networks and the rich fashion domain knowledge\nConsidering that the rules can be fuzzy and different rules may have different confidence levels to different samples, we present a neural compatibility modeling scheme with attentive knowledge distillation based on the teacherstudent network scheme.\nExtensive experiments on the real-world dataset show the superiority of our model over several state-ofthe- art methods\nBased upon the comparisons, we observe certain fashion insights that can add value to the fashion matching study\nAs a byproduct, we released the codes, and involved parameters to benefit other researcher"},
{"doc": "recommendation systems are vital to keeping users engaged and satisfied with personalized recommendations in the age of information explosion.\nusers expect personalized content in modern e-commerce, entertainment and social media platforms but the effectiveness of recommendations are restricted by existing user-item interactions and model capacity\nthe ability to leverage higher order reasoning may help alleviate the problem of sparsity\n a popular and successful technique, collaborative filtering (cf), establishes the relevance between users and items from past interactions (e.g., clicks, ratings, purchases) by assuming similar users will consume similar items\n cf can generally be grouped in three categories: memory or neighborhood-based approaches, latent factor models and hybrid models [17, 26]\nmemory or neighborhood-based methods form recommendations by identifying groups or neighborhoods of similar users or items based on the previous interaction history\nthe simplicity of these models such as item k nearest neighbor (knn) have shown success in production systems at amazon [21]\nlatent factor models such as matrix factorization project each user and item into a common low dimensional space capturing latent relations\nneighborhood methods capture local structure but typically ignore the mass majority of ratings available due to selecting at most k observations from the intersection of feedback between two users or items [17]\non the other hand, latent factor models capture the overall global structure of the user and item relationships but often ignore the presence of a few strong associations\nthe following weaknesses between the local neighborhood-based and global latent factor models lead to the development of hybrid models such as svd++ [17] and generalizations such as factorization machines [24] which integrate both neighborhood-based approaches and latent factor models to enrich predictive capabilities\nrecently, deep learning has made massive strides in many research areas obtaining state of the art performance in computer vision [9], question answering [18, 30, 35, 39], learning programs [8], machine translation [1] and many other domains\nthe successful integration of deep learning methods in recommendation systems have demonstrated the noticeable advantages of complex nonlinear transformations over traditional linear models [40]\nhowever, existing composite architectures incorporate the latent factor model ignoring the integration of neighborhood-based approaches in a nonlinear fashion\nhence, we propose to represent the neighborhood-based component with a memory network [30, 35] to capture higher order complex relations between users and items\nan external memory permits encoding rich feature representations while the neural attention mechanism infers the user specific contribution from the community\nwe propose a unified hybrid model which capitalizes on the recent advances in memory networks and neural attention mechanisms for cf with implicit feedback.\nthe memory component allows read and write operations to encode complex user and item relations in the internal memory\nan associative addressing scheme acts as a nearest neighborhood model finding semantically similar users based on an adaptive user-item state\nthe neural attention mechanism places higher weights on specific subsets of users who share similar preferences forming a collective neighborhood summary\nfinally, a nonlinear interaction between the local neighborhood summary and the global latent factors1 derives the ranking score.\nstacking multiple memory components allows the model to reason and infer more precise neighborhoods further improving performance\nour primary contributions can be summarized as follows\nwe propose collaborative memory network (cmn) inspired by the success of memory networks to address implicit collaborative filtering\ncmn is augmented with an external memory and neural attention mechanism\nthe associative addressing scheme of the memory module acts as a nearest neighborhood model identifying similar users\nthe attention mechanism learns an adaptive nonlinear weighting of the user\\s neighborhood based on the specific user and item\n the output module exploits nonlinear interactions between the adaptive neighborhood state jointly with the user and item memories to derive the recommendation\nwe reveal the connection between cmn and the two important classes of collaborative filtering models\nthe latent factor model and neighborhood-based similarity model\nfurthermore, we reveal the advantages of the nonlinear integration fusing the two types of models yielding a hybrid model\ncomprehensive experiments on three public datasets demonstrate the effectiveness of cmn against seven competitive baselines\nmultiple experimental configurations confirm the added benefits of the memory module\nqualitative visualizations of the attention weights provide insight into the memory component providing supporting evidence for deeper architectures to capture higher order complex interaction", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Recommendation systems play a vital role to keep users engaged with personalized content in modern online platforms\nDeep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF)\nHowever, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches\nWe propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion\nMotivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component\nThe associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood\nFinally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score\nStacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations\n Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models\n Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines\nQualitative visualization of the attention weights provide insight into the model\u2019s recommendation process and suggest the presence of higher order interactio"},
{"doc": "the success of many web-enabled services has largely been attributed to recommender systems, which are the key to attracting users and promoting items\nmost real-world systems generate massive data at an unprecedented rate\nfor example, more than 10 million transactions are made per day in ebay [1] and about half a billion tweets are generated every day [11]\nthese data are temporally ordered, large-volume and high-velocity, which necessitate real-time streaming recommendation algorithms [33]\nmost existing recommendation methods are static, e.g. nearest neighbors and correlation coefficients usually are precomputed for collaborative filtering or factor matrices in matrix factorization\nthis means that the new coming data in the streams cannot be integrated into the trained model efficiently for classic recommender systems\nthere are three major challenges to be tackled in designing a streaming recommender system\ncapturing users\u2019 long-term interests\none alternative of developing a streaming recommender system is to learn the parameters of some classic recommender systems (e.g., latent factor models) online with stochastic gradient descent method, updating users\u2019 interests based on each new observation [20].\nthe main issue with this online approach is that they cannot maintain users\u2019 long-term interests because of their short-term \u201cmemory\u201d\nspecifically, since the updates of users\u2019 interests are only based on the most recent data points, the model quickly \u201cforgets\u201d users\u2019 past behaviors [4]\nusers\u2019 drifted interests and modeling new users or items\nthe data in streams are temporarily ordered. users\u2019 preference may drift over time [30, 32, 35]\nfor example, a mother tends to be interested in different goods for children as her child grows up\nhow to capture users\u2019 latest interests to avoid being overwhelmed by the large amount of data in the past is also important in streaming recommender systems\n on the other hand, new users and new items arrive continuously in data streams\nfor example, from 2007 to 2015, the number of registered users on amazon saw a dramatic increase from 76 millions to 304 million\nhow to identify and model new users or items from the large-volume and high velocity data streams is another major challenge in streaming recommender systems\nstream-oriented systems usually confront higher input rates than they can immediately process with their available computing resources [10, 26], including recommender systems [20]\nwhen input rates exceed the computing capacity, the system becomes overloaded\nfigure 1 is a simplified overload scenario.\nassume that, for the streaming recommender systems in [1, 4], the process time cost by each new input data is 2 seconds but the streaming data are coming at 1 activity per second\nthe input arrival timeline shows the time points when a new activity arrives and the output delivery timeline shows the points where the processing of corresponding input activities are delivered\nsince the system resources are able to process only 1 activity every 2 seconds, a queue starts to build up and response time starts increasing due to the queue waiting time\nrecently, several methods have been developed to make recommendations dynamically for streaming data [1, 2, 4, 20, 23]\nas a matter of fact, these approaches either do not address all the three challenges, or address the challenges with ineffective strategies\nin [4], diaz-aviles et. al maintain a random sample from the coming data and update the model only based on the sampled instances\ntheir aim is to stop the system from forgetting users\u2019 long-term interests\nhowever, its sampling strategy tends to overlook users\u2019 drifted interests as it has smaller probability of sampling to more recent data\nspecifically, the coming data are sampled based on the rule including the t^{th} data instance in the stream with the probability proportional to \\\\frac{1}{t}\nwith this rule, as time goes by (i.e., t increases), the newly generated data have decreasing probabilities to be sampled.\nin this way, this model has low capability in capturing users\u2019 drifted preferences and modeling new generated users or items\nsome other works designed for online recommendation, such as [1, 23], are able to capture users\u2019 interest drifts by designing schemes to update the models only based on the new generated data.\nhowever, they all tend to forget users\u2019 long-term interests as they only update their model based on the most recent data points [4]\nanother major limitation is that they both overlook the overload problem by assuming that they are able to learn sufficiently from all the newly generated data\n to deal with the possible overload problem, rendle et al. present approaches that determine whether online updates are performed or not on a data instance based on two metrics in [20]\nfirst metric aims at selecting the instances related to users with fewer history activities.\nthe other metric is based on the error between the true rating and the predicted rating for a data instance.\nthis work is rating oriented and has been proven by diaz-aviles et al. tending to forget users\u2019 long-term interest in [4].\nin this paper, we propose spmf, a stream-centered probabilistic matrix factorization model, for streaming recommendation\n spmf is based on pmf (probabilistic matrix factorization model) [22], which has been widely used in recommendation because of its scalability in large data sets [22, 31].\nthe goal of most traditional pmf models is to predict a personalized score r_{ui} which represents the preference of user u for item i\nhowever, the task of personalized recommendation in many real applications is to provide a user with a ranked list of items\n in this paper, under the guide of feasible extension to streaming setting, we design a novel and scalable offline pairwise pmf based on bpr [19], which treats personalized recommendation problem as personalized ranking and thus integrates the goal of ranking into the optimization process\nstochastic gradient decent (sgd) method, which is well known suitable for online updating, is adopted to optimize the proposed model\nto capture users\u2019 long-term interests, spmf employs the technique of reservoir, which has been widely used in stream-centered data management systems [4, 5, 27], to maintain a sample of history data.\nspmf samples the data following the principle of capturing an accurate \u201csketch\u201d of history data instances under the constraint of fixed memory space\nto capture both long-term interests and drifted interests, spmf uses the samples in the reservoir plus the new observations in data streams as the input for updating the model\nfor the overload problem in streaming recommendation, we design a wise positive sampling strategy to sample informative subsets from the designed input to conduct model updating.\nthere are many different sampling strategies [18, 19, 25, 31], and all these sampling strategies reduce the load at the expense of increasing sampling time\nunder the high-velocity streaming setting, we have limited time to do both sampling and updating.\nthus, a major challenge in designing such a positive sampling strategy under streaming setting is to trade off between sampling time and updating accuracy.\nin this paper, we design a novel gaussian classification model, to sample the data wisely and efficiently from the reservoir and the new observations in data streams\nthe basic idea is that the activities at a lower rank should have a higher probability to be sampled, as this kind of activities are more informative and helpful in correcting the model\nwith this gaussian classification model, spmf is able to deal with new users, new items and interest drift problems\nthe rank of an instance (u_{i},v_{j}) is related to the current system\u2019s predicting ability on the fact that user u_{i} is interested in item v_{j}\nthis predicting ability is learned from the history activities containing the similar patterns\na higher rank represents that there are more history activities indicating that ui is interested in v_{j}.\n for new users, it is obvious that the system has no prior knowledge at all about their preferences and the instances related to them will get a lower rank.\nsimilarly, for the new items, there is also no history activities about them and the system cannot predict users\u2019 preference over them\nthus, the data containing new items will also get a lower rank\nit\u2019s obvious that this scheme also prefers to sample the data instances containing drifted interest for existing users as these instances contain very different behavior patterns compared with the history patterns captured by the obtained model parameters\nthis paper is organized as follows:\n section 2 provides some mathematical notations and preliminary concepts about the traditional recommendation model\nsection 3 formulates our streaming recommendation problem and describes the spmf model\nsection 4 discusses the experimental evaluation of spmf model.\nexisting research related to our work is surveyed in section 5\nsection 6 closes this paper with some conclusive remarks", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Studying recommender systems under streaming scenarios has become increasingly important because real-world applications produce data continuously and rapidly\nHowever, most existing recommender systems today are designed in the context of an offline setting\nCompared with the traditional recommender systems, large-volume and high-velocity are posing severe challenges for streaming recommender systems\nIn this paper, we investigate the problem of streaming recommendations being subject to higher input rates than they can immediately process with their available system resources (i.e., CPU and memory)\nIn particular, we provide a principled framework called as SPMF (Stream-centered Probabilistic Matrix Factorization model), based on BPR (Bayesian Personalized Ranking) optimization framework, for performing efficient ranking based recommendations in stream settings\nExperiments on three real-world datasets illustrate the superiority of SPMF in online recommendation"},
{"doc": "equipped with global positioning system (gps) devices, vehicles can now use various location-based services such as waze to navigate complex road networks\nby sampling a series of points of a vehicle path along the road network at fixed time intervals, a trajectory can be recorded using three features:\n 1) network-constrained, as most trajectories travel in a fixed road network [19]\n 2) varying sample rates \u2013 every trace from a same path can be sampled to a different number of points\n3) errors as a result of low gps accuracy\na wide variety of trajectory search queries [3, 10, 17\u201319, 32, 33, 36, 38] have been proposed over the years to support various location based services such as transportation monitoring and planning [47], or ranked retrieval, as shown in example 1.\nexample 1\nto monitor vehicles passing through manhattan, a user would issue a range query\n to monitor all cars that use wall street, a path query [19] would be issued\nfurther, a strict path query would identify every vehicle that traverse all of wall street.\nthese three kinds of queries are boolean trajectory search queries\ngiven a trajectory, a top-k trajectory similarity search query returns the k highest ranked trajectories based on a similarity metric [12, 47], which can be used to investigate driving habits [5], cluster trajectories to discover popular routes [46], or search over all of the trajectories which have a pre-specified trajectory as a k nearest neighbor \u2013 a task used in transportation route planning [42]\nit is desirable to support all of these search queries in a single trajectory search engine\nfurthermore, there are several commonly used similarity measures to determine the relevance between two trajectories (details in section 2), each with its own merits\nhowever, the effectiveness of these similarity measures has only been evaluated in domain specific search scenarios such as time series [4, 45]\nthe lack of an efficient trajectory search engine capable of supporting different similarity measures makes it very difficult to carry out careful effectiveness evaluations\nfor example, existing trajectory systems [2, 7, 41] only support trajectory storage and simple querying, but cannot perform top-k similarity search\n in this paper, we devise a trajectory search engine capable of supporting a richer set of complex queries and similarity measures\nthe two primary goals of a search engine are effectiveness (quality) and efficiency (speed) [6]\nthese two desiderata are often in tension with each other\ndue to the varying sampling rates of trajectories, existing similarity measures are rarely efficient and effective enough to search even modest sized trajectory collections (details in 4.1).\nmoreover, the effectiveness evaluation of top-k similarity search over trajectories currently rely on improving the classification precision of time series data\ndetermining whether a similarity measure can classify a set of trajectories into the corresponding labeled ground-truth, such as by chen et al. [4], is not a sufficient approach when evaluating the quality of a search engine.\n to achieve scalable efficiency, a space-efficient index representation and processing framework is crucial, but existing indexes for trajectory search [28, 31, 38] rely on an r-tree [14], which stores all points from the raw trajectories\nthey often require a prodigious amount of space in order to accelerate search, and the pruning method originally devised for point search is not effective for trajectory search as many of the similarity measures are non-metric.\ninspired by these observations, we design and implement torch, that contains the three modules shown in figure 1\nfirst, preprocessing derived from map matching [22] projects raw trajectories to a succinct path, where each mapped trajectory is represented by a list of vertices and edges in a road network\nbased on this new representation, a new similarity measure (lors) is developed which can significantly improve the effectiveness and efficiency of similarity search in real data collections\nthe processing speed is further improved by using a lightweight edge and vertex index (levi), which is highly compressable using standard integer list compression techniques\ntop-k search using both lors and existing similarity measures can be performed efficiently using the unified dynamic pruning algorithm presented in this paper\nin addition, boolean trajectory search can be performed efficiently in a manner similar to boolean processing in information retrieval systems [9]\nfinally, the search results from all similarity measures can be evaluated using a new path-based ground truth set.\nin summary, we make the following contributions:\nwe present torch, a trajectory search engine which integrates pre-processing, indexing, querying and evaluation\nwe propose a novel trajectory similarity measure \u2013 longest overlapped road segments (lors) based on the overlapped segments between query and trajectory data (section 4)\nwe propose a unified index \u2013 levi with compression (section 5), and an efficient search paradigm (section 6) to support similarity and boolean search over trajectories\nwe employ torch to conduct a comprehensive efficiency and effectiveness evaluation of similarity measures using two real taxi trajectory datasets (section 7", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "This paper presents a new trajectory search engine called Torch for querying road network trajectory data\nTorch is able to efficiently process two types of typical queries (similarity search and Boolean search), and support a wide variety of trajectory similarity functions\nAdditionally, we propose a new similarity function LORS in Torch to measure the similarity in a more effective and efficient manner\nIndexing and search in Torch works as follows\n First, each raw vehicle trajectory is transformed to a set of road segments (edges) and a set of crossings (vertices) on the road network\nThen a lightweight edge and vertex index called LEVI is built\nGiven a query, a filtering framework over LEVI is used to dynamically prune the trajectory search space based on the similarity measure imposed. \nFinally, the result set (ranked or Boolean) is returned\nExtensive experiments on real trajectory datasets verify the effectiveness and efficiency of Torc"},
{"doc": "the dramatic growth of publicly accessible mobile/geo-tagged data has triggered a revolution in location based services [10].\nan emerging thread is route planning, with pervasive applications in trip recommendation, intelligent navigation, ride-sharing, and augmented reality gaming, etc\naccording to [23], the travel and tourism industry directly and indirectly contributed us$7.6 trillion to the global economy and supported 292 million jobs in 2016\nthe majority of current route planning systems yields shortest paths or explores popular pois [28], or recommends routes based on users\u2019 historical records [15] or crowdsourced experience [20]\na practical problem that has not been well studied is that, a user wants to be suggested a small number of routes that not only satisfy her cost budget and spatial constraints, but also best meet her personalized requirements on diverse poi features\nwe instantiate this problem with a travel scenario\nconsider that a new visitor to rome wishes to be recommended a trip, starting from her hotel and ending at the airport, that allows her to visit museums, souvenir shops, and eat at some good italian restaurants (not necessarily in this order) in the remaining hours before taking the flight\nshe values the variety over the number of places visited, e.g., a route consisting of one museum, one shop, and one italian restaurant is preferred to a route consisting of two museums and two shops\nthe above problem is actually generalizable to various route planning scenarios, and they illustrate some common structures and requirements\nfirst, there is a poi map where pois are connected by edges with traveling cost between pois, and each poi has a location and is associated with a vector of features (e.g., museum) with numeric or binary ratings.\nthe poi map can be created from google map, and features and ratings of pois can be created from user rating and text tips available on location-based services such as foursquare, or extracted from check-ins and user provided reviews [7]\n  second, the user seeks to find top-k routes {p_{1}, \u00b7 \u00b7 \u00b7 , p_{k}},from a specified source x to a specified destination y within a travel cost budget b, that have highest values of a certain gain function gain(p_{iv} ) for the set of pois p_{iv} on the routes p_{i}\nthe user specifies her preference of routes through a weight vector w with w_{h} being the weight of a feature h, and a route diversity requirement, which specifies a trade-off between quantity (the number of pois with a preferred feature) and variety (the coverage of preferred features) for the pois on a route\nthe gain function has the form gain(p_{v} ) =\\\\sum _{h}w_{h}\\\\phi _{h}(p_{v}), where phi _{h} for each feature h aggregates the feature\u2019s scores of the pois p_{v} \nto better motivate the route diversity requirement, let us consider the poi map in figure 1 and a user with the source v1, destination v5 and the budget b = 18\nthe user weights the features park and museum using the vector w = (0.4, 0.6, 0), and values both quantity and variety\nif the sum aggregation \\\\phi _{h} is used, the route v_{1} \\\\rightarrow  v_{6} \\\\rightarrow  v_{4} \\\\rightarrow  v_{5} will have the highest gain\nhowever, the user may not prefer this route because it does not include any park though it includes 3 museums\n with the max aggregation used, the route v_{1} \\\\rightarrow  v_{3} \\\\rightarrow  v_{5} has the highest gain by including one top scored museum and one top scored park, but this route does not maximally use the entire budget available\nintuitively, the sum aggregation is \u201cquantity minded\u201d but ignores variety, whereas the max aggregation is the opposite;\nneither models a proper trade-off between quantity and variety as the user considered\n the above user more prefers the route v_{1} \\\\rightarrow  v_{2} \\\\rightarrow  v_{3} \\\\rightarrow  v_{5} that visits multiple highly scored museums and parks, which will better address both quantity and variety\nsolving the top-k route search problem faces two challenges\nchallenge i\none challenge is to design a general enough phi _{h} that includes a large class of aggregation functions to model a personalized route diversity requirement where each user has her own quantity and variety trade-off\n      our approach is treating the satisfaction by visiting each poi as the marginal utility and modeling the aggregation of such utilities of pois with the diminishing marginal utility property by submodular set functions phi _{h}\nthe intuition is that, as the user visits more pois of the same, her marginal gain from such visits decreases gradually.\nsubmodularity has been used for modeling user behaviors in many real world problems [14][11]\nto the best of our knowledge, modeling user\u2019s diversity requirement on a route by submodularity has not been done previously\nchallenge ii. the top-k route problem is np-hard as it subsumes the np-hard orienteering problem [4]\nhowever, users typically demand the routes not only be in high quality, even optimal, but also be generated in a timely manner (seconds to minutes)\nfortunately, the users\u2019 preferences and constraints on desired routes provide new opportunities to reduce the search space and find optimal top-k routes with fast responses.\nfor example, for a user with only 6-hour time budget and preferring museums and parks on a route, all the pois in other types or beyond the 6 hours limit will be irrelevant\nthe key of an exact algorithm is to prune, as early as possible, such irrelevant pois as well as the routes that are unpromising to make into the top-k list due to a low gain gain(p_{v} )\nhowever, this task is complicated by the incorporation of a generic submodular aggregation function phi _{h} motivated above in our objective gain(p_{v} ), and designing a tight upper bounding strategy on gain(p_{v} ) for pruning unpromising routes is a major challenge\ncontributions\nthe main contributions of this paper are\nwe define the top-k route search problem with a new personalized route diversity requirement where the user can choose any submodular function phi _{h} to model her desired level of diminishing return.\nas an instantiation, we show that the family of power-law functions is a sub-family of submodular functions and can model a spectrum of personalized diversity requirement. (section 3\n  our first step towards an efficient solution is to eliminate irrelevant pois for a query, by proposing a novel structure for indexing the poi map on both features and travel costs\nthis index reduces the poi map to a small set of pois for a query.(section 4\nour second step towards an efficient solution is to prune unpromising routes, by proposing a novel optimal algorithm, pacer\nthe novelties of the algorithm include an elegant route enumeration strategy for a compact representation of search space and the reuse of computed results, a cost-based pruning for eliminating non-optimal routes, and a gain-based upper bound strategy for pruning routes that cannot make into the top-k list\nthe algorithm works for any submodular function phi _{h}. (section 5\nto deal with the looser query constraints, we present two heuristic algorithms with a good efficiency-accuracy trade-off, by finding a good solution with far smaller search spaces.(section 6\nwe evaluate our algorithms on real-world data sets\npacer provides optimal answers while being orders of magnitude faster than the baselines\nthe heuristic algorithms provide answers of a competitive quality and work efficiently for a larger poi map and/or a looser query constraint. (section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We consider a practical top-k route search problem\ngiven a collection of points of interest (POIs) with rated features and traveling costs between POIs, a user wants to find k routes from a source to a destination and limited in a cost budget, that maximally match her needs on feature preferences\nOne challenge is dealing with the personalized diversity requirement where users have various tradeoff between quantity (the number of POIs with a specified feature) and variety (the coverage of specified features).\n Another challenge is the large scale of the POI map and the great many alternative routes to search\nWe model the personalized diversity requirement by the whole class of submodular functions, and present an optimal solution to the top-k route search problem through indices for retrieving relevant POIs in both feature and route spaces and various strategies for pruning the search space using user preferences and constraints\nWe also present promising heuristic solutions and evaluate all the solutions on real life dat"},
{"doc": "designing music information access systems requires understanding the diverse needs of users and their expectations of system performance.\nsuch needs include mood-setting, social standing, or nostalgia.\nthe growth of streaming platforms in the recent decade, however, has reshaped users\u2019 interactions with music  access systems.\nin streaming platforms, users are provided with access to large repositories of audio content with only a small fraction familiar to them.\nthis necessitates a new focus on one particular need: music discovery, which we define as the experience of finding and listening to content that is previously unknown to the user\nresearch has supported  the critical role of discovery in streaming platforms.\nm\u00e4ntym\u00e4ki and najmul islam surveyed 374 finnish music streaming users and identified  discovery as users\u2019 main moti vation to continue platform subscription [22].\nbrown and krause\u2019s survey of 440 music listeners provided additional support in discovery\u2019s role in improving user retention [2].\nadditional survey and interview  studies also demonstrated that discovery is an important need for music listeners [16\u201318].\nmoreover, laplante found that music discovery requires a \u2018different state of mind\u2019, suggesting that discovery deserves special treatment by music information  access designers [15].\nleong and wright found through interviews that discovery results in deeper social connection [21].\nlee and price identified several personas in music consumption that focus on discovery, demonstrating that discovery is a complex, nuanced, and personal need [19]\nwhile existing work suggests that discovery is important, user studies in the context of recommender systems showed that novel recommendations were negatively correlated with perceived quality [3, 7, 23].\nother user studies confirmed that, when novelty is desired, approaches based on popularity lead to suboptimal performance [14].\nas system designers, we need a more nuanced understanding of user expectations surrounding novelty and discovery.\nwe have limited knowledge about how users behave in a discovery  context and, moreover, how these behaviors change when systems fail to satisfy user expectations\nwe investigated the following questions\nwhat are users\u2019 expectations in the setting of discovery- oriented recommendation\nwhich interactions with a recommendation system for music discovery can be used to estimate user satisfaction\nhow can data-driven models of user satisfaction  be used to evaluate the performance of recommendation algorithms\nwe conducted five studies that explore different aspects of user experience and user satisfaction.\nfirst, we carried out face-to-face interviews with users to help us form hypotheses about user expectations of novel recommendations and what behaviors may be correlated with satisfactory and unsatisfactory experiences\nsecond, we validated  these hypotheses at scale by using unsupervised learning to analyze logged interaction data with a music streaming platform.\nnext, we deployed a survey that explicitly asked users how satisfied they were with their personalized discovery recommendations.\nthen, we fit a statistical model of user satisfaction  based on behavioral signals identified from the first round of analysis.\nfinally, we proposed and validated a set of online metrics for evaluating recommendation performance\nfor the purpose of this work, we gathered data from a music streaming platform, where users were provided with a fixed set of thirty personalized discovery recommendations.\nthese recommendations were updated weekly and contained tracks similar to the taste of the user, but were unknown  to them previously.\nusers could interact with the recommendations by playing or skipping tracks.\nthey could also navigate to the album or artist page for any of the tracks and listen to other tracks by the same artist.\neven though the content of the recommendations changed each week, users could add any of the tracks to a playlist or save them to their library for future access.\nusers could  listen  within a user playlist, library, artist page, or album page.\nwe refer to listening to the recommendations or tracks by the same artist within these contexts as \u201cdownstream listening.\nwe found that user needs and behaviors were dependent upon their goals; we identified and validated four user goals in music discovery.\ninferring these user goals, in addition to normalizing interaction data per-user and capturing peak interactions at the track level, were informative for estimating user satisfaction.\nmoreover, we developed online  metrics  based on these findings that can be used to compare performance between two recommendation algorithms\nthe paper is organized as follows.\n section 2 provides an overview of related work on novelty, discovery, and user satisfaction.\nsections 3-7 summarize the methods and results for each of the five studies: user interviews, unsupervised learning, survey, statistical model, and online metrics.\nsections 8-9 synthesize our findings and point to future wor", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We study the use and evaluation of a system for supporting music discovery, the experience of finding and listening to content previ- ously unknown to the user.\nWe adopt a mixed methods approach, including interviews, unsupervised learning, survey research, and statistical modeling, to understand and evaluate user satisfaction in the context of discovery.\nUser interviews  and survey data show that users\u2019 behaviors change according to their goals, such as listening to recommended tracks in the moment, or using recommendations as a starting point for exploration.\nWe use these findings  to develop a statistical model of user satisfaction at scale from interactions with a music streaming platform.\nWe show that capturing users\u2019 goals, their deviations from their usual behavior, and their peak interactions on individual tracks are informative for estimating user satisfaction.\nFinally, we present and validate heuristic metrics that are grounded in user experience for online evaluation of recommen- dation performance.\nOur findings, supported with evidence from both qualitative and quantitative studies, reveal new insights about user expectations with discovery and their behavioral responses to satisfying and dissatisfying system"},
{"doc": "literature search helps a researcher identify relevant papers and summarize essential claims about a topic, forming a critical step in any scientific research\nwith the fast-growing volume of scientific publications, a good literature search engine is essential to researchers, especially in the domains like computer science and biomedical science where the literature collections are so massive, diverse, and rapidly evolving\u2014few people can master the state-of the-art comprehensively and in depth\na large set of literature search queries contain multiple entities which can be either concrete instances (e.g., gabp (a gene)) or abstract concepts (e.g., clustering).\nwe refer these queries as entity-set queries\nfor example, a computer scientist may want to find out how knowledge base can be used for document retrieval and thus issues a query \u201cknowledge base for document retrieval\u201d, which is an entity-set query containing two entities\nsimilarly, a biologist may want to survey how genes gabp, tert, and cd11b are associated with cancer and submits a query \u201cgabp tert cd11b cancer\u201d, another entity-set query with one disease and three gene entities. \ncompared with typical short keyword queries, a distinctive characteristic of entity-set queries is that they reflect user\u2019s need for finding documents containing inter-entity relations\nfor example, among 50 queries collected from biologists in 2005 as part of trec genomics track [14], 40 of them are explicitly formulated as finding relations among at least two entities\nin most cases, a user who submits an entity-set query will expect to get a ranked list of documents that are most relevant to the whole entity set\n therefore, as in the previous examples, returning a paper about only knowledge bases or only one gene gabp is unsatisfactory\nentity-set queries pose non-trivial challenges to existing search platforms\n for example, among the 100 queries released by semantic scholar (s2), 40 of them are entity-set queries and s2\u2019s production ranking system performs poorly on these entity-set queries, as shown in table 1\nthe difficulties of handling entity-set queries mainly come from two aspects\nfirst, entity relations within entity sets have not been modeled effectively\nthe association or cooccurrence of multiple entities has not gained adequate attention from existing ranking models.\nas a result, those models will rank papers where a single distinct entity appears multiple times higher than those containing many distinct entities\nsecond, entity-set queries are particularly challenging for supervised ranking models\nas manual labeling of document relevance in academic search requires domain expertise, it is too expensive to train a ranking model based purely on manually labeling\nmost systems will first apply an off-the-shelf unsupervised ranking model during their cold-start process and then collect user interaction data (e.g., click information)\nunfortunately, entity-set queries are usually sparse (i.e., not so repetitive), and have less associated click information\n  furthermore, many off-the-shelf unsupervised models cannot return reasonably good candidate documents for entity-set queries within the top-20 positions\nmany highly relevant documents will not be presented to users, which further compromises the usefulness of clicking information.\nthis paper tackles the new challenge\u2014improving the search quality of scientific literature on entity-set queries and proposes an unsupervised ranking approach\nwe introduce setrank, an unsupervised ranking framework that explicitly models inter-entity relations and captures entity type information\n setrank first links entity mentions in query and documents to an external knowledge-base\n then, each document is represented with both bag-of-words and bag-of-entities representations [36, 37] and fits two language models respectively\non the query side, a novel heterogeneous graph representation is proposed to model complex entity information (e.g., entity type) and entity relations within the set\nthis heterogeneous query graph represents all the information need in that query.\nfinally, the query-document matching is defined as a graph covering process and each document is ranked based on the information need it covers in the query graph\nalthough being an unsupervised ranking framework, setrank still has some parameters that need to be appropriately learned using a labeled validation set\nto further automate the process of ranking model development, we develop a novel unsupervised model selection algorithm based on the technique of weighted rank aggregation\ngiven a set of queries with no labeled documents, and a set of candidate parameter settings, this algorithm automatically learns the most suitable parameter settings for that set of queries\nthe significance of our proposed unsupervised ranking approach is two-fold\nfirst, setrank itself, as an unsupervised ranking model, boosts the literature search performance on entity-set queries\nsecond, setrank can be adopted during the cold-start process of a search system, which enables the collection of high-quality click data for training subsequent supervised ranking model\n our experiments on two benchmark datasets demonstrate the usefulness of our unsupervised model selection algorithm and the effectiveness of setrank for searching scientific literature, especially on entity-set queries\nin summary, this work makes the following contributions\n(1) a new research problem, effective entity-set search of scientific literature, is studied\n(2) setrank, an unsupervised ranking framework, is proposed, which models inter-entity relations and captures entity type information\n(3) a novel unsupervised model selection algorithm is developed, which automatically selects setrank\u2019s parameter settings without resorting to a labeled validation set\n(4) extensive experiments are conducted in two scientific domains, demonstrating the effectiveness of setrank and our unsupervised model selection algorithm.\nthe remaining of the paper is organized as follows\n section 2 discusses related work\n  section 3 presents our ranking framework setrank\nsection 4 presents the unsupervised model selection algorithm\n  section 5 reports and analyzes the experimental results on two benchmark datasets and shows a case study of setrank for biomedical literature search\nfinally, section 6 concludes this work with discussions on some future direction", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Literature search is critical for any scientific research\nDifferent from Web or general domain search, a large portion of queries in scientific literature search are entity-set queries, that is, multiple entities of possibly different types\nEntity-set queries reflect user\u2019s need for finding documents that contain multiple entities and reveal inter-entity relationships and thus pose non-trivial challenges to existing search algorithms that model each entity separately\nHowever, entity-set queries are usually sparse (i.e., not so repetitive), which makes ineffective many supervised ranking models that rely heavily on associated click history\nTo address these challenges, we introduce SetRank, an unsupervised ranking framework that models inter-entity relationships and captures entity type information. \nFurthermore, we develop a novel unsupervised model selection algorithm, based on the technique of weighted rank aggregation, to automatically choose the parameter settings in SetRank without resorting to a labeled validation set\nWe evaluate our proposed unsupervised approach using datasets from TREC Genomics Tracks and Semantic Scholar\u2019s query log\nThe experiments demonstrate that SetRank significantly outperforms the baseline unsupervised models, especially on entity-set queries, and our model selection algorithm effectively chooses suitable parameter setting"},
{"doc": "natural language understanding has been a long desired goal in information retrieval\nin search engines, the process of text understanding begins with the representations of query and documents\nthe representations can be bag-of-words, the set of words in the text, or bag-of-entities, which uses automatically linked entity annotations to represent texts [10, 20, 25, 29].\nwith the representations, the next step is to estimate the term (word or entity) importance in text, which is also called term salience estimation [8, 9]\nthe ability to know which terms are salient (important and central) to the meaning of texts is crucial to many text-related tasks\nin ad hoc search, the document ranking is often determined by the salience of query terms in them, which is typically estimated by combining frequency-based signals such as term frequency and inverse document frequency [5].\neffective as it is, frequency is not equal to salience\n for example, a wikipedia article about an entity may not repeat the entity the most frequently\na person\u2019s homepage may only mention her name once\na frequently mentioned term may be a stopword\nin word-based retrieval, many approaches have been developed to better estimate term importance [3].\nhowever, in entity-based representations [20, 26, 29], while entities convey richer semantics [1], entity salience estimation is a rather immature task [8, 9] and its effectiveness in search has not yet been explored\nthis paper focuses on improving text understanding and retrieval by better estimating entity salience in documents\n we present a kernel entity salience model (kesm) that estimates entity salience end-to-end using neural networks\n given annotated entities in a document, kesm represents them using knowledge enriched embeddings and models the interactions between entities and words using a kernel interaction model [27]\n in the entity salience task [9], the kernel scores from the interaction model are combined by kesm to estimate entity salience, and the whole model, including the knowledge enriched embeddings and kernel interaction model, is learned end-to-end using a large number of salience labels\nkesm also improves ad hoc search by modeling the salience of query entities in candidate documents\ngiven a query-document pair and their entities, kesm uses its kernels to model the interactions of query entities with the entities and words in the document. \nit then merges the kernel scores to ranking features and combines these features to rank documents\n in ad hoc search, kesm can either be trained end-to-end when sufficient ranking labels are available, or be first pre-trained on the salience task and then adapted to search as a salience ranking feature extractor\nour experiments on a news corpus [9] and a scientific proceeding corpus [29] demonstrate kesm\u2019s effectiveness in the entity salience task\nit outperforms previous frequency-based and feature-based models by large margins, while requires much less linguistic preprocessing than the feature-based model\n our analyses find that kesm has a better balance on popular (head) entities and rare (tail) entities when predicting salience\n in contrast, frequency-based or feature-based methods are heavily biased towards the most popular entities\u2014less attractive to users as they are more expected\nalso,kesm is less sensitive to document length while frequency-based methods are not as effective on shorter documents\nour experiments on trec web track search tasks show that kesm\u2019s text understanding ability in estimating entity salience also improves search accuracy\nthe salience ranking features from kesm, pre-trained on the news corpus, outperform both word-based and entity-based features in learning to rank, despite various differences in the salience and search tasks.\n our case studies find interesting examples showing that kesm favors documents centering on query entities over those merely mentioning them\nwe find it encouraging that the fine-grained text understanding ability of kesm\u2014the ability to model the consistency and interactions between entities and words in texts\u2014is indeed valuable to ad hoc search\n the next section discusses related work.\nsection 3 describes the kernel entity salience model and its application to entity salience estimation\nsection 4 discusses its application to ad hoc search\nexperimental methodology and results for entity salience are presented in sections 5 and section 6\nthose for ad hoc search are in sections 7 and section 8. section 9 conclude", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents\nKESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience.\nThe whole model is learned end-to-end using entity salience labels\nThe salience model also improves ad hoc search accuracy, providing effective ranking features by modeling the salience of query entities in candidate documents.\nOur experiments on two entity salience corpora and two TREC ad hoc search datasets demonstrate the effectiveness of KESM over frequency-based and feature-based methods\nWe also provide examples showing how KESM conveys its text understanding ability learned from entity salience to searc"},
{"doc": "most information retrieval (ir) evaluation measures focus on estimating the quality of a ranked list of results, where each result is a simple link to another web page [32]\nhowever, modern web search engine result pages (serps) are complex, composite, responses with curated and computationally selected elements, consisting of algorithmic web results, advertisements and a variety of answer cards\nfurthermore, result elements are positioned in different layouts on the serp: e.g. in the header, left rail, core, right rail, or footer\nconsequently, the assumptions implicit in many retrieval measures no longer hold in the context of evaluating modern serps [2]\nwhile there has been a number of studies investigating which specialist answers (\u201cverticals\u201d) to include [1, 3, 39], which verticals are preferred [2], and how they affect search behaviour and satisfaction [11, 23], in this work we attempt to directly measure the quality of a whole serp\nto do so a number of sub-questions first need to be addressed\n(i) what are the different elements on a serp,\n(ii) in what order are the elements examined,\n(iii) what is the cost of inspecting different elements\n(iv) what is the benefit of those elements, and ultimately,\n(v) what is the expected (total) utility of a serp\nto start addressing these questions, we studied serps from a major web search engine analysing the different types of elements shown for one thousand popular queries, and evaluated different possible \u201corderings\u201d given the layout of the serps\n from this analysis, we were then able to estimate the time spent per element type, which was used to estimate the cost of processing serps\ngiven relevance assessments for the elements appearing on the top thousand queries, we then inferred an aggregate utility curve experienced over time\u2014that is, the total utility experienced by the population of users given the cost they incurred (time spent)\nwe argue that a \u201cgood\u201d measure will approximate observed behaviours (e.g. time spent and stopping rank) and inferred utility\nsince different measures provide different perspectives, and are in different units, it is not possible to directly compare them to the inferred utility curves\nto address this problem, this paper draws upon the recent theoretical developments regarding the measurement and modelling of ir systems [9, 25, 26] where measures can be expressed as either the expected utility (eu) or expected total utility (etu) [9], depending on how the stopping/continuation function that defines the user stopping model is expressed [25, 26]\nwe then draw upon information foraging theory [30] to develop a forager based user stopping model that adapts its stopping behaviour based on the gain accrued during the search process: directly connecting the theory of how we model people\u2019s search behaviour with how we measure it\nwe show that our forager-based user model provides a better approximation of the utility experienced than do other common model", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Web Search Engine Result Pages (SERPs) are complex responses to queries, containing many heterogeneous result elements (web results, advertisements, and specialised \u201canswers\u201d) positioned in a variety of layouts\nThis poses numerous challenges when trying to measure the quality of a SERP because standard measures were designed for homogeneous ranked lists\nIn this paper, we aim to measure the utility and cost of SERPs\nTo ground this work we adopt the C/W/L framework which enables a direct comparison between different measures in the same units of measurement, i.e. expected (total) utility and cost\nWithin this framework, we propose a new measure based on information foraging theory, which can account for the heterogeneity of elements, through different costs, and which naturally motivates the development of a user stopping model that adapts behaviour depending on the rate of gain.\nThis directly connects models of how people search with how we measure search, providing a number of new dimensions in which to investigate and evaluate user behaviour and performance\nWe perform an analysis over 1000 popular queries issued to a major search engine, and report the aggregate utility experienced by users over time\nThen in an comparison against common measures, we show that the proposed foraging based measure provides a more accurate reflection of the utility and of observed behaviours (stopping rank and time spent"},
{"doc": "with the rapid growth of multimedia contents, image search engines has become a popular and supplementary information sources for searchers\ncompared to that of the general (textual) web search engines, the presentation of image search results is quite different.\nfirst of all, most image search engines display image snapshots rather than document snippets so that users can directly see an image preview of the result\ndue to this, in addition to relevance, other factors, such as image visual attractiveness, may affect users\u2019 perceived satisfaction of image search results [15].\nsecondly, image search results are placed in a two-dimensional panel rather than the one dimensional ordered ranking lists within the traditional general web search engine result pages (serps)\nthis can affect user\u2019s examination behavior.\nbased on the analysis of eye gaze first arrival time and examination duration, xie et al. [54] observe a middle-position bias of user\u2019s examination behavior in image search\nthis behavior does not conform to the traditional \u201cgolden triangle\u201d phenomena in general web search\nthirdly, rather than clicking the \u201cnext page\u201d buttons to navigate for more results, image search engines generally load new pages (results) implicitly, which means that users can view images across different pages by scrolling up and down.\nthese differences between image search and generalweb search may have an impact on search evaluation, which sits at the center of ir research\ncarterette [4] formulates a conceptual framework to interpret traditional model-based measures like dcg [24], rbp [40] and err [5]\nhe argues that these measures are actually composed of three underlying models, which are browsing model, document utility model and utility accumulation model\nhowever, these models may face challenges in image search scenarios:\n(1) a browsing model describes how a user interacts with results in serps\nto date, the most well-developed browsing model is that users scan ranked results one-by-one from top to bottom before they stop\nhowever, the middle-position bias observed by xie et al. [54] indicates that there exists a different browsing model in image search.\n(2) a document utility model represents how a user derives utility from individual relevant documents\nrelevance judgments from external assessors are often collected to model document utility\nnevertheless, the user-perceived utility of an image result may be affected by topical relevance as well as other factors\nsimilar phenomenon was also observed by geng et al. [15]\ntheir user study in comparing image search results of google and bing suggests that topical relevance may not be the leading discriminating factor of web image search engines\n(3) a utility accumulation model depicts how a user accumulates utility in the course of browsing.\ncarterette [4] introduces four utility accumulation models and classifies measures into four distinct families.\nhowever, these models are all designed for general web search, not considering the two-dimensional result placement in image search\nsimilarly, online metrics, which have been widely adopted for modern search engines, are also confronted with challenges in web image search\nthe online metrics are mostly based on user behavior\ndue to the differences of user behavior between generalweb search and image search [3, 43], the effective general web search metrics may not be appropriate for measuring image search.\nthere exist a number of studies on revealing the relationship between different evaluation methods and user satisfaction [2, 7, 39, 45]\nrecently, chen et al. [7] meta-evaluate a series of existing online and offline metrics to study how well they infer actual search user satisfaction in different search scenarios\nthe results suggest that offline metrics work better in homogeneous search environment while online metrics are more consistent with user satisfaction in heterogeneous search environment\nalthough different evaluation metrics have been thoroughly studied in the context of generalweb search, as far as we know, no existing work sufficiently investigates how well these metrics perform in web image search scenarios\nconsidering the differences between generalweb search and image search, whether these evaluation metrics are applicable for image search remains an open research question\ntherefore, in this paper, we focus on the performances of different offline and online evaluation metrics on measuring user satisfaction in the context of image search.\nbased on a laboratory user study, we collect users\u2019 explicit satisfaction feedbacks as well as user behavior signals (such as click, hover, dwell time etc.) in image search\nfollowing the previous work [41], we also gather separate judgments for topical relevance and image quality from external assessors hired by a crowdsourcing platform.\nby comparing those two judgments, we find they are correlated but different.\nin general, the offline metrics can achieve a better performance based on a combination of those two judgments in the context of image search\nhowever, these two judgments can play different roles in affecting user satisfaction for different search tasks (e.g. image quality is more important for locating tasks, compared to exploring tasks)\ncompared with offline metrics, we find that online metrics based on mouse click information correlate much better with user satisfaction\nfurthermore, our findings indicate that users\u2019 click behavior is an essential and powerful signal to infer user satisfaction in image search scenarios.\nto summarize, the main contributions of this paper are as follows\nwe thoroughly compare the performance of different offline and online evaluation metrics and user satisfaction in the context of image search\nin general, we find online metrics based on mouse click information align better with user satisfaction than offline metrics based on the combination of topical relevance and image quality judgments\nwe propose to incorporate image quality with topical relevance for offline evaluation in image search scenarios and compare the difference between these two judgments\nwe adapt several existing web search offline evaluation metrics so that they can measure in a two-dimensional presentation in image search\nwe suggest that users\u2019 click behavior is a good indicator to infer user satisfaction in image search scenario", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Comparing to general Web search engines, image search engines present search results differently, with two-dimensional visual image panel for users to scroll and browse quickly.\nThese differences in result presentation can significantly impact the way that users interact with search engines, and therefore affect existing methods of search evaluation.\nAlthough different evaluation metrics have been thoroughly studied in the general Web search environment, how those offline and online metrics reflect user satisfaction in the context of image search is an open question\nTo shed light on this, we conduct a laboratory user study that collects both explicit user satisfaction feedbacks as well as user behavior signals such as clicks\nBased on the combination of both externally assessed topical relevance and image quality judgments, offlineine image search metrics can be better correlated with user satisfaction than merely using topical relevance\nWe also demonstrate that existing offline Web search metrics can be adapted to evaluate on a two-dimensional presentation for image search\nWith respect to online metrics, we find that those based on image click information significantly outperform offline metrics.\nTo our knowledge, our work is the first to thoroughly establish the relationship between different measures and user satisfaction in image searc"},
{"doc": "the development of better information retrieval systems is driven by how improvements are measured\nthe design of test collections and evaluation metrics that started with the cranfield paradigm in the early 1960s allowed researchers to analyze the quality of different retrieval models in an automated and cost-effective way\nsince then, many evaluation metrics have been proposed to measure the effectiveness of information retrieval systems [20, 22, 27].\nselecting a suitable set of metrics for a specific task is challenging\ncomparing metrics empirically against user satisfaction or search effectiveness requires data that is often unavailable.\nmoreover, findings may be biased to the subjects, retrieval systems or other experimental factors.\nan alternative consists of modeling theoretically the desirable properties of retrieval systems, as well as the abstraction of the expected users\u2019 behavior when performing a specific task\nfor instance, a metric that looks at how early the relevant document is retrieved in the ranking \u2013 such as reciprocal rank [26] \u2013 would be an appropriate metric to analyze the performance of systems on a single-item navigational task\nhowever, is often challenging to come up with the proper evaluation tools for more complex search scenarios, as is the case of search result diversification [19]\nin this context, the ranking of retrieved documents must be optimized in such a way that diverse query aspects are captured in the first positions\nthe challenge is that the evaluation of system outputs is affected by multiple variables such as\nthe deepness of ranking positions, the amount of documents in the ranking related to the same query aspect, relevance grades, the diversity of query aspects captured by single documents or the user\u2019s effort when inspecting the ranking.\naxiomatic analysis has been shown to be an effective methodology to better understand the foundamentals of evaluation metrics [3, 4, 10, 25]\nin the context of evaluation, axiomatic approaches consist of a verifiable set of formal constraints that reflect which quality factors are captured by metrics, facilitating the metric selection in specific scenarios\nto our knowledge, there is no comprehensive axiomatic analysis of the behavior of diversity metrics in the literature.\nthis paper provides a set of ten formal constraints that focus on both retrieval and diversity quality dimensions.\nwe found that every constraint is satisfied at least by one metric\nhowever, none of the existing diversity metrics satisfy all the proposed constraints simultaneously.\nin order to solve this gap, we define the metric rank-biased utility (rbu) by integrating components from different metrics in order to capture every formal constraints\nrbu is an adaptation of the well-known rank-biased precision metric [16] that incorporates redundancy and the user\u2019s effort associated to the inspection of documents in the ranking\nour experiments using standard diversity test collections validate our axiomatic analysis\nresults show that, satisfying every constraint with a single metric leads to unanimous evaluation decisions when compared against other existing metrics, i.e., rbu captures quality criteria which are reflected by different metrics.\ntherefore, this metric offers a solution in the absence of knowledge about the specific characteristic of a diversity-oriented retrieval scenario\nmoreover, the theoretical framework presented in this paper helps to decide which metric should be used.\nthe paper is organized as follows\nsection 2 describes related work on evaluation of evaluation metrics.\nsection 3 introduces the formal constraints that we propose to analyze relevance and diversity properties of metrics.\nsection 4 provides a comprehensive analysis of existing diversity metrics according to these constraints and section 5 defines the proposed rbu metric.\nsection 6 details the results of our experiments\nfinally, section 7 concludes the wor", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Many evaluation metrics have been defined to evaluate the effectiveness ad-hoc retrieval and search result diversification systems\nHowever, it is often unclear which evaluation metric should be used to analyze the performance of retrieval systems given a specific task\nAxiomatic analysis is an informative mechanism to understand the fundamentals of metrics and their suitability for particular scenarios\nIn this paper, we define a constraint-based axiomatic framework to study the suitability of existing metrics in search result diversification scenarios\nThe analysis informed the definition of Rank-Biased Utility (RBU) \u2013 an adaptation of the well-known Rank-Biased Precision metric \u2013 that takes into account redundancy and the user effort associated to the inspection of documents in the ranking.\nOur experiments over standard diversity evaluation campaigns show that the proposed metric captures quality criteria reflected by different metrics, being suitable in the absence of knowledge about particular features of the scenario under study"},
{"doc": "\u201cit takes me a lot more time to find a useful paper... and it takes me even longer to read it... \u201d while a non-english speaking phd student complained this in a seminar, other phd candidates, in the similar background, agreed with her and they shared the same frustration when they are trying to find and consume the helpful english publications\nprofessor\u2019s (a native speaker) response came later as a relief \u201cwell, i agree, but my problem is even bigger... i cannot read the papers in your language at all...\n this dialog initialized our thinking about this new problem - cross language publication (citation) recommendation, a.k.a. how can we propose a useful method/system to assist scholars to efficiently locate the useful publications written in different languages (a typical scenario of this task is to help non-english speaking students to search for useful english papers)\nincreased academic globalization is forcing a scholar to break the linguistic boundaries, and english (or any other dominant language) may not always serve as the gatekeeper to scientific discourse\nunfortunately, existing academic search engines (e.g. google scholar, microsoft academic search, etc.) along with many sophisticated retrieval and recommendation algorithms [11, 12, 28] cannot cope with this problem efficiently\nfor instance, most of the existing citation recommendation algorithms work in a monolingual context, and the scholarly graph-based random walk may not work well in a multilingual environment (section 4 will prove this)\nmoreover, cross-language citation recommendation (ccr) can be a quite challenging problem comparing with classical scholarly recommendation due to the following reasons\ninformation need shifting\ndifferent from monolingual citation recommendation, we cannot directly calculate the relevance between the papers written in two different languages\na straightforward solution is to utilize machine translation (mt) [1] to translate the query content (e.g., keyword, text or user profile), then, use existing matching models [10, 30] to recommend the proper papers in target language.\nhowever, mt based methods and the ccr task can be fundamentally different\nthe goal of mt is to find a target text given a source text based on the same semantic meaning [1] (e.g., find the papers contain exact or similar matched phrases or sentences), while the ccr task is focusing on recommending \u201crelevant\u201d papers in target language to the given query in the source language [29]\nwhen research context changes, content translation may not perform well\nfor instance, in chinese/japanese research context, machine learning methods can be important for word segmentation studies, which may not be the case for the english counterpart\nmt approach cannot address this kind of information need shifting problem\nsparse inter-repositories citation relations\nbesides textual information, citation relations are quite important for citation recommendation\n in the prior studies, recommendation algorithms can learn the \u201crelevance\u201d by using citation relations on a graph [15, 22]\nhowever, compared to the enormous monolingual citation relations, cross-language citations can be very sparse\n for instance, in a computer science related bilingual (chinese-english) context, we find the papers in acm and wanfang1  on average, have about 28 times more monolingual citation relations than cross-language ones\nit is difficult to effectively employ the citation relations for cross-language citation recommendation by using classical graph mining methods\nheterogeneous information environment.\nintuitively, one could integrate the cross-language content semantics, citation relations and other useful heterogeneous information (e.g., keywords and authors) to address ccr\nhowever, most existing text or graph based ranking algorithms rely on a set of human defined rules (e.g., sequential relation path [14] and meta-path [26]) to integrate different kinds of information\non a complex cross-language scholarly graph, this kind of handcrafting features can be time-consuming, incomplete and biased\n to address these challenges, in this study, we propose a novel solution, hierarchical representation learning on heterogeneous graph (hrlhg), for cross-language citation recommendation\nby constructing a novel cross-language heterogeneous graph with various types of vertexes and relations, we \u201csemantically\u201d enrich the basic citation structure to carry more rich information.\nto avoid the handcrafting feature usage, we propose an innovative algorithm to project a vertex (on the heterogeneous graph) to a low-dimensional joint embedding space\nunlike prior hyperedge or meta-path approaches, the proposed algorithm can generate a set of relation type usefulness distributions (rtud), which enables fully automatic heterogeneous graph navigation\nas figure 1 shows, the hierarchical random walk algorithm enables a two-level random walk guided by two different sets of distributions\nthe global one (relation type usefulness distributions) is designed for graph schema level navigation (task-specific);\nwhile the local one (relation transition distributions) targets on graph instance level walking (task-independent)\nby using hrlhg, we can recommend a list of ranked crosslanguage citations for a given paper/query in the source language\nwe evaluate the proposed algorithm in chinese and english scholarly corpora, i.e., wanfang and acm digital libraries\nthe results demonstrate that the proposed approach is superior than state-ofthe-art models for cross-language citation recommendation task\n the contribution of this paper is fourfold\n first, we propose a novel method (hierarchical representation learning on heterogeneous graph) to characterize both the global and local semantic plus topological information for the publication representations\nsecond, we improve the interpretability of the publication representation model\n by using an iterative em (expectationmaximization) approach, the proposed algorithm can learn the implicit biases for cross-language citation recommendation, which significantly differs from classical heterogeneous graph mining algorithms\nthird, we apply the proposed embedding method for a novel cross-language citation recommendation task\nan experiment on real-world bilingual scientific datasets is employed to validate the proposed approach.\n last but not least, although in this study we focus on cross-language citation recommendation task, the proposed method can be generalized for different tasks that are based on heterogeneous graph embedding learning.", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "While the volume of scholarly publications has increased at a frenetic pace, accessing and consuming the useful candidate papers, in very large digital libraries, is becoming an essential and challenging task for scholars\nUnfortunately, because of language barrier, some scientists (especially the junior ones or graduate students who do not master other languages) cannot efficiently locate the publications hosted in a foreign language repository\nIn this study, we propose a novel solution, cross-language citation recommendation via Hierarchical Representation Learning on Heterogeneous Graph (HRLHG), to address this new problem\nHRLHG can learn a representation function by mapping the publications, from multilingual repositories, to a low-dimensional joint embedding space from various kinds of vertexes and relations on a heterogeneous graph.\n By leveraging both global (task specific) plus local (task independent) information as well as a novel supervised hierarchical random walk algorithm, the proposed method can optimize the publication representations by maximizing the likelihood of locating the important cross-language neighborhoods on the graph\nExperiment results show that the proposed method can not only outperform state-ofthe-art baseline models, but also improve the interpretability of the representation model for cross-language citation recommendation tas"},
{"doc": "to address the information overload issue, recommender systems have been widely deployed in online information systems, such as e-commerce platforms, social media sites, news portals and so on\nan effective recommendation solution not only can increase the traffic and profit for service providers, but also can help customers find the items of interest more easily [6, 7, 34].\nmoving beyond the traditional task of recommending content for individual users, in this work we focus on providing recommendation for a group of users, known as the group recommendation task\nowing to the prevalence of social media [26, 27], online group activities have become very common in current social web\nfor example, a group of traveler can work together to plan a trip on mafengwo , a group of teenagers can organize a social party on meetup , and a group of researchers can discuss a paper on mendeley\nsuch rich sources of information provide us a valuable opportunity to study the online behaviors of groups, which in turn benefit us to develop a better recommendation service for groups\ndistinct to the decision making of an individual user, the decision making process of a group is more complicated, since each member in the group may contribute to the final decision\n more importantly, the process can be rather dynamic.\nfor example, a member may play different roles and exhibit different influence in choosing items of different types due to her specialty.\nnevertheless, existing recommendation solutions largely applied a predefined and fixed strategy to aggregate the preferences of group members, such as average [3, 4], least misery [1], maximum satisfaction [5] and so on.\nas such, these solutions are insufficient to capture the complicated and dynamic process of group decision making, resulting in the suboptimal performance for group recommendation. \nin this work, we approach the fundamental problem in group recommendation \u2014 how to aggregate the preference of group members to decide a group\u2019s choice on items\ninstead of applying a predefined strategy, we propose to learn the aggregation strategy from the historical data of group-item interactions\n the key challenges here are how to design an expressive model to learn the influence of a member, and how to adapt the influence when the group interacts with different items\ninspired by the recent success of representation learning, we approach the challenges from the perspective of group representation learning in the embedding space\nto get the embedding vector that represents a group, we aggregate the embedding of its members in a learnable way\nspecifically, we design a neural attention network to learn the weight of a member, which is capable of assigning different weights for a user when the group interacts with different items\nthrough this way, we can dynamically adjust the aggregation strategy for a group to capture the complicated process of group decision making. \nmoreover, many group-aware social platforms also have abundant data of user-item interactions, the key data source to infer user preference to address the item recommendation task [29]\nintuitively, a group\u2019s decision should be dependent on the preference of its members\n as such, another challenge here is how to effectively leverage the user-item interaction data to improve the performance of group recommendation\nto this end, we propose to address both tasks of group recommendation and item recommendation simultaneously under the same framework\nspecifically, we employ the state-of-the-art neural collaborative filtering (ncf) framework [16] to learn the user-item and group-item interactions in the same embedding space\nthrough this way, we not only improve the performance of both tasks, but also enable recommendation for cold-start users that have no historical individual behaviors by leveraging their group data\nthe main contributions of this work are summarized as follows\nthis is the first group recommender system that leverages neural attention network to learn the aggregation strategy from data in a dynamic way\nuser-item interactions are further integrated to improve the performance of group recommendatio\nas a byproduct, the user cold-start problem in item recommendation can be alleviated\nextensive experiments are performed on a self-constructed dataset and a public dataset to demonstrate our method\nmeanwhile, the datasets and codes are released to facilitate the research communi", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Due to the prevalence of group activities in people\u2019s daily life, recommending content to a group of users becomes an important task in many information systems.\nA fundamental problem in group recommendation is how to aggregate the preferences of group members to infer the decision of a group.\nToward this end, we contribute a novel solution, namely AGREE (short for \u201cAttentive Group REcommEndation\u201d), to address the preference aggregation problem by learning the aggregation strategy from data, which is based on the recent developments of attention network and neural collaborative filtering (NCF)\nSpecifically, we adopt an attention mechanism to adapt the representation of a group, and learn the interaction between groups and items from data under the NCF framework\nMoreover, since many group recommender systems also have abundant interactions of individual users on items, we further integrate the modeling of user-item interactions into our method\nThrough this way, we can reinforce the two tasks of recommending items for both groups and users\nBy experimenting on two real-world datasets, we demonstrate that our AGREE model not only improves the group recommendation performance but also enhances the recommendation for users, especially for cold-start users that have no historical interactions individuall"},
{"doc": "online  streaming  services, such as netflix and spotify , have become popular and accumulated massive user bases.\npremium users have the privileges to enjoy high-quality contents and real-time streaming  events, but these services usually come at a fee.\nbecause of the membership fee of the premium accounts, it is not rare that users share a premium  account to split the cost between themselves.\nhowever, illegal sharing may compromise not only the service provider\u2019s financial interests but also the service quality in general.\nfirst, account sharing implies loss of potential customers who may bring additional revenue to the service provider.\nsecond, current customer profiling and recommendation systems operate under the assumption that each account is used by a single user and hence cannot accurately model individual  user preferences from a mixture of activities by multiple users.\nthis may impair its ability to provide high quality recommendations to users.\nconsequently, unsatisfied users may decide to switch to other providers\nto detect account sharing and enhance the quality of recommender systems in the presence of account sharing, this paper aims to identify individual users behind shared accounts\nthe goal of our work is three-fold: first, given a list of registered accounts, along with the corresponding session logs that record the activities of the accounts, we aim to accurately identify  the set of users behind each account based on its session activities from the set of users who are using this account\nthen we can accordingly predict whether an account is shared by multiple users.\nsecond, given a newly-coming session issued by a certain account, we aim to identify the corressponding user from the identified  users of that account.\nthird, we will enhance the performance of item recommendation by integrating account-level and user-level item recommendation\nthe session log of an account contains lists of entries\neach entry records the item requested and the timestamp of such request.\nwe organize the log of each account into a list of consecutive  sessions.\nin addition, each item may be associated with several metadata attributes (e.g., a song may have genres, artists, albums, and published years)\nit has been shown in the literature that modeling multi-user behaviors in shared accounts [1, 37, 38, 42, 44] and session-based recommendations [19, 36] successfully improve the performance of item recommendation\nhowever, these studies do not attempt to identify individual users.\nto the best of our knowledge, zhang et al. [43] is the first and only attempt to identify users in shared accounts by a specialized subspace clustering, which is employed as a baseline in our experimental studies.\nin addition, the information of metadata is not taken into account in their approach\nsince we do not know the accounts that may be shared by multiple users and the users that share the same account, we propose an unsupervised learning-based framework, session-based heterogeneous graph embedding for user identification (she-ui).\nthe main idea is to model the preference of individual users via a novel technique of session embedding that learns a unique feature representation for each session.\nwe first create a heterogeneous  information network to represent the relationships among items and their meta information\nthen, by applying a specialized random walk mechanism, the feature representation of each node can be derived using the skip-gram learning architecture [25, 27].\nsubsequently the item-based session embedding  is learned through the node embedding.\nfor each account, the user identification problem is then mapped to the problem of session clustering.\n we develop a clustering algorithm based on affinity propagation [15] to simultaneously determine the number of clusters and group sessions into clusters.\neach cluster represents the sessions issued by the same user, and the number of clusters represents the number of users sharing this account\nfor any incoming  session of this account, we may find the potential user who issues the session by computing its representation (from the first few items in the session) in the space of session embedding and finding its nearest cluster.\nlast, to boost the performance of recommender systems, we propose a hybrid recommender, termed aurec, which combines conventional account-level and user-level (derived by she-ui) item recommendation\nwe summarize the contributions of this work in the following\n we propose to deal with two research tasks: (1) identifying users behind a shared account based on historical sessions and meta- data of the items, and (2) given a new session initiated by a multi-user account, identifying which user issued this session.\nthe former can benefit the service provider to detect multi-user accounts so that new pricing strategies can be established, while the latter boosts the performance of item recommendation.\nit is also worth mentioning that no prior knowledge about the mappings between users and accounts are given here\nwe develop an unsupervised framework she-ui that cannot only identify users in shared accounts, but also learn the preferences of individual users.\nthrough a novel session embedding technique, she-ui effectively learns feature representations from a heterogeneous graph that represents the relationships between items and their metadata\nexperiments  conducted  on two large-scale datasets of online streaming services, last.fm and kkbox, demonstrate that she-ui clearly outperforms existing item-based and embedding-based methods on both tasks of user identification.\na study of parameter sensitivity  also manifests the robustness of she-ui\nbased on the identified  users behind accounts, we devise a hybrid recommender aurec that combines account-level and user-level item recommendation.\nexperiments on kkbox data show that aurec is able to significantly outperform the state-of-the-art account-level recommendation methods by 39% in terms of precision\nthe remainder of this paper is organized as follows. \nafter presenting the relevant work in section 2, we present the problem statement in section 3.\nthe proposed methodology is described in section 4.\nwe show the experimental results in section 5, and conclude this work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Online streaming services are prevalent.\nMajor service providers, such as Netflix (for movies) and Spotify (for music), usually have a large customer  base.\nMore  often than not, users may share an account.\nThis has attracted increasing attention recently, as account sharing not only compromises the service provider\u2019s financial interests but also impairs the performance of recommendation systems and consequently the quality of service provided to the users\nTo address this issue, this paper focuses on the problem of user identification in shared accounts\nOur goal is three-fold: (1) Given an account, along with its historical  session logs, we identify a set of users who share such account; (2) Given a new session issued by an account, we find the corresponding user among the identified users of such account; (3) We aim to boost the performance of item recommendation by user identification\nWhile the mapping between users and accounts is unknown, we propose an unsupervised learning-based framework, Session-based Heterogeneous graph  Embedding for User Identification (SHE-UI), to differentiate and model the preferences of users in an account, and to group sessions by these users\nIn SHE-UI, a heterogeneous graph is constructed to represent items such as songs and their available metadata such as artists, genres, and albums\nOur experiments conducted on two large-scale music streaming datasets, Last.fm and KKBOX, show that SHE-UI not only accurately identifies users, but also significantly  improves the performance of item recommendation over the state-of-the-art method"},
{"doc": "email has been an important web-based communication medium for more than 25 years\nwhile email was first designed for asynchronous communication people have \"overloaded\" the use of email with other functions such as task management and personal archiving [22, 35]\nas online services and the web grow, email not only continues to serve all of these purposes [25] but an ever-increasing number, e.g. as a receipt file cabinet for e-commerce purchases, as a standard part of identity/authentication flow, and as calendar management\nwith regard to the last, because meeting arrangement and time negotiation often happen through email, nearly every modern email service \u2013 both web email and client applications \u2013 offer fully featured calendar management\ndespite this integration, the majority of feature development has focused on moving information from email into a user\u2019s calendar but little work has focused on the implications of calendar information for improving other functionality of an email service\nin this paper, we focus on one such problem and ask whether the contextual information offered in calendar items can be used for more than simply reminding people of upcoming appointments\nin particular, calendar items are a strong indicator of a person\u2019s future intent and identify a time for the appointment (informing a likely horizon of need) as well as often including the people involved, a subject, and a description\nthis forms a rich contextual basis to explore proactively finding and recommending emails the user will likely need for an upcoming calendar appointment\nwe study a setting particularly focused on meetings and associated information finding for meeting preparation in an enterprise email setting\nproactive email recommendation can both improve email search efficiency and reduce the difficulty of query specification by enabling \"search by meeting\".\nthe ideal experience is that the system does not wait for users to issue the query, but forms and issues the query for the user based on what we know about the user\u2019s current and future calendar appointments, (i.e., zero-query search experience)\nwe envision this could be activated by simply clicking on an appointment or activated by default (predicting both calendar item in focus and associated information)\nin this paper we study the setting where a user chooses the focus calendar item\nto understand the interaction of key factors, we construct an artifact named capers: a calendar-aware proactive email recommender system\ncapers specifically focuses on meetings based on the observation that in work environments, people heavily rely on both emails and calendars to organize their daily activities and necessary information \u2013 an intuition that we first investigate using a survey\nwhen the system has access to users\u2019 calendars, it gains a rich context including the time, duration and the content of a person\u2019s upcoming information needs\ncapers targets at learning the usefulness of emails with respect to the tasks of preparing for or having meetings\n usefulness is a stricter criteria beyond content relevance (and also preference), because emails can be highly topically relevant to a meeting but have no utility for preparation for the meeting itself (e.g., sent time and people involved need to considered as well)\ncapers bootstraps the predictive models through pilot studies and further updates the models through online learning algorithms\nin this paper, we aim to answer the following five research questions:\nrq1: how much email access is related to meetings\nrq2: what factors affect whether people prepare for a meeting\nrq3: what factors affect the usefulness of emails to a meeting and how\nrq4: how do machine-learning-based recommendation models and their online learning algorithms affect user experience in this system-level cold-start scenario\nrq5: what is the role of email recency in email recommendation (given prior work has shown that recency is an specially important factor for email search [10])\nto address the above research questions, we employed mixed methods including a survey for rq1-2 and a task-based and field experiment using the capers prototype for rq3-5\nin the following, we first show through a survey that a large proportion of email access is related to meetings\nthen, we examine how group size and the organizational relationship between meeting participants impacts the likelihood of preparing for a meeting\nnext, we focus on learning recommendation models that leverage four categories of factors, including: amount of email content (e.g. length, number of attachments, etc.), email recency, calendar-email content match and calendar-email people match\nour aim is to both understand the modeling choices in this domain and understand the impact of each of these factors on predicting email usefulness for meeting preparation\nfinally, through both a short task-based explicit judgment experiment and a longitudinal (one-month) field experiment with a small set of users, we study how people perceive the usefulness of the results returned by the different types of models and the impact on engagement with the recommendation model", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "In this paper, we study how to leverage calendar information to help with email re-finding using a zero-query prototype, CalendarAware Proactive Email Recommender System (CAPERS)\nCAPERS proactively selects and displays potentially useful emails to users based on their upcoming calendar events with a particular focus on meeting preparation\nWe approach this problem domain through a survey, a task-based experiment, and a field experiment comparing multiple email recommenders in a large technology company.\n We first show that a large proportion of email access is related to meetings and then study the effects of four email recommenders on user perception and engagement taking into account four categories of factors\nthe amount of email content, email recency, calendar-email content match, and calendar-email people match.\n We demonstrate that these factors all positively predict the usefulness of emails to meeting preparation and that calendar-email content match is the most important.\nWe study the effects of different machine learning models for predicting usefulness and find that an online-learned linear model doubles user engagement compared with the baselines, which suggests the benefit of continuous online learnin"},
{"doc": "relevance assessment is an integral part of information retrieval (ir) evaluation, since the cranfield experiments, through the trec and trec-like initiatives\nin the recent years the collection of relevance judgments is being studied using crowdsourcing.\n to gather relevance labels, several scales have been used in the past\n the most common are the classical binary scale, or ordered scales with a limited number of categories (usually ranging from 3 to 7)\nit has recently been proposed [15, 21] to use magnitude estimation (me) to gather relevance assessments on a ]0, +\u221e[ scale that has the following advantages\n(1) it is more fine-grained than the above alternatives (and thus, at least potentially, allowing to capture relevance differences that would otherwise be lost)\n(2) it is able of always providing to the assessor a smaller or higher relevance value, and even a value in between other two, always allowing to assign to a new document a relevance value unforeseen in advance\n this happens in particular at the extremes of the scale, but also for the values internal to the range; an\n(3) it can adapt to different assessors\u2019 preferences (e.g., those who prefer to use a binary scale can do that and those who prefer to judge in a scale from 1 to 10 can also do that)\nme is not free from disadvantages, though\nit requires a normalization of the collected scores since each assessor is free to use a different \u201cinternal\u201d relevance scale\nit does not allow for a direct comparison of scores provided by different judges and/or on different topics as the score normalization is typically performed on a topic-by-topic basis\nit is somehow unnatural, or at least it requires some adaptation for the human assessor as compared to most common rating scales which are bounded\nand it leads to a log-normal distribution of relevance scores\nin this paper we discuss and experimentally evaluate by means of large-scale crowdsourced relevance judgments the use a finegrained scale on 100 levels (s100)\nusing the proposed 100 levels scale, the human assessor judges the relevance of a document with respect to a query by means of a number in the [0..100] range (extremes included, thus the levels are actually 101; we name it s100 anyway)\nsuch a scale can be seen as a sort of compromise between the classical a-few-categories relevance scales and me\nwe run a large scale crowdsourcing experiment to collect more than 50 thousand labels on such a scale, we discuss its advantages and disadvantages with respect to the already proposed alternatives, and we experimentally compare our judgments with judgments on coarse-grained scales (i.e., binary and 4-levels) and with judgments using me\nmore specifically, our research questions are\ncan relevance values be collected in a reliable way using a 100 levels scale in a crowdsourcing setting\nhow do crowd workers choose to use the proposed scale\nare the collected labels consistent with standard ground truths\nwhat are the differences between s100 and me\nwhat are the effects of the relevance scale on ir system evaluation/ranking\nwhich scale is more robust to decreasing the number of judgments per topic/document pair\nwhat happens when collecting fewer judgments per assessor\nme requires some learning to be used effectively by crowd workers as it is not like rating scales they are already used to\nwhat happens to judgment quality when the number of documents judged by eachworker in each hit decreases\nme allows to go beyond the maximum and minimum judgment level previously used, and to always find a judgment in between two previously expressed judgments\nare these properties required and useful in practice when using s100\nare s100 and me different w.r.t. the time needed to express judgments\ndoes me require more adaptation time when used for the first time (i.e., on the first documents)\nour main findings are\nw.r.t me, s100 gives assessors a reference point by providing upper and lower scale boundaries (see section 4)\ns100 is more robust than me to both fewer assessors per document and fewer documents per assessor (see section 5)\nthe theoretical problem of running out of values (at the extremes of the scale) does not occur often in practice, at least in our setting. of course, with more document to judge for each worker, the problem might manifest (see section 6)\nif a fine-grained scale is preferred, using me in a crowdsourcing setting can provide results faster while s100 enables direct comparison over topics and workers and does not require normalization (see section 7)\nwhile me shows a steeper learning curve with more time needed to judge the first few documents, it becomes faster for crowd workers to judge with me compared to s100 in the long term\nconsidering that crowd work is long-tail distributed with most workers completing very few hits, s100 may be a more efficient strategy for crowdsourced relevance judgments (see section 7)\nthis paper is structured as follows.\nsection 2 surveys related work in the area of relevance scales and crowdsourced relevance judgments\nsection 3 presents an analysis of the relevance judgments we collected on the newly proposed s100 scale\nin section 4 we compare s100 with other commonly used bounded  scales with 2 and 4 levels and with the me unbounded scale\nsection 5 compares the robustness of s100 and me to having few judgments per document and few documents per assessor\nsection 6 presents an analysis looking at how s100 may lead to assessors running out of values as compared to me which enables higher section 7 compares s100 and me in terms of the time required for assessors to express their judgments\nsection 8 summarizes our main findings and the main benefits and drawbacks of the newly proposed s100 scale as compared to commonly used relevance scale", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "In Information Retrieval evaluation, the classical approach of adopting binary relevance judgments has been replaced by multi-level relevance judgments and by gain-based metrics leveraging such multi-level judgment scales\nRecent work has also proposed and evaluated unbounded relevance scales by means of Magnitude Estimation (ME) and compared them with multi-level scales\n While ME brings advantages like the ability for assessors to always judge the next document as having higher or lower relevance than any of the documents they have judged so far, it also comes with some drawbacks\nFor example, it is not a natural approach for human assessors to judge items as they are used to do on the Web (e.g., 5-star rating)\nIn this work, we propose and experimentally evaluate a bounded and fine-grained relevance scale having many of the advantages and dealing with some of the issues of ME\n We collect relevance judgments over a 100-level relevance scale (S100) by means of a large-scale crowdsourcing experiment and compare the results with other relevance scales (binary, 4-level, and ME) showing the benefit of fine-grained scales over both coarse-grained and unbounded scales as well as highlighting some new results on ME\nOur results showthat S100 maintains the flexibility of unbounded scales like ME in providing assessors with ample choice when judging document relevance (i.e., assessors can fit relevance judgments in between of previously given judgments)\n It also allows assessors to judge on a more familiar scale (e.g., on 10 levels) and to perform efficiently since the very first judging tas"},
{"doc": "with the increasing usage of social media platforms and online reporting channels, information is produced and disseminated online faster and in larger volumes than ever before\nas a result, users expect to have easy access to up-to-date information about topics of interest, resulting in a large number of new real-time information-seeking scenarios\nthese scenarios require solutions that can identify relevant (topical), non-redundant (avoids repeated information), and timely (up-to-date) content from noisy highvolume text streams\n a common class of solutions that require these characteristics are event timeline/real-time summary generation systems\nsuch systems take as input a topic of interest and a large volume of textual items (e.g. news articles or tweets), most of which are non-relevant and/or redundant, and select a subset of those items to be emitted over time into a timeline or an updating summary [12, 19, 26]\nan example extract from the output of such systems is shown in table 1.\nthis work is concerned with how to effectively and efficiently evaluate the quality of timeline items produced by such systems\nover the last few years new methodologies to evaluate the quality of timelines have been proposed [4, 17].\nthese methodologies typically use human annotators to manually identify atomic units of information that form a ground truth representing the information a \u2018good quality\u2019 summary about a topic should contain (see figure 1)\n finally, the pooled text items are manually checked to see what atomic information units for the topic they cover (if any), forming \"text item,information unit\" pairs\nmetrics such as expected latency gain [12] use the resultant pairs to estimate the degree to which individual text items included in a timeline (and hence the timeline as a whole) contains relevant, non-redundant, and timely information\nthe use of atomic information units as a ground truth for evaluating timelines/real-time summaries is generally accepted and has been successfully deployed within the temporal summarization and real-time summarization tracks at the text retrieval conference (trec) [4, 17].\nhowever, while these tracks produced test collections that can in theory be used to evaluate any timeline generation system, prior works have reported cases where these test collections fail to accurately estimate the performance of new timeline generation systems [19, 20].\nin particular, it was observed in these past works that the overlap between items included in the initial pools (i.e. the assessed set) and those returned by their new proposed systems was insufficient to facilitate a robust comparison of systems.\nas a result, it is unclear to what extent the test collections produced during these tracks can be used to evaluate the quality of new systems that were not included in the initial pooling [5]\n in this paper, we investigate to what extent current atomic information unit-based test collections are able to distinguish between timeline summarization systems with different effectiveness levels, as well as propose and evaluate automatic solutions to reduce the likelihood of errors occurring when evaluating such systems.\ncontributions\nthe main contribution of our work is an in-depth analysis of the trec 2013-2015 temporal summarization track test collections that quantifies how robust these collections are when evaluating unpooled systems, as well as an effectiveness evaluation of different automatic \"text item,information unit\" expansion techniques aimed at increasing the robustness of these test collections\nspecifically, we tackle two main research questions\nrq1: to what extent can the trec temporal summarization track test collections accurately rank unpooled systems\nrq2: if we use automatic methods to generate additional \"text item,information unit\" pairs can we reduce the likelihood of new systems being miss-ranked\nour results show that the trec 2013-2015 temporal summarization track test collections do not accurately estimate the effectiveness of unpooled systems.\nmoreover, the discrepancies observed between actual and estimated performances are sufficient to cause errors when ranking those systems\nfurthermore, we found that the likelihood of encountering ranking errors is not uniform across system effectiveness levels \u2013 the better a system is, the more negatively it is impacted by not being pooled\nfor this reason, we conclude that it is potentially risky to use the trec-ts test collections outof- the-box\nwe then experiment with two types of automatic \"text item,information unit\" expansion techniques aimed at reducing these discrepancies, namely: item-item similarity expansion and item-item semantic expansion\nour experiments using these two expansion techniques show that automatically adding even a small number of \"text item,information unit\" pairs can markedly reduce the number of ranking errors observed when using the text collections\nin particular, we found that item-item similarity expansion can reduce the number of ranking errors by up to 30% while item-item semantic expansion can reduce the number of ranking errors by up to 50%\nwe conclude that these expansion techniques improve the robustness of the trec-ts test collections, reducing the risk of miss-ranking new systems that were not poole", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The development of automatic systems that can produce timeline summaries by filtering high-volume streams of text documents, retaining only those that are relevant to a particular information need (e.g. topic or event), remains a very challenging task\n To advance the field of automatic timeline generation, robust and reproducible evaluation methodologies are needed\n To this end, several evaluation metrics and labeling methodologies have recently been developed - focusing on information nugget or cluster-based ground truth representations, respectively\n These methodologies rely on human assessors manually mapping timeline items (e.g. tweets) to an explicit representation of what information a \u2018good\u2019 summary should contain\nHowever, while these evaluation methodologies produce reusable ground truth labels, prior works have reported cases where such labels fail to accurately estimate the performance of new timeline generation systems due to label incompleteness.\n In this paper, we first quantify the extent to which timeline summary ground truth labels fail to generalize to new summarization systems, then we propose and evaluate new automatic solutions to this issue\nIn particular, using a depooling methodology over 21 systems and across three high-volume datasets, we quantify the degree of system ranking error caused by excluding those systems when labeling\nWe show that when considering lower-effectiveness systems, the test collections are robust (the likelihood of systems being miss-ranked is low)\nHowever, we show that the risk of systems being miss-ranked increases as the effectiveness of systems held-out from the pool increases\n To reduce the risk of miss-ranking systems, we also propose two different automatic ground truth label expansion techniques\nOur results show that our proposed expansion techniques can be effective for increasing the robustness of the TREC-TS test collections, markedly reducing the number of miss-rankings by up to 50% on average among the scenarios teste"},
{"doc": "much research on information retrieval (ir) investigates alternative methods to better evaluate systems\nsome of these seek a higher correlation between offline system measures and online user measures, more power to discriminative between systems, or reduction of judgment pool incompleteness.\nin this type of research we typically ask \u201cwhat if\" questions, like what if we use non-expert relevance assessors bailey et al. [5], what if we use a different parameterization of dcg [20], or what if we evaluate on a different document collection [24]\none class of such problems is concerned with the reliability of our evaluation experiments and its trade-off with efficiency [29], that is, the extent to which evaluation results can be replicated, maybe under budget restrictions\ntypical questions in this class are what if we change the topic set [37], what if we use p@10 instead of ap [8], what if we use the wilcoxon test instead of the t -test [32], or how many topics will we need to achieve a certain level of confidence [12]\nunfortunately, research questions of this nature pose fundamental problems that make it impossible to find a direct answer or, at the very least, limit our ability to do so\nfinite data.\n evaluation research is often of empirical nature and uses existing data from archives like trec\nhowever, these data are limited to dozens of systems and topics for a given task, so the precision and generalizability of our results are severely constrained\nwe usually overcome this limitation by resampling the existing data, as a way to simulate new topic sets\nunknown truth\nin many cases the researcher needs to know some underlying property of the systems, such as their true mean score and variance over the possibly infinite population of topics, which are of course impossible to know.\nthe workaround usually consists in making random splits of the topic set, considering one as representing the population of topics and the other one as a random sample from it\nhowever, and because of the limited amount of data, these splits are not really independent samples\nlack of control\nvery often we want to study systems of predefined characteristics, such as systems with the same mean or variance, or with a certain degree of dependence\nbut we can not control these properties\nthe systems and topics in the existing data are what they are; we can not change them\nsometimes we work around this limitation with artificial modifications of the effectiveness scores, but they result in unrealistic data (eg. shifting scores by some quantity)\nconsider for instance the problem of choosing an appropriate number of topics for a new test collection [30]\nthe ir literature contains data-based approaches that repeatedly split the topic sets to calculate some statistic like kendall \\\\tau  [8, 25, 40]\nextrapolating to larger topic sets, we find empirical results on the reliability of various topic set sizes\nhowever, these studies are clearly limited by the small data sources, they calculate statistics between two samples of topics as opposed to between a sample and a population, and in the end we do not have full knowledge of the true system distributions anyway, so we can not really assess whether the extrapolation is accurate or not\nthere are also statistical approaches that make various assumptions like normal distributions or homoskedasticity, which clearly do not hold in ir evaluation data [12, 23, 31]\nstill, the extent to which these assumptions pose a practical problem, remains largely unknown because we can not control these properties of the data\nthese limitations, however unfortunate, are present in our everyday research\n as a consequence, one may wonder the extent to which past and current results really hold in practice\nit is our firm believe that the ir community should seek experimentation methods that help us remove these barriers and allow us to study this kind of questions directly and under desired conditions\nfor this, we propose the use of stochastic simulation for the generation of evaluation data to serve as input in evaluation research\nthe idea of using simulation in ir research is of course not new\nin the early days simulation was attempted to build collections, and in particular simulate judgments [14, 28].\nmore recent work has been devoted for instance to simulating queries [3], document scores [22] and various aspects of user interaction [4]\nfor problems pertaining to reliability we find few cases, such as [9, 10] and [30], which do simulate effectiveness scores\nhowever, simulations therein are rather limited.\nin the former the scores are drawn from beta and uniform distributions without adjusting parameters and correlations based on existing data\nthe latter work does this to some degree, but the resulting simulations are still unrealistic in terms of support (eg. they are only continuous) and still do not allow us to have full control over system properties\nbuilding upon these works, in this paper we propose a method for the stochastic simulation of effectiveness scores that effectively avoids the three limitations discussed above\nthe general idea is to build a model for the joint distribution of system scores given by some effectiveness measure, such that we can simulate endlessly from it\nit is important to note that we do not aim at creating a model of the systems that generated the given data, but rather a realistic model of a set of systems similar to those\n for this model to be realistic, we implement several alternatives to model the marginal system distributions in a way that the underlying properties of the measure are preserved (eg. the support), as well as several alternatives to model the full dependence structure among systems\nby fully specifying the effectiveness distributions, the model is also useful because we have complete knowledge of properties of the data such as the expected mean and variance\nfurthermore, by separating the modeling of margins from the modeling of dependencies, we have a high level of customization that allows us to control aspects such as the levels of homoskedasticity and correlation\na full implementation of the proposed method is open-sourced as an r package, available at https://github.com/julian-urbano/simireff\nthe results of the paper can be fully reproduced with data and code available at https://github.com/julian-urbano/sigir2018-simulation\nsections 2 and 3 discuss how to model system dependencies and score distributions\nsection 4 evaluates the proposed method, and section 5 presents two application uses cases replicating past experiments\nsection 6 presents the conclusions and future wor", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Part of Information Retrieval evaluation research is limited by the fact that we do not know the distributions of system effectiveness over the populations of topics and, by extension, their true mean scores\nThe workaround usually consists in resampling topics from an existing collection and approximating the statistics of interest with the observations made between random subsamples, as if one represented the population and the other a random sample\nHowever, this methodology is clearly limited by the availability of data, the impossibility to control the properties of these data, and the fact that we do not really measure what we intend to\nTo overcome these limitations, we propose a method based on vine copulas for stochastic simulation of evaluation results where the true system distributions are known upfront\nIn the basic use case, it takes the scores from an existing collection to build a semi-parametric model representing the set of systems and the population of topics, which can then be used to make realistic simulations of the scores by the same systems but on random new topics.\nOur ability to simulate this kind of data not only eliminates the current limitations, but also offers new opportunities for research.\nAs an example, we show the benefits of this approach in two sample applications replicating typical experiments found in the literature\nWe provide a full R package to simulate new data following the proposed method, which can also be used to fully reproduce the results in this pape"},
{"doc": "implicit feedback such as user clicks has long been an important signal for measuring the effectiveness of search engines.\nsuch signals have advantages over more traditional explicit feedback such as relevance judgments in that they are available at low cost for engines running at scale; they are collected in a natural setting; they reflect a more personal notion of relevance than typically used in relevance judging\nthus they have become widely-used metrics for online a/b testing and interleaving experiments\nthere is the risk of user attrition if results being deployed are worse than users expect\nin addition, the time it takes to get sufficient feedback may be prohibitive\u2014in large-scale web search engines it may not be a problem, but in settings where traffic is much lower or in which there are daily or weekly periodicity effects, it could take several weeks to obtain enough of the implicit feedback to be confident in a decision\nfinally, users\u2019 behavior may be biased in unpredictable and hard-to-control ways\nthus there has been recent interest in using historical log data for offline learning and evaluation.\nthis has its own challenges\n in particular, due to strong biases for users to click top-ranked results and for the engine itself to influence user clicks, a click can rarely be interpreted sans context\nfurthermore, when new approaches to ranking result in entirely new documents being retrieved, historical log data has limited use in evaluation\nsome of the most recent work on this topic has investigated it from the perspective of counterfactual analysis, which asks the question \u201cwhat would users have done had they seen this alternative ranking?\nfor example, one recent work by joachims et al. [15] uses inverse propensity weighting to train rankers from historical log data\nthe method requires some degree of randomization of results to remove position bias\nwhile this randomization carries the same risk of user attrition mentioned above, the method joachims et al. present is meant to minimize the amount of perturbation necessary so that most users will see most results as intended\nin this paper we build on joachims\u2019 et al.\u2019s work to extend it to evaluating new rankers that can retrieve never-before-seen documents\nsince these rankers may retrieve more (or fewer) relevant documents than the production ranker, we must assume that they are capable of biasing user behavior just as display position is\n and since they may retrieve entirely new documents, we need a way to collect some incremental feedback on those documents without perturbing the production ranker results too much so that we are able to accurately compare rankers\u2019 effectiveness\nour proposed methodology requires a set of alternate rankers along with a production ranker to curate a reusuable offline dataset\nthe curated dataset consists of a set of logs with user feedback along with propensity estimates to debias the logs\nin spirit, the idea is similar to how trec creates a resuable test collection using a pool of rankers [30]\nthe primary contribution of this work are these methods to curate an offline dataset, which allow comparisons of new rankers to each other and to the production ranker\nas a secondary contribution, we introduce a full simulation environment for rigorous testing of offline evaluation methods that use historical log data\nour simulations allow the generation of runs of varying effectiveness and user models that can produce very noisy click data\u2014as real users do\n we demonstrate the effectiveness of our methods even in this adversarial simulation environment\nthis paper is organized as follows\nin section 2 we discuss related work on online evaluation and counterfactual analysis\nin section 3 we describe our propensity weight-based method\nin section 4 we present our simulation models, and section 5 presents experimental results and analysis\nwe conclude in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We investigate the use of logged user interaction data\u2014queries and clicks\u2014for offline evaluation of new search systems in the context of counterfactual analysis.\nThe challenge of evaluating a new ranker against log data collected from a static production ranker is that new rankers may retrieve documents that have never been seen in the logs before, and thus lack any logged feedback from users\nAdditionally, the ranker itself could bias user actions such that even documents that have been seen in the logs would have exhibited different interaction patterns had they been retrieved and ranked by the new ranker\nWe present a methodology for incrementally logging interactions on previously-unseen documents for use in computation of an unbiased estimator of a new ranker\u2019s effectiveness\nWe demonstrate how well our methods work in a simulation environment designed to be challenging for such methods to argue that they are likely to work in a wide variety of scenario"},
{"doc": "the bipartite network is a ubiquitous data structure to model the relationship between two types of entities\nit has been widely used in many applications such as recommender systems, search engines, question answering systems and so on\n for example, in search engines, queries and webpages form a bipartite network, where the edges can indicate users\u2019 click behaviors that provide valuable relevance signal [1, 2]; in another application of recommender systems, users and items form a bipartite network, where the edges can encode users\u2019 rating behaviors that contain rich collaborative filtering patterns [3]\nto perform predictive analytics on network data, it is crucial to first obtain the representations (i.e., feature vectors) for vertices\ntraditional vector space methods such as the bag-of-words representations capture too few semantics and are inefficient to deal with large-scale dynamic networks in practical applications\nrecent advances in data mining and information retrieval have focused on learning representations from data [4\u20137]\nin particular, they embed vertices into a low dimensional space, i.e., representing a vertex as a learnable embedding vector\nbased on the vertex embeddings, standard machine learning techniques can be applied to address various predictive tasks such as vertex labeling, link prediction, clustering and so on\nto date, existing works have primarily focused on embedding homogeneous networks where vertices are of the same type [4, 8\u2013 10]\nfollowing the pioneering work of deepwalk [8], these methods typically apply a two-step solution\nfirst performing random walks on the network to obtain a \u201ccorpus\u201d of vertices, and then applying word embedding methods such as word2vec [11] to obtain the embeddings for vertices\ndespite effectiveness and prevalence, we argue that these methods can be suboptimal for embedding bipartite networks due to two primary reasons\n(1) the type information of vertices is not considered.\ndistinct from homogeneous networks, there are two types of vertices in a bipartite network\nalthough edges exist between vertices of different types only, there are essentially implicit relations between vertices of the same type\nfor example, in the useritem bipartite network built for recommendation, there exists an implicit relation between users which can indicate their preference in consuming the same item; and importantly, it is recently reported that modeling such implicit relations can improve the recommendation performance [12]\nhowever, existing network embedding methods modeled the explicit relation (i.e., observed edges) only and ignored the underlying implicit relations\nwhile the corpus generated by random walks may capture such high-order implicit relations to a certain extent, we argue that a more effective way is to encode such implicit relations into representation learning in an explicit manner\n(2) the generated corpus may not preserve the characteristics of a bipartite network\n to demonstrate this point, we plot the frequency distribution of vertices in a real youtube dataset in figure 1(a)\nwe can see that the vertices exhibit a standard power-law distribution with a slope of \u22121.582.\nby contrast, we plot the frequency distribution of vertices in a corpus generated by deepwalk in figure 1(b)\nwe find that the generated distribution differs significantly from the real distribution, and it cannot be well described by a power-law distribution\nwe point out that the failure of deepwalk is due to the improper design of the random walk generator, which is suboptimal for embedding bipartite networks\nspecifically, it generates the same number of random walks starting from each vertex and constrains the length of walks to be the same; this limits the capability of the generator and makes it difficult to generate a corpus following a power-law distribution \u2014 which is a common characteristics of many real-world bipartite networks [13]\nto our knowledge, none of the existing works has paid special attention to embed bipartite networks\nwhile a recent work by dong et al. [14] proposed metapath2vec++ for embedding heterogeneous networks which can also be applied to bipartite networks, we argue that a key limitation is that it treats the explicit and implicit relations as contributing equally to the learning\nin real-world bipartite networks, the explicit and implicit relations typically carry different semantics\nas such, they should be treated differently and assigned to varying weights in learning the vertex embeddings\nthis can be evidenced by existing recommendation works [15] that usually assign varying weights on different sources of information to allow a flexible tuning on the learning process\nin this work, we focus on the problem of learning vertex representations for bipartite networks\nwe propose bine (short for bipartite network embedding), which addresses the aforementioned limitations of existing network embedding methods\nbelow we highlight two characteristics of our bine method\n(1) to account for both the explicit relations and implicit relations, we propose a joint optimization framework.\nfor each relation, we design a dedicated objective function; by sharing the vertex embeddings, the objective functions for different relations reinforce each other and lead to better vertex embeddings\nspecifically, the modeling of the explicit relations aims to reconstruct the bipartite network by focusing on observed links\nfor the modeling of implicit relations, we aim to capture the high-order correlations in the bipartite network\nto avoid the explosive growth of complexity in expanding a network, we similarly resort to performing random walks and design the objective function based on the generated corpora\n(2) to retain the properties of the bipartite network as many as possible, we propose a biased and self-adaptive random walk generator\nspecifically, we set the number of random walks starting from each vertex based on its importance, making the vertex distribution in the generated corpus more consistent with the original bipartite network\nmoreover, instead of setting a uniform length for all random walks, we allow a walk to be stopped in a probabilistic way\nthrough this way, we can generate vertex sequences of varying lengths, which is more analogous to the sentences in natural language\nour empirical study shows that our generator can generate corpus more close to the distribution of the real-world networks\nthe remainder of the paper is organized as follows\n we first review related work in section 2\nwe formulate the problem in section 3, before delving into details of the proposed method in section 4\nwe perform extensive empirical studies in section 5 and conclude the paper in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "This work develops a representation learning method for bipartite networks\nWhile existing works have developed various embedding methods for network data, they have primarily focused on homogeneous networks in general and overlooked the special properties of bipartite networks\nAs such, these methods can be suboptimal for embedding bipartite networks\nIn this paper, we propose a new method named BiNE, short for Bipartite Network Embedding, to learn the vertex representations for bipartite networks\n By performing biased random walks purposefully, we generate vertex sequences that can well preserve the long-tail distribution of vertices in the original bipartite network\nWe then propose a novel optimization framework by accounting for both the explicit relations (i.e., observed links) and implicit relations (i.e., unobserved but transitive links) in learning the vertex representations\nWe conduct extensive experiments on several real datasets covering the tasks of link prediction (classification), recommendation (personalized ranking), and visualization\nBoth quantitative results and qualitative analysis verify the effectiveness and rationality of our BiNE metho"},
{"doc": "convolutional neural networks (cnn) have convincingly been regarded as a powerful class of models for learning visual representation, which are generically workable across different tasks and visual domains [38].\nto date in the literature, there are various datasets (e.g., imagenet [22]) that include expert labeled training data available for developing cnn from alexnet [10] to more recent resnet [4] and its variants [35]\nnevertheless, given a new dataset, the typical solution is still to perform \u201cintensive manual labeling\" and then fine-tune the pre-learnt cnn with the examples collected for the new dataset\nmany previous experiences have also shown that by doing so can generate satisfactory performance, though the learnt cnn may not be that transferable when being applied in another dataset due to a phenomenon known as \u201cdomain shift.\nas a result, there have been several domain adaptation techniques being proposed for alleviating this challenge by learning deep transformations to map both domains into a common representation distribution\na general practice to optimize the mappings is to minimize the measure of domain shift such as correlation distances [25] or maximum mean discrepancy [28]\nthe inspiration of recent attempts on domain adaptation are from the advances of adversarial learning, which is to model domain distribution via an adversarial objective with respect to a domain discriminator\nthe spirit behind is from generative adversarial learning [6], that trains two models, i.e., a generative model and a discriminative model, by pitting them against each other.\nthis process corresponds to a minimax two-player game, in which a generative model is to capture the data distribution and a discriminative model aims to estimate the probability that a sample is from the real training data rather than the generative model\nthe two models are trained simultaneously and the learning of the generative model is to fool the discriminative model into making mistakes.\nin the context of domain adaptation, this adversarial principle is then equivalent to guiding the representation learning in both domains making the difference between source and target representation distributions indistinguishable through the domain discriminator\nwe follow this elegant recipe and capitalize on adversarial learning for domain adaptation\n furthermore, with the tremendous increase of multimedia data on the web, the need to search for millions of visual examples in a high-dimensional feature space motivates the surge of research in large-scale visual search.\nin between, hashing techniques [32] are probably the \u201chottest\" topic that produces a compact binary representation for each example and measures visual similarity by computing the hamming distance between each two hash codes, leading to a very efficient search.\ntherefore, we are particularly investigating the problem of domain adaptation on binary representation learning in this work\nby consolidating the idea of adversarial learning into domain adaptation for enhancing binary representation generation, we present a novel deep domain adaptation hashing with adversarial learning (dedaha) architecture, as shown in figure 1.\nspecifically, we form a set of image (video frame) triplets as the input and each tuple contains one query image, one semantically similar image and one dissimilar image.\n a cnn is employed to produce visual representations of each image (video frame), followed by an adversary stream to differentiate the representation distributions of source and target domains\nmeanwhile, a hash stream is devised to encode hash codes in each domain and benefited from interacting with adversary stream to aggregate domain-invariant and domainspecific knowledge for enhancing hash learning\nmore importantly, we untie weight sharing of cnn in each domain to explicitly model the domain shift rather than enforcing the target close to source as much as possible\nthe whole architecture of dedaha is trained end-to-end by optimizing two kinds of losses, i.e., triplet ranking loss to characterize relative similarity ordering in the triplets from each domain and adversarial loss to make the domain discriminator unable to differentiate between the source and target distributions\n as such, our dedaha endows the target model with more power of exploring source distribution and thus ensures good generalization ability\nmoreover, the generated hash codes could better reflect semantic relations between images (video frames).\nit is also worth noting that our framework could be easily extended to unsupervised scenario, i.e., there is no labeled data in target domain, which will be explored and elaborated in section 3.5.  \nthe main contribution of this work is the proposal of dedaha framework to learn domain adaptive binary representation in a domain adversarial manner\nthe solution also leads to the elegant views of how adversarial training should be leveraged for domain adaptation when there are only a few and even no labeled data in target domain and how the interactions across streams could be taken into account to boost hash learning, which are problems not yet fully understood\nthe remaining sections are organized as follows\nsection 2 describes the related works on both hashing and domain adaptation.\nsection 3 presents our proposed dedaha model and the extensions on unsupervised setting\nsection 4 provides empirical evaluations, followed by the conclusions in section 5", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The recent advances in deep neural networks have demonstrated high capability in a wide variety of scenarios\nNevertheless, finetuning deep models in a new domain still requires a significant amount of labeled data despite expensive labeling efforts\nA valid question is how to leverage the source knowledge plus unlabeled or only sparsely labeled target data for learning a new model in target domain\nThe core problem is to bring the source and target distributions closer in the feature space\nIn the paper, we facilitate this issue in an adversarial learning framework, in which a domain discriminator is devised to handle domain shift\nParticularly, we explore the learning in the context of hashing problem, which has been studied extensively due to its great efficiency in gigantic data\nSpecifically, a novel Deep Domain Adaptation Hashing with Adversarial learning (DeDAHA) architecture is presented, which mainly consists of three components\na deep convolutional neural networks (CNN) for learning basic image/frame representation followed by an adversary stream on one hand to optimize the domain discriminator, and on the other, to interact with each domain-specific hashing stream for encoding image representation to hash codes\nThe whole architecture is trained end-to-end by jointly optimizing two types of losses, i.e., triplet ranking loss to preserve the relative similarity ordering in the input triplets and adversarial loss to maximally fool the domain discriminator with the learnt source and target feature distributions\nExtensive experiments are conducted on three domain transfer tasks, including cross-domain digits retrieval, image to image and image to video transfers, on several benchmarks\nOur DeDAHA framework achieves superior results when compared to the state-of-the-art technique"},
{"doc": "similarity search lays the foundation for many fields, such as computer vision, information retrieval and recommendation [30, 31, 34, 45, 54]\nin the last decades, the volume of data has increased explosively, and it is usually represented in high dimensional spaces.\nas an exhaustive comparison between a query and the instances in a large database is very time-consuming in the original space, traditional similarity search methods are unsuitable for scalable search\non the contrary, hashing methods transform the high dimensional data points into compact binary codes meanwhile preserving the similarity and structural information of the original data\nmoreover, pairwise comparisons can be carried out extremely efficiently by using xor operation in the hamming space.\ntherefore, hashing is more suitable for the large-scale search and many promising hashing methods have been explored in recent years [6, 37, 43, 47, 49].\nroughly speaking, hashing methods can be divided into two categories, i.e., data-independent and data-dependent\nthe former is independent of the training data and uses random projections to generate the hash function\nlocality-sensitive hashing [8] is one of the most representative data-independent methods\nhowever, data-independent methods need long hash codes to achieve high precision, leading to large memory usage.\nin contrast, the latter learns the hash function from data and is able to achieve higher accuracy with shorter codes.\n to efficiently generate desirable binary codes, both unsupervised [5, 12, 46] and supervised [26, 27, 42, 53] methods have been developed for data-dependent hashing.\ninstead of leveraging the label information, unsupervised methods learn the hash function by harnessing the relations in the training data\ntypical examples include spectral hashing [40], iterative quantization [9], inductive hashing on manifolds [36], and scalable graph hashing [16]\nin contrast, supervised methods take the label information into account to learn the hash function and generate the similarity-preserving binary codes\nthey have demonstrated higher accuracy than the unsupervised methods in many real applications, attracting increasing attention.\nalthough many supervised hashing methods have been proposed with promising results, there are several challenges remaining to be addressed\nfirst, as the binary constraints make the hash codes learning problem hard to solve, most of them, like kernel-based supervised hashing (ksh) [28], solve a continuous optimization problem by relaxing the binary constraints.\nhowever, the errors caused by relaxation may degrade the performance.\nalthough some of them, such as fasthash [24], column sampling based discrete supervised hashing (cosdish) [17], discrete supervised hashing (dish), [51] and scalable supervised discrete hashing (ssdh) [29], can solve the binary optimization problem without relaxation, they are rather time-consuming due to the bit-wise learning strategy\nthat is because that one step can only optimize one bit, and making lots of steps needed to optimize all bits.\nanother problem comes from the large memory cost of storing the n \u00d7 n pairwise similarity matrix\nto solve it, most supervised methods sample a subset of the entire dataset for training [24, 25, 28]or sample several columns of the similarity matrix in each iteration [17, 48]\nthis inevitably causes some information loss and degrades the performance\nalthough ssdh [29] can reduce the space cost without sampling a subset, the reduced space cost is relevant to the amount of data which can still be very large\nbesides, among the supervised methods that leverage the pairwise similarity matrix to learn hash codes, most of them fail to exploit the inherent characteristics in the training data, and only the semantic information is considered in the learning of hash codes.\nin this paper, we present a novel supervised hashing method, named fast scalable supervised hashing, fssh for short, to discretely and efficiently learn the hash codes\nour main contributions are summarized as follows\na new loss function is introduced in fssh.\nduring optimization, the large pairwise similarity matrix is replaced with a pre-computed intermediate term such that the memory space cost can be reduced remarkably, making it scalable to large datasets\nmoreover, fssh can jointly harness the label information and features of data\nan iterative algorithm comprising of three steps is proposed to solve the optimization problem.\nin the algorithm, all variables have a closed-form solution, making it converge quickly\nmoreover, instead of learning the hash codes bit by bit, it learns all bits simultaneously\nthey both contribute to the short training time of fssh.\nextensive experiments are conducted over three public benchmark datasets\nthe results demonstrate the superiority of fssh over several state-of-the-art supervised hashing methods in both accuracy and scalability\nthe rest of this paper is organized as follows\nin section 2, we briefly review the related literature\nsection 3 details the proposed method\nsection 4 describes the experimental settings and results on three benchmark datasets\nsection 5 presents a deep variant of fssh and comparisons with several end-to-end hashing methods followed by the conclusion and future work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Despite significant progress in supervised hashing, there are three common limitations of existing methods\nFirst, most pioneer methods discretely learn hash codes bit by bit, making the learning procedure rather time-consuming\nSecond, to reduce the large complexity of the n by n pairwise similarity matrix, most methods apply sampling strategies during training, which inevitably results in information loss and suboptimal performance\nsome recent methods try to replace the large matrix with a smaller one, but the size is still large\nThird, among the methods that leverage the pairwise similarity matrix, most of them only encode the semantic label information in learning the hash codes, failing to fully capture the characteristics of data\nIn this paper, we present a novel supervised hashing method, called Fast Scalable Supervised Hashing (FSSH), which circumvents the use of the large similarity matrix by introducing a pre-computed intermediate term whose size is independent with the size of training data\nMoreover, FSSH can learn the hash codes with not only the semantic information but also the features of data\nExtensive experiments on three widely used datasets demonstrate its superiority over several state-of-the-art methods in both accuracy and scalability.\nOur experiment codes are available at: https://lcbwlx.wixsite.com/fss"},
{"doc": "human knowledge is inherently organized in the form of semantic, content-specific hierarchies\nlarge-scale taxonomies such as wikipedia categories, freebase [2] and wordnet [6] are crucial sources of structured knowledge useful for various natural language processing applications\nhowever, although these hierarchies are well developed, they are largely generic and laborious to augment and maintain, with new concepts and relations from newly emerging, or rapidly evolving domains such as public health and current affairs.\nthese challenges necessitate the development of automated, scalable techniques to solve the problem of taxonomy enrichment, i.e., to augment a taxonomic hierarchy by accurately placing novel or unfamiliar concepts in it at an appropriate location and level of granularity (avoiding links that are too specific, too general, or contextually related but non-ancestral)\nfor example, based on the wikipedia article on the hurricane harvey in august 2017, we would like to link it to coherent, specific categories such as category 4 atlantic hurricanes and august 2017 events in the united states rather than semantically related but non-ancestral classes such as hurricane irma, or overly general categories like hurricane.\nexisting work in the area of automated taxonomy enrichment is still either highly language-specific [47, 48], domain-specific [28] or cannot scale to large taxonomies [3, 21, 43]\nmany techniques depend on the unique synset structure specific to wordnet [14, 31, 37], cannot generalize to other taxonomies, and can only identify a single category for new concepts\neven with these limitations, not all attempts at taxonomy enhancement have succeeded [28]\nin contrast, we identify that the main challenge for this task is to find a computational measure that can proxy the logic of semantic subsumption, independent of the language and knowledge domain, in order to automatically find good parents for new concepts.\nto address this fundamental challenge, we propose a combination of a carefully selected set of highly effective graph-theoretic features and semantic similarity based features leveraging external knowledge sources\nwe show that this combination measure can predict the links to the parents of new concepts with high accuracy.\ninterestingly, our measure does not require any assumptions on the number of parents for each new concept\nmoreover, our evaluation methods are fully automated using publicly available data, and don\u2019t rely on domain experts or manual judgment as in [4, 20, 25, 31, 42].\nintuitively, we believe that etf is similar to how a human would approach the task of adding a new concept to a taxonomy\nto complete such a task, one might:\n(i) find a set of existing concepts that are related to the new one,\n(ii) rank each concept in the union of ancestors of this set, i.e., the closure via hypernymy/parent relations, and;\n(iii) link the new concept to a few selected top-ranked parents.\nto formalize the intuitive method we just outlined, our proposed approach etf first constructs a vector representation for new concepts\nhere, we use a representation that aggregates two kinds of embeddings formed from the context of the concept\nwe observed that the embeddings complement each other and so the aggregated vector provides a good measure for semantic similarity, that can identify the nearest neighbors of the new concept in the taxonomy\nwith this representation, the search for potential parents is restricted to the (small) set of ancestors of these nearest neighbors\nexperiments show that on test concepts from the wikipedia category taxonomy with more than 5 million concepts, on average this set contains a few thousand nodes and covers 83% of the true parents of the test concepts\nfinally, we develop algorithms that rank the ancestors in this set, selecting only those above a global scoring threshold to be the parents of the new concept\ntherefore, the key contributions of our work are\nwe develop a novel, fully automated framework, etf, that generates semantic text-vector embeddings for each new concept\nthese embeddings allow us to find semantically related concepts in the existing taxonomy, which in turn allows us to extract the ancestors of these related concepts\nwe propose the use of a learning algorithm that combines a carefully selected set of graph-theoretic and semantic similarity based features to rank candidate parent relations\nthis ranker accurately links new concepts to good candidate parents by ranking the ancestors of their semantic neighbors\nwe test etf on large, real-world, publicly available knowledge bases such as wikipedia and wordnet, and outperform baselines at the task of inserting new concepts\ncrucially, our experimental design is easily reproducible, and can be used as a benchmark for future research on this topic\nthrough two case studies, we show that etf can accurately categorize new concepts from rapidly evolving real-world domains, as well as new questions and answers from quor", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The rising need to harvest domain specific knowledge in several applications is largely limited by the ability to dynamically grow structured knowledge representations, due to the increasing emergence of new concepts and their semantic relationships with existing ones\nSuch enrichment of existing hierarchical knowledge sources with new information to better model the \u201cchanging world\u201d presents two-fold challenges:\n(1) Detection of previously unknown entities or concepts, and\n(2) Insertion of the new concepts into the knowledge structure, respecting the semantic integrity of the created relationships\nTo this end we propose a novel framework, ETF, to enrich large-scale, generic taxonomies with new concepts from resources such as news and research publications.\nOur approach learns a high-dimensional embedding for the existing concepts of the taxonomy, as well as for the new concepts\nDuring the insertion of a new concept, this embedding is used to identify semantically similar neighborhoods within the existing taxonomy\nThe potential parent-child relationships linking the new concepts to the existing ones are then predicted using a set of semantic and graph features\nExtensive evaluations of ETF on large, real-world taxonomies of Wikipedia and WordNet showcase more than 5% F1-score improvements compared to state-of-the-art baselines\nWe further demonstrate that ETF can accurately categorize newly emerging concepts and question-answer pairs across different domain"},
{"doc": "searching the web for information is among the most frequent online activities.\nbroder categorized web search queries into having either navigational, transactional or informational intents [4].\nin informational web search sessions, the intent  of a user is to acquire some information assumed to be present on one or more web pages\n recent research in the search as learning  (sal) domain  has recognized the importance of learning  scopes and focused on observing and detecting learning needs during web search.\neickhoff et al. investigated the correlation between several query and search mission-related metrics and learning progress [8].\nwu et al. pre- dicted the difficulty of search tasks from query and mission-related features [21].\ncollins-thompson et al. investigated the effectiveness of user interaction with respect to certain learning outcomes [7].\nin addition, [22] has shown that data obtained during the search process provides valuable indicators about the domain knowledge of a user. \nalthough the importance of learning  as an implicit element of web search has been established, there is still only a limited  understanding of the impact of search behavior on a user\u2019s knowledge state and knowledge gain\n prior work has focused on improving the learning experience and efficiency during search sessions, but the measurement of a user\u2019s knowledge gain through the course of an informational search session has not yet been addressed.\nthis is in part due to the difficulty in accurately quantifying knowledge gain through the course of a search session.\nif web search engines that are currently optimized for relevance can be remolded to serve learning outcomes, the capability to predict knowledge gain will be a crucial  step forward\nin this paper, we aim to address the aforementioned  gap.\nwe used crowdsourcing to recruit users who participated in real-world search sessions spanning  11 different topics and information needs.\nby using scientifically formulated knowledge tests, we calibrated the knowledge of users before and after their search sessions, quantifying their knowledge gain.\nwe introduce a supervised model to predict a user\u2019s knowledge state and knowledge gain from features captured during the search sessions\noriginal contribution\nthrough our work in this paper, we make the following contributions to the current body of literature\na model for predicting  the user\u2019s knowledge gain and state during real-world informational search sessions\nan analysis of the affect of user interactions (ranging from the queries entered to their browsing behavior) on their knowledge state and knowledge gain\nwe release a dataset capturing  user behavior and interactions  in 468 experimentally orchestrated informational  search missions, capturing all features across the aforementioned  dimensions as well as knowledge  assessments obtained through pre- and post- tests. \ngiven the lack of comparable datasets which  are both recent and publicly  available, we anticipate that this corpus can facilitate sal research related to different tasks\nimplications.\nthe capability to predict a user\u2019s knowledge state and gain through the course of an informational search session has the potential to reshape search engines to support learning outcomes as an implicit part of retrieval and ranking.\nthis is of particular importance given that web search already augments learning processes in a variety of informal as well as formal learning scenarios, such as classrooms, libraries  and in work environments\nour contributions advance the current understanding of learning through web search, setting important precedents for further researc", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Web search is frequently  used by people to acquire new knowledge and to satisfy learning-related objectives.\nIn this context, infor- mational search missions with an intention to obtain knowledge pertaining to a topic are prominent.\nThe importance of learning as an outcome of web search has been recognized.\nYet, there is a lack of understanding of the impact of web search on a user\u2019s knowledge state.\nPredicting the knowledge gain of users can be an important step forward if web search engines that are currently optimized for relevance can be molded to serve learning outcomes.\nIn this paper, we introduce a supervised model to predict a user\u2019s knowledge state and knowledge gain from features captured during the search sessions.\nTo measure and predict the knowledge gain of users in informational search sessions, we recruited 468 distinct users using crowdsourcing and orchestrated real-world search ses- sions spanning 11 different topics and information needs.\nBy using scientifically  formulated knowledge tests, we calibrated the knowl- edge of users before and after their search sessions, quantifying their knowledge gain.\nThrough our results, we demonstrate the ability to predict and classify the knowledge state and gain using features obtained during search sessions, exhibiting superior performance to an existing baseline in the knowledge state prediction tas"},
{"doc": "the emerging of large-scale knowledge bases (kbs) such as freebase [2], yago [16] and dbpedia [1] has stimulated the development of novel information-retrieval applications with great commercial or societal impact.\nusually a kb is presented as a collection of triples (h, r , t ), where h and t are entities and r is a relation\nfor example, a knowledge triple could be (paris, iscapitalof, france)\nalthough the existing kbs contain enormous amount of such information, it is well known that significant amount of factual knowledge is in fact missing\nfor example, not all facts about paris, about france, or about which city is the capital of which country are collected in the kb\nthis has motivated intense research on knowledge base completion, where the objective is to \u201cfill in\u201d the missing information based on the current content of a kb\nmost commonly the kb completion problem is posed as link prediction\nbriefly explained, in a link prediction task (h, r , ?) for a given entity h and a given relation r , the objective is to determine which entity (or entities) t that can form a factual triple (h, r , t ).\nthe current dominating methodology for kb link prediction is via learning a kb embedding model [3, 5, 10\u201312, 15, 18, 20], in which the entities and relations of the kb are represented as quantities in some euclidean space\nsuch a methodology, fundamentally based on distributed representations [8], has not only proved to be effective for kb link prediction, but also helped to improve our understanding and engineering of knowledge representation.\ndespite the success of various kb embedding models, we argue that the evaluation criterion used in link prediction is inappropriate\nspecifically, when answering the link prediction task (h, r , ?), implicitly or explicitly the top-k criterion is used\nthat is, for a given value of k, the predictive algorithm outputs k answers based on ranking certain score computed for each entity.\nwe argue that this criterion is problematic when for different (h, r , ?) tasks, the number of correct answers, or \u201canswer multiplicity\u201d, varies significantly\nin this case it is impossible to select a global cutoff that compromises well across such a wide spectrum of answer multiplicities.\nthis paper presents a new evaluation criterion, which we refer to as max-k.\nin this criterion, the predictive algorithm is asked to give at most k answers for a prescribed k value\nin other words, the predictive algorithm is free to use a strategy or protocol to output any number of answers, no greater than k\nwe adapt the classical precision, recall and f1 metrics to this setting to evaluate the answers provided by the predictive algorithm\nwe show that when the predictive algorithm is an oracle knowing the correct answers, then max-k is at least as good as top-k under these metrics\nwe then present two protocols applying universally to all prediction models\nthe first is a sampling protocol which draws k entities from the predictive distribution generated by the model, using distinct drawn entities as the answers\nwe show that if the model\u2019s predictive distribution reflects the oracle\u2019s knowledge of the correct answers, the sampling protocol is optimal in an average sense\nthe second is a deterministic greedy protocol, developed and theoretically justified via an asymptotic analysis of the sampling protocol.\nusing these protocols, we carry out extensive experiments investigating the performance of some representative kb embedding models over several popular datasets\nwe highlight the following experimental findings\nfirst, the precision, recall and f1 metrics under the max-k criterion behave differently from the conventional metrics used for the top-k criterion\nwe believe that these max-k metrics provide a more balanced measure for model performances\nadditionally, we may use the sampling and greedy protocols on the existing models and compare their performance against the oracle max-k limits\nthis allows us to assess the performance gap between these models and the oracle limits thereby assessing the performance of the current art relative to the theoretical limits\nfinally, a comparison of the top-k and max-k oracle limits suggests that the top-k criterion is fundamentally and significantly inferior", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Building knowledge base embedding models for link prediction has achieved great success.\nWe however argue that the conventional top-k criterion used for evaluating the model performance is inappropriate\nThis paper introduces a new criterion, referred to as max-k\nThrough theoretical analysis and experimental study, we show that the top-k criterion is fundamentally inferior to max-k\nWe also introduce two prediction protocols for the max-k criterion\nThese protocols are strongly justified theoretically\nVarious insights concerning the max-k criterion and the two protocols are obtained through extensive experiment"},
{"doc": "knowledge graphs (kgs) have become essential for applications such as search, query understanding, recommendation and question answering because they provide a unified viewof real-world entities and the facts (i.e., relationships) that hold between them [6, 7, 22, 34]\nfor example, kgs are increasingly being used to provide direct answers to user queries [34], or to construct so-called entity cards that provide useful information about the entity identified in the query\nrecent work [10, 17] suggests that search engine users find entity cards useful and engage with them when they contain information that is relevant to their search task, for instance in the form of a set of recommended entities and facts that are related to the query [6]\nprevious work has focused on augmenting entity cards with facts that are centered around, i.e., one-hop away from, the main entity of the query [17]\nhowever, oftentimes a user is interested in kg facts that by definition involve more than one entity (e.g., \u201cwho founded microsoft?\u201d \\\\rightarrow \u201cbill gates\u201d)\nin such cases, we can exploit the richness of the kg by providing query-specific additional facts that increase the user\u2019s understanding of the fact as a whole, and that are not necessarily centered around only one of the entities\nadditional relevant facts for the running example would include bill gates\u2019 profession, microsoft\u2019s founding date, its main industry and its co-founder paul allen (see figure 1)\nin this case, bill gates\u2019 personal life is less relevant to the fact that he founded microsoft.\nquery-specific relevant facts can also be used in other applications to enrich the user experience.\nfor instance, they can be used to increase the utility of kg question answering (qa) systems that currently only return a single fact as an answer to a natural language question [5, 34]\nbeyond qa, systems that focus on automatically generating natural language from kg facts [15, 20] would also benefit from query-specific relevant facts, which can make the generated text more natural and human-like\n this becomes even more important for kg facts that involve tail entities, for which natural language text might not exist for training [32]\nin this paper, we address the task of kg fact contextualization, that is, given a kg fact that consists of two entities and a relation that connects them, retrieve additional facts from the kg that are relevant to that fact\nthis task is analogous to ad-hoc retrieval:\n(i) the \u201cquery\u201d is a kg fact, (ii) the \u201cdocuments\u201d are other facts in the kg that are in the neighborhood of the \u201cquery\u201d\nwe propose a neural fact contextualization method (nfcm), a method that first generates a set of candidate facts that are part of {1,2}-hop paths from the entities of the main fact\nnfcm then ranks the candidate facts by how relevant they are for contextualizing the main fact\nwe estimate our learning to rank model using supervised data\nthe ranking model combines (i) features we automatically learn from data and (ii) those that represent the query-candidate facts with a set of hand-crafted features we devised or adjusted for this task\ndue to the size and heterogeneous nature of kgs, i.e., the large number of entities and relationship types, we turn to distant supervision to gather training data\nusing another, human-verified test collection we gauge the performance of our proposed method and compare it with several baselines\nwe sum up our contributions as follows.\nwe introduce the task of kg fact contextualization where the goal is to, given a fact that consists of two entities and a relationship that connects them, rank other facts from a kg that are relevant to that fact\nwe propose nfcm, a method to solve kg fact contextualization using distant supervision and learning to rank.\nour results show that: (i) distant supervision is an effective means for gathering training data for this task and (ii) a neural learning to rank model that is trained end-to-end outperforms several baselines on a human-curated evaluation set\nwe provide a detailed result analysis and insights into the nature of our task\nthe remainder of the paper is organized as follows\nwe first provide a definition of our task in section 2 and then introduce our method in section 3\nwe describe our experimental setup and detail our results and analyses in sections 4 and 5, respectively.\nwe conclude with an overview of related work and an outlook on future directions", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Knowledge graphs (KGs) model facts about the world; they consist of nodes (entities such as companies and people) that are connected by edges (relations such as founderOf ).\nFacts encoded in KGs are frequently used by search applications to augment result pages\nWhen presenting a KG fact to the user, providing other facts that are pertinent to that main fact can enrich the user experience and support exploratory information needs\nKG fact contextualization is the task of augmenting a given KG fact with additional and useful KG facts.\nThe task is challenging because of the large size of KGs; discovering other relevant facts even in a small neighborhood of the given fact results in an enormous amount of candidates.\nWe introduce a neural fact contextualization method (NFCM) to address the KG fact contextualization task\nNFCM first generates a set of candidate facts in the neighborhood of a given fact and then ranks the candidate facts using a supervised learning to rank model\nThe ranking model combines features that we automatically learn from data and that represent the query-candidate facts with a set of hand-crafted features we devised or adjusted for this task\nIn order to obtain the annotations required to train the learning to rank model at scale, we generate training data automatically using distant supervision on a large entity-tagged text corpus\nWe show that ranking functions learned on this data are effective at contextualizing KG facts.\nEvaluation using human assessors shows that it significantly outperforms several competitive baseline"},
{"doc": "previous studies showed that user clicks can be used as implicit relevance feedback to improve the ranking of search results [13].\nhowever, clicks on a result are inherently stochastic and systematically biased by factors such as the position [6, 13] and presentation style [2, 24] of the result\ntherefore, a number of click models (see [3] for an overview) have been proposed to model users\u2019 click behavior as a stochastic process and obtain unbiased relevance feedback from the biased click logs.\nthe performance of a click model depends heavily on making correct assumptions on users\u2019 search behavior\nby assuming a user will examine and click the results on the search engine result page (serp) in a certain way, a click model can estimate how different kinds of biases affect users\u2019 click actions and derive unbiased relevance feedback from click logs\nhowever, users\u2019 search behavior in the mobile environment are different from those in the desktop context\nfor example, previous studies suggest that users will pay more attention to the top-ranked results and scan fewer results on a small screen [16]; relevance judgments for documents are also affected by search devices [23]\ntherefore, the existing click models originally designed for the desktop environment may not be as effective in the mobile search context\nwe need to refine the existing behavioral assumptions of click models to adapt to the shift from desktop to mobile.\none of the factors that may alter users\u2019 behavior in the mobile environment is the heterogeneity of search results\ntoday\u2019s search engines return richer results than the homogeneous ten blue links on both mobile and desktop\nthe heterogeneous results have a larger impact on users\u2019 interaction behavior on mobile serps because\n1) compared to desktop search, direct answer and knowledge card results are federated into mobile serps more frequently.\nin many circumstances, these results present useful information on the serp and users do not need to click the hyperlinks to visit the corresponding landing pages.\nwhile loading a page on mobile devices may take a longer time than on desktop devices, this strategy helps to reduce users\u2019 interaction costs as well as data usage on mobile\n2) due to the limit of screen size, the heterogeneous results are usually injected into the main ranking list and often occupies a large proportion of user viewport\n in a recent study, luo et al. [19] showed these two factors may affect users\u2019 behavior in mobile search and proposed to incorporate them in the evaluation of mobile search engines.\nas an example, we show two serps for the same query, ann arbor, on mobile and desktop in figure 1\ncompared with the desktop serp in figure 1b that displays the knowledge graph result on the right side, the knowledge graph result is placed at the first position in the mobile serp (figure 1a) and occupies almost the whole initial viewport\n this result is highly likely to be examined by users and affect their following actions\nthe knowledge graph result contains a brief introduction to the city, as well as the information about the weather and local time\na user who wants to gather some basic information about ann arbor will find the knowledge graph result relevant and useful even without clicking it\nshe may even feel satisfied and leave the serp just after examining the first knowledge graph result.\nin this case, an existing click model will\n1) mistakenly regard the skipping (i.e. no click) behavior on the first result as a negative relevance feedback\n2) ignore the cut off effect [18], that the user can be satisfied with the non-clicked knowledge graph result, but still assume the user will scan the following results.\nwhile some studies [2, 4, 24] tried to incorporate the heterogeneity of search results into click models in the desktop environment, they mainly focused on modeling the presentation bias or attetion bias but ignored the click necessity bias.\nchen et al. [2] found that users are more likely to examine the vertical result and more likely to be satisfied if they click it\nchuklin et al. [4] assumed that the probability that a user will examine a result is determined by her intent and the type of the result\nbased on the findings in an eyetracking study, wang et al. [24] further assumed that the vertical result will affect not only the examination probabilities but also the examination order of search results\nhowever, none of these existing efforts considered the situations where some results provide sufficient information on serps and thus are less likely to be clicked.\nto address this problem in the mobile search context, we propose a novel click model named mobile click model (mcm).\nthe proposed mcm assumes that\n1) some types of search results (e.g. the knowledge graph and direct answer results) have lower click necessity than others, which means that they can fulfill users\u2019 information needs without requiring any clicks (click necessity bias);\n2) a user can be satisfied after examining a search result with low click necessity because this kind of results are designed to satisfy users\u2019 common information needs directly on serps (examination satisfaction bias)\nwe will further introduce how we incorporate these two biases into the proposed model in section 3.\nthrough extensive experiments on a large-scale mobile search log from a popular commercial search engine in china, we show that the proposed model can effectively infer the parameters for click necessity and examination satisfaction, along with the parameters for relevance and click satisfaction, from users\u2019 interaction logs with heterogeneous mobile serps\nwith these parameters learned from logs, we can\n1) improve the ranking of heterogeneous results in mobile search;\n2) analyze how users interact with a certain type of vertical results\nthe experiment results also show that mcm achieves better performance in both click prediction and relevance estimation tasks than the baseline click models which are not specifically designed for the mobile environment.\nthe rest of the paper is organized as follows\nwe first provide an overview of the background of mobile search and click models in section 2\nin section 3, we will formally introduce mcm model and compare it with existing click models\nwe then present the experiment setup and results in section 4\nfinally, we conclude the paper and discuss directions for future work in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Users\u2019 click-through behavior is considered as a valuable yet noisy source of implicit relevance feedback for web search engines\nA series of click models have therefore been proposed to extract accurate and unbiased relevance feedback from click logs\nPrevious works have shown that users\u2019 search behaviors in mobile and desktop scenarios are rather different in many aspects, therefore, the click models that were designed for desktop search may not be as effective in mobile context\nTo address this problem, we propose a novel Mobile Click Model (MCM) that models how users examine and click search results on mobile SERPs\n Specifically, we incorporate two biases that are prevalent in mobile search into existing click models\n1) the click necessity bias that some results can bring utility and usefulness to users without being clicked\n2) the examination satisfaction bias that a user may feel satisfied and stop searching after examining a result with low click necessity\nExtensive experiments on large-scale real mobile search logs show that\n 1) MCM outperforms existing models in predicting users\u2019 click behavior in mobile search;\n2) MCM can extract richer information, such as the click necessity of search results and the probability of user satisfaction, from mobile click logs\nWith this information, we can estimate the quality of different vertical results and improve the ranking of heterogeneous results in mobile searc"},
{"doc": "prospective information needs are typically posed against document streams such as posts on twitter, rss feeds, newswire articles, etc\nfor example, a user might be concerned about tensions on the korean peninsula and wishes to receive updates whenever there is a new development.\nreal-time summarization systems monitor these streams and identify documents that are relevant, typically operationalized as on-topic, non-redundant, and timely\nthe nature of prospective information needs means that relevant documents may appear at any time, and thus an open question is how system output should be delivered to users\nwe explore this question in the context of mobile devices that are ubiquitous today\nthere are two basic mechanisms for delivering updates from real-time summarization systems\nin one approach, an update can be delivered to a user\u2019s mobile device as a push notification, typically accompanied by a visual and/or auditory signal that purposely interrupts the user to attract attention\nalternatively, updates can be silently deposited into an \u201cinbox\u201d, waiting for the user to examine on the user\u2019s own initiative (much like email)\nthese two mechanisms can be characterized as either \u201cpush-based\u201d or \u201cpullbased\u201d\u2014 terminology we will use throughout this paper\nalthough in reality a system can adopt both mechanisms, for simplicity we examine either approach in isolation\nwe present a two-year study drawn from the trec real-time summarization (rts) tracks involving over 50 users who evaluated live system updates on their mobile devices in situ, i.e., as they were going about their daily lives\nthese users interacted with a mobile app that employed either the push- or pull-based mechanism described above.\nwe are specifically interested in three research questions focused on users\u2019 attention to system updates\n(rq1) how do users in the push vs. pull treatments differ in terms of quantifiable behavioral characteristics\n(rq2) can we generalize patterns of user behavior within and across push- and pull-based update delivery\n(rq3) can we operationalize our findings into design recommendations for developers of real-time summarization systems\ncontributions\nin answering the above research questions, we view our work as having the following contributions\nwe present, to our knowledge, the first contrastive study of push vs. pull-based delivery mechanisms for addressing prospective information needs for mobile users\nsince the main experimental manipulation was the delivery mechanism, we can attribute differences in user behavior to the system treatment\nwe present a novel methodology for data analysis that exploits different visualizations to empirically characterize how much and when users pay attention to updates from systems with different delivery mechanisms\nwe identify, via a clustering analysis, distinct and coherent patterns of user behaviors and the impact of delivery mechanisms\nwe hypothesize that users can be characterized as eager or apathetic with respect to information consumption, which provides an explanation of observed behaviors\nwe operationalize our findings into concrete design recommendations for system developers, identifying categories of users for which push notifications might or might not be valuable\nas an initial foray into information delivery mechanisms on mobile devices for real-time summarization systems, our work raises as many interesting questions as it answers\nwe are forthright in discussing the limitations of our study, most of which point directly to future avenues of exploratio", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Real-time summarization systems that monitor document streams to identify relevant content have a fewoptions for delivering system updates to users.\nIn a mobile context, systems could send push notifications to users\u2019 mobile devices, hoping to grab their attention immediately\nAlternatively, systems could silently deposit updates into \u201cinboxes\u201d that users can access at their leisure.\nWe refer to these mechanisms as push-based vs. pull-based, and present a twoyear contrastive study that attempts to understand the effects of the delivery mechanism on mobile user behavior, in the context of the TREC Real-Time Summarization Tracks\nThrough a cluster analysis, we are able to identify three distinct and coherent patterns of behavior\nAs expected, we find that users are likely to ignore push notifications, but for those updates that users do pay attention to, content is consumed within a short amount of time\nInterestingly, users bombarded with push notifications are less likely to consume updates on their own initiative and less likely to engage in long reading sessions\u2014which is a common pattern for users who pull content from their inboxes\nWe characterize users as exhibiting \u201ceager\u201d or \u201capathetic\u201d information consumption behavior as an explanation of these observations, and attempt to operationalize our findings into design recommendation"},
{"doc": "searching for items by their attribute values or metadata is a commonplace task today\nfor example, consider searching for a particular research paper you recall as published in 2013, or computer monitors under $200\ncurrent search tools support a retrieval style more akin to a database than a modern information retrieval system, most often with faceted or boolean search.\nin that model, users place hard constraints on acceptable attribute values to limit the result set.\nunfortunately, this rigid style of retrieval often creates difficulties for the user\nin the examples above, what if the sought research paper was published in 2012, not 2013 as remembered\nthere are thousands of computer monitors under $200, far too many to examine, but constraining all desired attributes might yield no results\nand what if a monitor that best fits the user\u2019s desire is just outside the stated range, listing at $213? boolean retrieval often yields no results or too many\nfaceted search usually avoids empty result sets, but the facets are often pre-computed and may not match the user\u2019s intent well.\nthe problem with such retrieval systems is that they do not try to understand what the user is seeking\ninstead, they give the user a tool to explicitly manage the result set by stating what to return and how to order it\nin this paper, we explore an alternative approach that is more aligned with current information retrieval approaches \u2013 implicitly managing result sets by estimating relevance to a description of the sought item\nwe model the user\u2019s utility function, and in the process, allow the system to trade off among conflicting criteria on the user\u2019s behalf, and in this way get closer to the underlying query intent\nin contrast to the boolean and faceted approaches in use today, our approach does not use constraints\nwe evaluate this new approach against the de facto standard of boolean retrieval and several models proposed in the literature in two user studies using amazon mechanical turk\nthe domains and tasks in these user studies are diverse, with one involving searching for airline tickets and in the other for healthy (daily) meal plans\nin both studies, test subjects read a short scenario and used a randomly chosen retrieval model to find an appropriate item (ticket or meal plan).\nwe ask the following questions\nrq1 which approach and specific retrieval algorithm is most effective?\nrq2 are constraints beneficial or harmful\nrq3 how should results be ordered?\nin this work, we make the following contributions:\n(1) we cast the item retrieval problem as finding the item with the most desirable combination of attribute values, and use utility theory to develop a basic model\n(2) we expand upon the basic retrieval model by developing subutility models that estimate the utility of every possible value of each attribute\n(3) we develop a bayesian hierarchical model around our item utility model so the models\u2019 parameters can be fit to data of users\u2019 selections\n(4) we evaluate our novel item retrieval model against the two common approaches, boolean and faceted retrieval, along with several models from the literature, in two user studies in very different domains\nthe rest of this paper is organized as follows\nin section 2, we review the related research and introduce the previously published methods included in our study\nin section 3, we introduce an initial retrieval method based on multi-criteria decision-making theory and a subsequent enhancement to that method\nthe learning framework we later use to tune this model\u2019s parameters is described in section 4\nin section 5, we detail the design of our two user studies, including the baseline retrieval methods that we include for comparison\nfinally, we present the results of these user studies in section 6 and present our conclusions in section 7", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Retrieval systems have greatly improved over the last half century, estimating relevance to a latent user need in a wide variety of areas\nOne common task in e-commerce and science that has not enjoyed such advancements is searching through a catalog of items\nFinding a desirable item in such a catalog requires that the user specify desirable item properties, specifically desirable attribute values\nExisting item retrieval systems assume the user can formulate a good Boolean or SQL-style query to retrieve items, as one would do with a database, but this is often challenging, particularly given multiple numeric attributes\nSuch systems avoid inferring query intent, instead requiring the user to precisely specify what matches the query\nA contrasting approach would be to estimate how well items match the user\u2019s latent desires and return items ranked by this estimation.\nTowards this end, we present a retrieval model inspired by multi-criteria decision making theory, concentrating on numeric attributes\nIn two user studies (choosing airline tickets and meal plans) using Amazon Mechanical Turk, we evaluate our novel approach against the de facto standard of Boolean retrieval and several models proposed in the literature\nWe use a novel competitive game to motivate test subjects and compare methods based on the results of the subjects\u2019 initial query and their success in the game\nIn our experiments, our new method significantly outperformed the others, whereas the Boolean approaches had the worst performanc"},
{"doc": "crowdsourcing platforms such as amazon\u2019s mechanical turk provide a low-cost and scalable way of collecting relevance judgments [2, 14]\nwhile crowdsourcing is most often motivated by improved scalability, it offers other potential benefits as well\ninstead of relying on a single expert judgment for each document, a set of crowd judgments can be collected and aggregated to guard against human error (even trusted judges are fallible), or again human bias, by reflecting average opinion in what is an inherently subjective judging task\nit may even be easier to find a crowd judge with relevant expertise than personnel available in one\u2019s local area [7, 22].\nwhile crowdsourced judgments have been used in developing several test collections [5, 16], crowd task designs require special attention to ensure the quality of the collected judgments\ntherefore, understanding reasons for crowd disagreements with trusted assessors is important for designing better crowd tasks\ndespite many studies reporting high disagreement in relevance judging between trusted assessors [26, 27], disagreements with crowd workers are sometimes attributed to workers being lazy, stupid, or deceitful\nwhile much prior work has sought to improve the quality of collected crowd data, relatively less work has sought to better understand and characterize the types of judging disagreement the crowd tends to exhibit\nmoreover, most work has assumed crowd disagreement constitutes error rather than trying to distinguish valid disagreement from actual error\nunderstanding the reasons behind judging disagreement is difficult without having insights into the judges\u2019 thought-processes\nconsequently, prior work studying relevance judgments of primary vs. secondary assessors has sometimes relied on research methods involving interaction with participant judges, such as think-aloud [1] and interviewing [26]\nhowever, it can be challenging to apply these methods on the current crowdsourcing platforms.\nour earlier work [20, 21] proposed a rationale task (rt) design for collecting crowdsourced relevance judgments\nin particular, simply asking judges to provide short excerpts from each document to explain their judgment for it was shown to yield a multitude of benefits\nin this work, we investigate how we can further exploit these rationales to gain new insights into reasons for judging disagreement, especially with remote crowd work\nlargely following our original rt design, we collect roughly 25k crowd judgments for 5k document-topic pairs sampled from the 2014 trec web track [11]\nas a refinement, we show that having judges focus on judging within a topic, rather than across topics, improves label quality\noverall, we find that crowd judgments are good enough for ranking information retrieval (ir) systems reliably.\nnext, we conduct a qualitative analysis using rationales to understand the disagreements and present a novel taxonomy of types of disagreement\nin our analysis, we manually inspect 1k crowd judgments for 200 documents (5 judgments per document) in which the aggregated crowd judgment differs from the original trec judgment, and we assess the relative importance of each disagreement cause\nour analysis distinguishes between valid disagreement due to subjective considerations (e.g., relevance thresholds) from consistently recognizable human error (e.g., clearly missed evidence of relevance)\namong the 200 documents we inspected,we agreed with trec assessors in only 51.5% of the cases, disagreeing otherwise due to perceived human error (in 21.5%), subjective considerations (20%), and other miscellaneous reasons (7%)\nsimilarly, we agree with 51% of the 1k crowd judgments while disagreeing due to perceived human error (37% of cases), subjective considerations (6%), and other miscellaneous reasons (6%)\ncontributions of our work are as follows\nwe show that topic-focused crowd judging improves quality vs. our earlier design judging across topics [21]\nwe show that crowd judgments we collect are largely valid and plausible, and that they enable reliable ranking of participant ir systems in the 2014 trec web track.\nwe demonstrate the value of assessor rationales for helping to explain disagreements in relevance judging\nwe present a novel taxonomy over types of disagreement, qualitatively and quantitatively distinguish objective vs. subjective disagreements, and estimate the extent of human error in trusted judgments\nwe share our webcrowd25k dataset, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed.\nin separate work [13], we also describe and share (3) crowd judging behavioral data\nthe remainder of this paper is organized as follows\nwe first present related work in section 2\nwe then describe our prior rationale task design [21] in section 3\nsection 4 explains our crowd task design and collection, and evaluates the quality of the judgments\nin section 5, we discuss our qualitative analysis and outline our novel taxonomy of disagreement types, presenting the results of the analysis in section 6\nfinally, we conclude in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "While crowdsourcing offers a low-cost, scalable way to collect relevance judgments, lack of transparency with remote crowd work has limited understanding about the quality of collected judgments\nIn prior work, we showed a variety of benefits from asking crowd workers to provide rationales for each relevance judgment [21]\nIn this work, we scale up our rationale-based judging design to assess its reliability on the 2014 TREC Web Track, collecting roughly 25K crowd judgments for 5K document-topic pairs\nWe also study having crowd judges perform topic-focused judging, rather than across topics, finding this improves quality\nOverall, we show that crowd judgments can be used to reliably rank IR systems for evaluation.\nWe further explore the potential of rationales to shed new light on reasons for judging disagreement between experts and crowd workers\nOur qualitative and quantitative analysis distinguishes subjective vs. objective forms of disagreement, aswell as the relative importance of each disagreement cause, and we present a new taxonomy for organizing the different types of disagreement we observe\nWe show that many crowd disagreements seem valid and plausible, with disagreement in many cases due to judging errors by the original TREC assessors\nWe also share our WebCrowd25k dataset, including\n(1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyze"},
{"doc": "selective search is a distributed search architecture that avoids searching the entire corpus for each query\nwhen the index is built, it is divided into small, topically-oriented shards\nduring retrieval, first shards (or resources) are ranked by their likelihood of returning documents relevant to the query, and then only the most queryrelevant shards are searched\nthe accuracy and efficiency of the selective search architecture depends upon the number of shards that are searched (the cutoff )\nsearching too few shards harms accuracy, while searching too many shards harms efficiency\nwhile a large body of work now exists around this technology [16, 19\u201321, 23\u201325], many interesting problems remain\nin this paper we focus on two related issues\nfirst, distributed search is increasingly viewed as an early-stage retrieval process where the task is to efficiently collect as many possibly relevant documents before applying more expensive learning-to-rank algorithms [5, 28, 37, 44]\nas such, optimizing for early precision metrics such as err [8] and ndcg@10 [17] during selective search may not be desirable\nsecond, the ideal number of shards to search can depend heavily on the resource selection algorithm, the desired search type (recall-driven or early-precision-driven), and the specific query\nin spite of the importance of selecting the right number of shards with respect to targeted evaluation, this aspect of selective search has not been studied extensively by prior research\nsome approaches treat the cutoff as a parameter to be tuned for a query set; that is, the same value is used for every query [2, 4, 13, 23, 40, 41]\nother approaches treat it as part of the resource selection problem, where the result is a shard ranking and a cutoff\nfor example, sushi [45] selects shards that are expected to have documents in the top-n of the final ranking, taily [1] sets the cutoff based on an estimate of the minimum number of relevant documents in each shard, and rank-s [25] sets the cutoff using a rank-based decay function of the shard\u2019s relevance score\nquery-based cutoffs produced by algorithms such as sushi, taily, and rank-s are appealing, however there has been little study of the prediction accuracy.\nanother limitation in past work is the assumption that singlepass retrieval using bm25 or language models and focusing on early precision is sufficient\nin this scenario, only a few shards are required for most queries, and less accurate cutoff predictions tend to not hurt efficiency\nhowever, complex multi-stage ranking pipelines are now common [9, 37]\nif selective search is used in a pipeline for early-stage retrieval, recall should be a priority [31], and maximizing for recall often means searching more shards initially\nquery-specific shard cutoff prediction \u2013 the problem of predicting the number of shards to search for each query \u2013 depends on the size of the desired result set\nthus, we distinguish between early precision and high recall search requirements\nan early precision scenario measures accuracy in the first few ranked documents (e.g., 1 . . . 10), and thus is likely to be more efficient because fewer shards are searched\nthis is the scenario studied most often in prior work\nin contrast, a high recall scenario attempts to find all relevant documents for a query, and can require thousands of documents to be returned\nthus, it is likely to require more shards to be searched\na robust shard cutoff prediction method should be effective, stable, and usable for both early precision and high recall search scenarios\nthis paper presents a new, feature-based approach to queryspecific shard cutoff prediction that is easily tuned for early precision or high recall, and can be used in conjunction with any resource selection algorithm\nthe research and experiments presented in this paper are designed to answer the following five research questions\nrq1: how accurate are existing shard cutoff predictions\nrq2: how accurate are existing shard rankings\nrq3: are ranker-independent cutoff predictions effective\nrq4: how do the competing goals of precision-oriented and recall-oriented selective search affect tradeoffs between efficiency and effectiveness\nrq5: is it necessary or useful for the shard cutoff prediction algorithm to be trained for a specific resource selection algorithm\nthe next section reviews prior research on selective search, resource ranking, and measuring the similarity of search results.\nsection 3 describes our new approach to predicting shard cutoffs\nsection 4 describes our experimental methodology and evaluation\nsection 5 reports experimental results. section 6 conclude", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Selective search architectures use resource selection algorithms such as Rank-S or Taily to rank index shards and determine how many to search for a given query\nMost prior research evaluated solutions by their ability to improve efficiency without significantly reducing early-precision metrics such as P@5 and NDCG@10\nThis paper recasts selective search as an early stage of a multistage retrieval architecture, which makes recall-oriented metrics more appropriate\nA new algorithm is presented that predicts the number of shards that must be searched for a given query in order to meet recall-oriented goals\nDecoupling shard ranking from deciding how many shards to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be optimized independently\nExperiments on two corpora demonstrate the value of this approac"},
{"doc": "evaluation is a crucial component of any information retrieval (ir) system [2]\nreusable test collections and off-line evaluation measures [7] have been the dominating paradigm for experimentally validating ir research for the last 30 years\nthe popularity and ubiquity of off-line ir evaluation measures is partly due to the text retrieval conference (trec) [5]\ntrec led to the development of the trec_eval software package that is the standard tool for evaluating a collection of rankings\nthe trec_eval tool allows ir researchers to easily compute a large number of evaluation measures using standardized input and output formats\nfor a document collection, a test collection of queries with query/document relevance information (i.e., qrel) and a set of rankings generated by a particular ir system (i.e., a system run) for the test collection queries, trec_eval outputs a standardized output format containing evaluation measure values\nthe adoption of trec_eval as an integral part of ir research has led to the following benefits\n(a) standardized formats for system rankings and query relevance information such that different research groups can exchange experimental results with minimal communication, and\n(b) open-source reference implementations of evaluation measures\u2014provided by a third party (i.e., nist)\u2014that promotes transparent and consistent evaluation\nwhile the availability of trec_eval has brought many benefits to the ir community, it has the downside that it is available only as a standalone executable that is interfaced by passing files with rankings and ground truth information\nin recent years, the python programming language has risen in popularity due to its feature richness (i.e., scientific libraries and data structures) and holistic language design [3]\nresearch progresses at a rate proportional to the time it takes to implement an idea, and consequently, scripting languages (e.g., python) are preferred over conventional programming languages [6]\nwithin ir research, retrieval systems are often implemented and optimized using python (e.g., [4, 9]) and for their evaluation trec_eval is used\nhowever, invoking trec_eval from python is expensive as it involves (1) serializing the internal ranking structures to disk files, (2) invoking trec_eval through the operating system, and (3) parsing the trec_eval evaluation output from the standard output stream\nthis workflow is unnecessarily inefficient as it incurs (a) a double i/o cost when the ranking is first serialized by the python script and subsequently parsed by trec_eval, and (b) a context-switching overhead as the invocation of trec_eval needs to be processed by the operating system.\nwe introduce pytrec_eval to counter these excessive efficiency costs and avoid a wild growth of ad-hoc python-based evaluation measure implementations\npytrec_eval builds upon the trec_eval source code and exposes a python-first interface to the trec_eval evaluation toolkit as a native python extension\nrankings constructed in python can directly be passed to the evaluation procedure, without incurring disk i/o costs; evaluation is performed using the original trec_eval implementation.\ndue to pytrec_eval\u2019s implementation as a native python extension, context-switching overheads are avoided as the evaluation procedure and its invocation reside within the same process\nnext to improved efficiency, pytrec_eval brings the following benefits:\n(a) current and future reference trec_eval implementations of ir evaluation measures are available within python, and\n(b) as the evaluation measures are implemented in c, their execution are typically faster than native python-based alternatives\nthe main purpose of this paper is to describe pytrec_eval, provide empirical evidence of the speedup that pytrec_eval delivers, and showcase the use of pytrec_eval in a reinforcement learning application\n we ask the following questions\n(rq1) what speedup do we obtain when using pytrec_eval over trec_eval (serialize-invoke-parse workflow)?\n(rq2) how fast is pytrec_eval compared to native python implementations of ir evaluation measures?\nwe also present a demo application that combines pyndri [9] and pytrec_eval in a query formulation reinforcement learning setting and provide the environment and the reward signal, integrated within the openai gym [1", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We introduce pytrec_eval, a Python interface to the trec_eval information retrieval evaluation toolkit\npytrec_eval exposes the reference implementations of trec_eval within Python as a native extension\nWe show that pytrec_eval is around one order of magnitude faster than invoking trec_eval as a sub process from within Python.\nCompared to a native Python implementation of NDCG, pytrec_eval is twice as fast for practically-sized rankings\nFinally, we demonstrate its effectiveness in an application where pytrec_eval is combined with Pyndri and the OpenAI Gym where query expansion is learned using Q-learnin"},
{"doc": "with huge amounts of data and large query volumes, search engines consume significant resources, both in terms of computer hardware and energy usage.\nthe end user sees fast queries because the dataset is partitioned across many machines\nhowever, resource costs are still present, so any improvement in efficiency will give significant cost savings\nin this paper, we present various optimization techniques for search execution that maintain the ranking effectiveness of the system, so called rank-safe execution.\nthe details of executing queries in a search system are complex, but the basic idea is simple.\nat indexing time, the data is inverted to form a postings list for each token containing the locations of that token in the dataset\neach list is stored in a compressed format and often in document identifier (docid) order\nwithin-document locations are often dropped and only \"docid, frequency\" pairs are stored\nat query time, the system finds the lists for the query terms and combines them to give the top-k best documents according to a specified ranking algorithm\nthe standard approach merges lists ordered by docid, resulting in document-at-a-time execution that is fast and requires little temporary memory [5]\nduring list merging, each iteration of the search execution loop will select the next candidate document, score the candidate, and possibly save it in a top-k heap, as depicted in figure 1 (ignoring the green italic writing for now)\nthe exhaustive-or method considers all documents in all the query lists (disjunctive merging), resulting in slow queries, but rank-safe results since every candidate is scored\nthe exhaustive-and method considers only documents contained in all the query lists (conjunctive merging), resulting in fast queries, but non-rank-safe results since high-scoring documents missing one or more terms are not found\nthe wand (weak-and) [4] and state-of-the-art bmw (block- max wand) [10] approaches start executing as an or query and transition towards an and as intermediate results improve\nthe details of this transition produce both fast query execution and rank-safe results\nthe wand approach uses the current top-k results stored in the heap to give a threshold allowing more efficient skipping of potential candidates during select processing\nthis is done by ordering the query lists by their current docid and summing up their maximum list scores \u2014in order\u2014 until it exceeds the threshold at the pivot docid; then the smallest list before the pivot is advanced, and a new pivot is calculated, repeating until enough lists are on the same pivot docid to score the candidate\n the bmw approach adds maximum scores for each encoded list block in the index, and uses them to prune candidates on each pivot calculation\nthus, both approaches change select processing to reduce candidates sent to the score stage.\nthe required additions to the basic search execution loop are marked in figure 1 (green italic writing).\nwe start with a recent wand and bmw implementation using quantized scores and incremental scoring for fast execution [6]\nwe then improve upon this original code in three ways:\nfirst, we apply small code optimizations to both the wand and bmw implementations that give significant runtime improvements.  \nsecond, we improve the query runtime of the wand and bmw algorithms for large k values by starting at an initial threshold generated from indexing time analysis of the query lists.\nthird, we show that split-lists (i.e., 2-layer lists split by score) can significantly improve wand performance, making it faster than the state-of-the-art bmw approach for many configurations\nwhile the 2-layer approach has been used with bmw [9, 10], the 2-layer split-list wand approach has not been previously examine", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We examine search engine performance for rank-safe query execution using the WAND and state-of-the-art BMW algorithms\nSupported by extensive experiments, we suggest two approaches to improve query performance: initial list thresholds should be used when k values are large, and our split-list WAND approach should be used instead of the normalWAND or BMW approaches\nWe also recommend that reranking-based distributed systems use smaller k values when selecting the results to return from each partitio"},
{"doc": "the term ontology refers to \"a formal, explicit specification of a shared conceptualization\" [9]\nalthough there is a great deal of recent researches on ontology construction [8], much less progress has been made with regard to ontology evaluation\nas a result, there are few commonly agreed-upon methodologies and metrics for evaluating ontology complexity.\nthe entropy can quantify the amount of lexical information included in the ontology\nin this regard, each ontology can be considered as a graph or a network, the entropy rate of which is a measure of the complexity of the graph [12].\nthe information density is operationalized based on the normalized entropy measured between all concept pairs in the ontology\na higher entropy value signifies a higher redundant ontology\ncurrent entropy applications used to evaluate ontology have three limitations, in that they\nexclusively consider single point connectivity rather than paths [5]: an ontology is an undirected graph with edges that connect unordered pairs of vertices\nconnectivity that depends directly on single point connectivity neglects information pertaining to non-adjacent nodes and is, thus, considered the weakest means of measuring network connectivity.\nassign equal weights to edges and paths [1]\nthe equal weights assignment is unrealistic and can induce a loss of diversity\nconsidering a node in a graph with several paths of equal weight will result in an incorrect path search, because the lack of path differences leaves each other path with no probability mass.\nassume vertices are static\nontologies are wildly used to real-world applications, in where vertexes usually attach more than one meaning and hence demonstrate various aspects when interacting with different neighboring vertices [2].\nfor instance, in the field of medicine, it is necessary to consider vertices as dynamic, because interactions and weights between nodes vary greatly among different diagnosis and treatment scenarios.\nto address these three limitations, the present study describes a path-based text-aware entropy computation method (ptec) by comparing its information densities to those of other ontologies\n  to this end, we utilize path information between different vertices in the ontology and textual information included in the path to calculate different weights between various nodes and the connectivity path of the entire network\nspecifically, we apply cnn to learn two types of embeddings for a vertex: structure-based embedding and text-based embedding.\ngiven a vertex u and its neighboring vertex v, the embedding of u differs with different neighbors; and, when u interacts with v, the embeddings of u and v are derived from each vertex\u2019s textual information, respectively\nsubsequently, the information gain, which is in the form of a matrix obtained by a cosine similarity calculation of the relevancy between nodes u and v, is multiplied by the connectivity matrix of entropy computations.\nbecause ontological redundancy is mainly manifested in loose structures and lengthy textual information, we experimentally evaluated three infectious disease-relevant ontologies\nthe ontology statistical information and textual attention visualization is used as a reference to evaluate the validity of the calculation.\nthis paper makes several novel contributions.\n(1)we evaluate an ontology by adjusting the connectivity matrix of entropy computations in consideration of both path information and text information within neural network architectures, overcoming the limitations of single point connectivity and the equal weighting problem\n(2) to solve the unrealistic assumption of traditional ontology computations, in which each vertex is represented as a static embedding vector, we consider diverse interactions between nodes by adapting mutual attention to emphasize those words that are focused by its neighbor vertices\n(3) based on experiments utilizing real-world ontologies, the method proposed herein achieves better performance than existing methods in evaluating ontologies", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "With the rising importance of knowledge exchange, ontologies have become a key technology in the development of shared knowledge models for semantic-driven applications, such as knowledge interchange and semantic integration\nSignificant progress has been made in the use of entropy to measure the predictability and redundancy of knowledge bases, particularly ontologies\nHowever, the current entropy applications used to evaluate ontologies consider only single-point connectivity rather than path connectivity, assign equal weights to each entity and path, and assume that vertices are static\nTo address these deficiencies, the present study proposes a Path-based Text-aware Entropy Computation method, PTEC, by considering the path information between different vertices and the textual information within the path to calculate the connectivity path of the whole network and the different weights between various nodes\nInformation obtained from structure-based embedding and text-based embedding is multiplied by the connectivity matrix of the entropy computation\nAn experimental evaluation of three real-world ontologies is performed based on ontology statistical information (data quantity), entropy evaluation (data quality), and a case study (ontology structure and text visualization)\nThese aspects mutually demonstrate the reliability of our method.\nExperimental results demonstrate that PTEC can effectively evaluate ontologies, particularly those in the medical fiel"},
{"doc": "reinforcement learning, including the multi-armed bandit(mab) [7] and markov decision process(mdp) [5], have been successfully used in variant machine learning applications recently\namong the algorithms that solve the reinforcement learning problems, policy gradient [13] has shown its advantages in effectiveness in highdimensional/ continuous action spaces, fast convergence rate, and handling stochastic policies etc\nroughly speaking, policy gradient relies upon optimizing parametrized policies (a distribution over the agent actions) with respect to the expected return (long-term cumulative reward) by gradient ascent\nto calculate the parameter gradients at each optimization iteration, policy gradient algorithms such as reinforce [12, 13] usually adopt the monte carlo method [9] to estimate the expectation of the gradient\nthe gradient estimated by the monte carlo method is unbiased but usually has large variance, which hurts the efficiency and effectiveness of the traditional policy gradient algorithm\nhow to reduce the variance of the estimated gradient becomes a key issue in policy gradient algorithms\na number of research has been conducted to reduce the gradient variance in policy gradient\nfor example, policy gradient with baseline [3] is commonly used in real reinforcement learning tasks\nin the method, a baseline variate, which is designed as the averaged rewards of the history steps, is first designed\nthen, the real reward of the action minus the baseline is used as the reward for the gradient estimation and parameter updating\nit also shows that variance of the newly estimated gradients is reduced while its expectation is identical to that of the traditional policy gradient\nmore methods on variance reduction for policy gradient please referred to [1, 8, 11]\nfrom the viewpoint of statistics, the policy gradient with baseline is a direct application of control variates method [2], a variance reduction approach in monte carlo method, to improve the traditional policy gradient\nthe baselines are implementations of the control variates in the reinforcement learning environment\nin general, designing reliable control variates is critical for the success of control variates method\ninappropriate setting of the control variates may result in the raising of variance and hurt the estimation\nwhen applied to reinforcement learning, though the policy gradient with baseline heuristically constructs the baselines, which is far away from the ideal control variates, it is difficult to achieve its optimal effect\nto get rid of this problem, antithetic variates method [4] is proposed\nevery time antithetic variates method draws a pair of antithetic samples for the estimation\nsince one antithetic sample in the pair is easily derived from another, the auxiliary functions (e.g., the control variates) is not a mandatory anymore\nbased on the observation, we propose a novel policy gradient method which uses antithetic variates to improve policy gradient for mab\nthe proposed method, referred to as antithetic-arm bandit (aab), estimates the parameter gradients through sampling a pair of antithetic arms at each time\nto achieve this, aab adopts the coordinate ascent framework for the optimization where each coordinate corresponds an arm\nat each iteration, the arms are sorted according to their estimated gradients\nafter that, a pair of antithetic arms are sampled on the basis of the sorted arms\nthe gradient of the target arm (determined with another sampling) is then re-estimated and updated\ntheoretical analysis showed that the gradients calculated with aab was an unbiased estimation and the variance of the estimation was effectively reduced with high confidence\nexperiments were conducted to show the effectiveness of the proposed aab\nthe experimental results based on an mab task showed that aab outperformed the baseline of traditional policy gradient and achieved comparable performances with the policy gradient with baselin", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Policy gradient, which makes use of Monte Carlo method to get an unbiased estimation of the parameter gradients, has been widely used in reinforcement learning\nOne key issue in policy gradient is reducing the variance of the estimation\nFrom the viewpoint of statistics, policy gradient with baseline, a successful variance reduction method for policy gradient, directly applies the control variates method, a traditional variance reduction technique used in Monte Carlo, to policy gradient\nOne problem with control variates method is that the quality of estimation heavily depends on the choice of the control variates\nTo address the issue and inspired by the antithetic variates method for variance reduction, we propose to combine the antithetic variates method with traditional policy gradient for the multi-armed bandit problem\nFurthermore, we achieve a new policy gradient algorithm called Antithetic-Arm Bandit (AAB)\nIn AAB, the gradient is estimated through coordinate ascent where at each iteration gradient of the target arm is estimated through: 1) constructing a sequence of arms which is approximately monotonic in terms of estimated gradients, 2) sampling a pair of antithetic arms over the sequence, and 3) re-estimating the target gradient based on the sampled pair\nTheoretical analysis proved that AAB achieved an unbiased and variance reduced estimation\nExperimental results based on a multi-armed bandit task showed that AAB can achieve state-of-the-art performance"},
{"doc": "many organizations in business, economics or information technology operate in a multi-item, multi-level environment\ntime series data representing the behaviors of these organizations can be often organized in a hierarchical (tree) structure where different time series interact and influence each other.\nthese related multivariate time series are referred to in the literature as hierarchical time series (hts) [5].\nindividual time series within the same hierarchy not only interact and correlate with each other, but they often satisfy additional constraints imposed by the hierarchical structure.\nmany internet companies, such as popular social network and web portal sites collect web traffic and web page views (pvs) data that are naturally organized in a hierarchy\nfor example, web pages that are linked together often follow a hierarchical structure with the main company web site at the root and other web pages covering different aspects or functions of the company\nthese time series are often related in time\nmodeling and forecasting of web traffic with page hierarchy is important for the core business\nintuitively, a higher volume web page category usually indicates a series of more profitable online ads placements.\nin general, hts that include related time series do not have to satisfy the equality constrains across the different hierarchical levels\nfor example the counts of web page views on a lower level of the web site hierarchy are not expected to be equal to the counts of web page views made at the parent level.\nin a website hierarchy, each time series represents the volume of daily traffics originating from the corresponding web page\ninternet users might access the child web page directly without accessing the parent page, which breaks down the inclusion assumption (the observations in the parent level are strictly equal to the sum of observations of their children) and leads to the inequality constraints in the hierarchy.\nin this work, we refer to hts with equality constraints as type i hts and hts with inequality constraints as type ii hts\neven though various forecasting models are proposed for individual time series and type i hts, type ii hts forecasting in web traffic domain still remain unsolved and challenging due to\n(1) inequality constraints which require that forecasts made at each time stamp have to incorporate hierarchical implicit inequality information to improve the prediction performance;\nand (2) seasonal patterns for each individual web traffic\nweb traffics usually exhibit seasonal patterns and sometimes are the additive results of multiple seasonal patterns based upon human activity cycles.\nin this paper, we address the above modeling issues by presenting a novel and flexible hierarchical forecasting framework (hf) to make accurate hierarchical prediction of type ii hts.\nour approach first converts the type ii hts to type i hts by introducing leak time series and goes through multiple rounds of seasonality decomposition to remove seasonal cycles\nonce done, it builds individual forecasting models on each time series without seasonality (which is referred as \"trend\" later in section 4)\nin the final step, each individual forecasts are hierarchically combined through the hierarchy via certain combination heuristics\nin summary, our approach makes the following contributions: (1) introduce leak time series into each tree unit, which reduces the forecasting problems of type ii hts to the forecasting problem of type i hts.\n(2) hts with multiple seasonal patterns are automatically detected and removed and the framework uses the underlying trend as the forecasting target.\n (3) various individual forecasting models and hierarchical combination heuristics can be incorporated into our framework", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "In this work, we focus on models and analysis of multivariate time series data that are organized in hierarchies\nSuch time series are referred to as hierarchical time series (HTS) and they are very common in business, management, energy consumption, social networks, or web traffic modeling and analysis\nWe propose a new flexible hierarchical forecasting framework, that takes advantage of the hierarchical relational structure to predict individual time series\nOur new forecasting framework is able to (1) handle HTS modeling and forecasting problems\n(2) make accurate forecasting for HTS with seasonal patterns\n(3) incorporate various individual forecasting models and combine heuristics based on the HTS datasets\u2019 own characterization\n The proposed framework is evaluated on a real-world web traffic data set.\nThe results demonstrate that our approach is superior when applied to hierarchical web traffic prediction problems, and it outperforms alternative time series prediction models in terms of accurac"},
{"doc": "we focus on the post-retrieval query performance prediction (qpp) task [4]\ngiven a query, a corpus and a retrieval method that evaluates the query, our goal is to predict the query\u2019s performance based on its retrieved result list [4].\nmotivated by previous works on document retrieval using passage information [1\u20133],we propose to use such information for the post-retrieval qpp task as well\nto this end, we extend kurland et al.\u2019s probabilistic qpp framework [10] and show how passage information may be utilized for an enhanced prediction. \n using an evaluation with several trec corpora, we first demonstrate that, existing state-of-the-art document-level post retrieval qpp methods, are mostly suited for prediction tasks that involve short (and probably more ambiguous) queries; whereas such methods are less suited for prediction tasks that involve verbose (long and probably more informative) queries\nwe next demonstrate that, our proposed qpp method which makes use of passage information provides a more robust prediction, regardless of query type\n we further set a direct connection with roitman et al.\u2019s mean retrieval score estimation framework [12]\nmoreover, by integrating our proposed passage-information qpp signal as an additional calibration feature within roitman et al.\u2019s framework [12], we are able to achieve the overall best qpp accuracy", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We focus on the post-retrieval query performance prediction (QPP) task\nSpecifically, we make a new use of passage information for this task\nUsing such information we derive a new mean score calibration predictor that provides a more accurate prediction\nUsing an empirical evaluation over several common TREC benchmarks, we show that, QPP methods that only make use of document-level features are mostly suited for short query prediction tasks; while such methods perform significantly worse in verbose query prediction settings\nWe further demonstrate that, QPP methods that utilize passage-information are much better suited for verbose settings\nMoreover, our proposed predictor, which utilizes both documentlevel and passage-level features provides a more accurate and consistent prediction for both types of queries\nFinally, we show a connection between our predictor and a recently proposed supervised QPP method, which results in an enhanced predictio"},
{"doc": "measurement of the quality of search techniques is usually done in one of two ways:\nvia the application of an effectiveness metric to the rankings (serps) generated in response to a set of queries, and aggregation or statistical testing over the set of numeric scores that result; or via user satisfaction studies that elicit feedback from people using the search service\nin the first of these, the choice of metric is a key consideration \u2013 even though metrics scores are usually correlated, different metrics reflect different assumptions in regard to user behavior, and hence choosing a metric is tantamount to choosing a user model\nfor example, rr, where the score assigned is the reciprocal of the rank at which the first relevant document appears \u2013 corresponds to a model in which the user searches from the top of the serp, and stops as soon as they find a first relevant document\nin this framework, the score assigned by the metric is the average rate at which the user gains satisfaction from the serp, measured in units of \u201crelevance gain per document inspected\u201d.\nthe duality between metrics and models was noted by moffat and zobel [8], who proposed a user model (and corresponding metric rank-biased precision, rbp) in which users are assumed to always examine the first document in the serp, and then to continue from one document to the next with a fixed probability \\\\phi , regardless of the relevance already accumulated.\nthat is, in rbp the conditional continuation probability at depth i, denoted c(i), is taken to be constant as the serp is examined\nmoffat and zobel [8] also anticipated the possibility of other options, writing (page 17)\na further variant is to relax the assumption that \\\\phi  [. . . ] is independent of whether or not the document just considered is relevant\nan arrangement in which the conditional probability of advancing given a relevant document is \\\\phi_{1}, and the conditional probability of advancing given an irrelevant document is \\\\phi_{2} [. . . ] would lead to another mechanism for scoring runs [. . . ]\nrecent work has taken up that challenge, with moffat et al. [10] describing a user model (and corresponding metric, inst) in which c(i ) is recomputed at each depth i in the ranking, altering the user model in two critical ways\nfirst, in inst c(i ) increases as a function of i, reflecting that the longer the examination of the serp continues, the more likely it is (in a conditional sense) that the next document will also be examined\nsecond, inst is adaptive in that c(i ) is also modified as relevance is accumulated, with (when other factors are equal) the computed c(i ) at depth i being lower if the serp has already provided a high degree of relevance, reflecting that the closer the user is to satisfying their information need, the smaller their likelihood of continuing to search.\nscores computed by inst are also in units of relevance gain per document inspected, but now the number of documents examined varies depending on the quality of the serp. moffat et al. employ a parameter t , the desired \u201cvolume\u201d of relevance that the searcher anticipated when they commenced their search, and show that inst models users as searching to an expected depth of between t +0.25 documents (when every item in the serp is relevant) and 2t + 0.5 documents (when nothing in the serp is relevant).\nin suggesting this arrangement, moffat et al. implicitly assert that the user is more persistent on low-quality rankings than on high-quality ones\n for example, a user who is nominally searching for t = 2 units of relevance, and at some depth has accumulated three units of relevance already, is less likely to continue to the next document in the serp than a user who has not yet accumulated any relevance.\nin other recent work, jiang and allan [7] develop a different approach\nthey also suggest that the user\u2019s reaction to each serp is influenced by the documents that appear in it, and seek to modify the user model embedded in rbp (and other weighted-precision metrics) to account for that behavior\nbut (in the context of rbp) their approach makes use of a constant c(i ) = \\\\phi  function, with \\\\phi  computed not according to characteristics of the user and their inherent understanding of what they are seeking, but instead based on characteristics of the documents in the serp.\nby fitting coefficients to eye-tracking and click-through data derived from user interactions, jiang and allan suggest that \\\\phi  (and similarly, the parameter k that governs the evaluation depth of the sdcg@k metric) be adjusted so that users faced with a low quality serp examine fewer documents, thereby accounting for bad abandonment [5, 6]\nthat is, jiang and allan require that a smaller value of \u03d5 be employed when the serp contains few or no relevant documents, and a larger one be used when the serp contains a high fraction of relevant documents\nnote that this corresponds to viewing behavior that is the opposite of what is suggested by moffat et al. [10]\nour contribution\nwe first examine the implications of the jiang and allan approach, examining the scores that are generated for a range of serps\nwe then draw these two different models of user behavior into a cohesive whole that captures both the user behaviors observed by jiang and allan and also embeds the fully adaptive approach espoused by moffat et al.\nin particular, we provide an enhanced mechanism with the following properties\nall other aspects being equal, c(i ) increases with i.\nall other aspects being equal, c(i ) is smaller at any depth i if a larger volume of relevance has been accumulated from documents 1 to i.\nall other aspects being equal, c(i ) is smaller at any depth i if a larger number of egregiously non-relevant responses have been encountered from documents 1 to i\nas is suggested by the third factor, we achieve this blend by partitioning non-relevant documents into two categories: those that are plausibly non-relevant, and those that are egregiously non-relevant.\n for example, if the query were \u201cthe melting point of lead\u201d, we would regard a document that described the \u201cmelting point of tin\u201d as being plausibly non-relevant, and would feel encouraged that a relevant document might appear soon in the ranking\nconversely, a document that described the role of \u201cfreddie mercury as the lead singer of the band queen\u201d could suggest abandoning that particular serp, and indicate that reformulation might be required \u2013 it would fall into the \u201cdiscouraging\u201d, or egregiously non-relevant category.\nthat is, we propose a categorization in which non-relevant documents are of two distinct types: those that are regarded as being encouraging of eventual success, and those that are discouragin", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We consider two recent proposals for effectiveness metrics that have been argued to be adaptive, those of Moffat et al. (ACM TOIS, 2017) and Jiang and Allan (CIKM, 2017), and consider the user interaction models that they give rise to\nBy categorizing non-relevant documents into those that are plausibly non-relevant and those that are egregiously non-relevant, we capture all of the attributes incorporated into the two proposals, and hence develop an effectiveness metric that better reflects user behavior when viewing the SERP, including bad abandonmen"},
{"doc": "ranking question answer pairs, also known as answer selection, has become increasingly important in a variety of qa such as community-based question answering (cqa) and factoid question answering\ngiven a question, answer selection aims to pick out the most relevant answer from a set of candidates\ninspired by the recent successes of deep learning in natural language processing, the majority of literature employed deep neural networks, e.g., convolutional neural network (cnn) [6] or recurrent neural network (rnn) [11], to automatically select answers.\nthe key idea behind deep neural networks is to encode the input sentences as vector representations\nbased on the representations, an output layer is utilized to provide the matching score of two texts.\ninstead of learning the representations of the question and the answer separately, some recent studies exploit attention mechanisms to learn the interaction information between questions and answers, which can better focus on relevant parts of the input [2, 4, 7].\ndespite the effectiveness of previous studies, ranking question answer pairs in real-world remains a challenge.\n(i) first, the background knowledge from open-domain knowledge graphs (kgs) plays a crucial role in question answering\nconsidering the example in table 1, existing context-based models may assign a higher score to the negative answer than the positive answer, since the negative answer is more similar to the given question at word level.\nhowever, with the background knowledge, we can correctly identify the positive answer based on the relative facts contained in the kg such as (pokemon, owned_by, nintendo), (pokemon, created_by, satoshi tajiri) and even (pokemon, created_in, 1996)\ndespite its usefulness, to our best knowledge, the background knowledge from kgs receives little attention in recent neural network models to rank question answer pairs [5, 9, 10]\n(ii) in addition, the issues of redundancy and noise prevalent in real-world applications (e.g., cqa) are remained to be settled.\nhowever, previous researches [3, 13] leverage external knowledge from kg to exclusively conduct the knowledge-aware learning of individual sentence rather than capture the interrelations between different sentences, which is important for ranking question answer pairs.\nto alleviate these limitations, we propose a knowledge-aware attentive neural network to interactively learn knowledge-based sentence representations and context-based sentence representations for ranking qa pairs\nin specific, we first employ knowledge embedding methods to pre-train the knowledge embeddings from kg.\nthen, we design a context-guided attentive cnn to learn the knowledge-based sentence representation from discrete candidate entity embeddings in kg.\nfinally, we present a knowledge-aware attention mechanism to learn knowledge-aware sentence representations of questions and answers, which adaptively decides the important information of questions and answers based on both the context and the background knowledge\nthe main contributions of this paper can be summarized as follows:\n(1) we propose a novel deep learning model, knowledgeaware attentive bi-lstm (kablstm), which leverages external knowledge from kg to capture background information of the questions and answers for ranking qa pairs;\n(2) we develop a context-knowledge interactive learning architecture, which exploits the interactive information from input texts and kg to supervise the representational learning of both sentences and external knowledge;\n(3) the experimental results show that kablstm consistently outperforms the state-of-the-art method", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Ranking question answer pairs has attracted increasing attention recently due to its broad applications such as information retrieval and question answering (QA)\nSignificant progresses have been made by deep neural networks\nHowever, background information and hidden relations beyond the context, which play crucial roles in human text comprehension, have received little attention in recent deep neural networks that achieve the state of the art in ranking QA pairs.\nIn the paper, we propose KABLSTM, a Knowledge-aware Attentive Bidirectional Long Short-Term Memory, which leverages external knowledge from knowledge graphs (KG) to enrich the representational learning of QA sentences\nSpecifically, we develop a context-knowledge interactive learning architecture, in which a context-guided attentive convolutional neural network (CNN) is designed to integrate knowledge embeddings into sentence representations\nBesides, a knowledge-aware attention mechanism is presented to attend interrelations between each segments of QA pairs.\nKABLSTM is evaluated on two widely-used benchmark QA datasets: WikiQA and TREC QA.\nExperiment results demonstrate that KABLSTM has robust superiority over competitors and sets state-of-the-ar"},
{"doc": "in recent years, personalized recommendation has attracted much attention from both research community and industry\n by considering users\u2019 preferences, recommendation systems will have more chance to attract users\nmany researchers have found that it\u2019s very beneficial for personalized recommendation systems to give explanations\ngiven reasonable explanations, users are more likely to buy or try.\nfurthermore, explanations will help convincing users that the system knows them very well and makes custom-made recommendations for them.\nthere have been plenty of techniques such as content based algorithms proposed to address this explainability problem\n besides, some review-aware methods [14] based on sentiment analysis have been newly proposed with good performance on some datasets.\nhowever, these methods heavily rely on both quality and quantity of users\u2019 reviews.\nbesides, there are some users unlikely to show personal preferences in their reviews, which makes it tough for review-aware methods to capture users\u2019 preferences.\nlatent factor models such as matrix factorization (mf) [8] have become very popular and welcomed by the research community and industry\n by representing features with latent vectors, it becomes very convenient to integrate various item features by adding or concatenating\n item features are usually easy to collect and very helpful, because users make decisions generally based on item features\n with many advantages, lfms reach high prediction accuracy on some benchmark datasets.\nhowever, lfms also encounter some problems in personalized recommendation\non one hand, when conducting interactions between latent vectors of users and items, lfms ignore how users make decisions according to their preference for various item features, which makes lfms do it the same way for all users.\non the other hand, due to the latent representation, the lack of explainability weakens the ability of a personalized recommendation system to gain users\u2019 trusts.\nto address the challenges above, we develop a novel cf-based model named attention-driven factor model (afm) as a general framework for personalized recommendation, which makes recommendations according to users\u2019 attention on different aspects of the item\nfor estimating the users\u2019 preferences via ratings and item features, we propose the gated attention units (gaus) for afm to generate attention distributions for different users, as figure 1 shows\n with users\u2019 attention distributions, afm can reach a high prediction accuracy and give feature-level explanations for users\u2019 preferences.\nthe main contributions of this work are summarized into three folds:\n1) by considering users\u2019 preferences, we develop a general framework afm for explainable personalized recommendation, which can give reasonable explanations for users\u2019 preferences and keep a high prediction accuracy.\n2) we propose the gated attention units(gaus) to extract explicit users\u2019 preferences from latent representations\n 3) we perform experiments on several real-world datasets to demonstrate the effectiveness and explainability of af", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Latent Factor Models (LFMs) based on Collaborative Filtering (CF) have been widely applied in many recommendation systems, due to their good performance of prediction accuracy\nIn addition to users\u2019 ratings, auxiliary information such as item features is often used to improve performance, especially when ratings are very sparse\nTo the best of our knowledge, most existing LFMs integrate different item features in the same way for all users.\nNevertheless, the attention on different item attributes varies a lot from user to user\nFor personalized recommendation, it is valuable to know what feature of an item a user cares most about\nBesides, the latent vectors used to represent users or items in LFMs have few explicit meanings, which makes it difficult to explain why an item is recommended to a specific user.\nIn this work, we propose the Attention-driven Factor Model (AFM), which can not only integrate item features driven by users\u2019 attention but also help answer this \"why\"\nTo estimate users\u2019 attention distributions on different item features, we propose the Gated Attention Units (GAUs) for AFM.\nThe GAUs make it possible to let the latent factors \"talk\", by generating user attention distributions from user latent vectors\nWith users\u2019 attention distributions, we can tune the weights of item features for different users\nMoreover, users\u2019 attention distributions can also serve as explanations for our recommendations\nExperiments on several real-world datasets demonstrate the advantages of AFM (using GAUs) over competitive baseline algorithms on rating predictio"},
{"doc": "in information retrieval (ir), effectiveness evaluation is an essential process.\na widely-adopted methodology is evaluation by means of a test collection, which consists of: a set of documents, a test suite of information needs descriptions (called queries or topics), and the ground-truth of a set of relevance judgements made by experts for a subset of the topic-document pairs\ncampaigns such as trec, ntcir, fire, clef, inex, etc. evaluate effectiveness of systems by comparing their output with the ground-truth, using standard or custom evaluation metrics, often several of them working in different configurations\nto facilitate the entire evaluation process, ad-hoc software tools have been created\ntrec_eval is probably the most common: it is used to evaluate the results of the participants to trec competitions, as well as in other initiatives\n trec_eval serves well its purpose but it is not free from limitations.\nin this paper, we present a more general framework that aims to extend it, as well as similar ir evaluation software and tool-kits.\nour system, called irevaloo, is conceived according to the object- oriented (oo) programming paradigm and, more precisely, it is an oo framework\nbesides allowing system evaluations as trec_eval, by exploiting the advantages of oo frameworks, irevaloo offers a set of useful features: the easy implementation of new custom evaluation metrics, the management of multiple types of measurement scales, the handling of different input formats, and the customization of measurement sessions and results visualisation\nthe paper is structured as follows: section 2 describes trec_eval and other evaluation tools, as well as object oriented frameworks, section 3 presents irevaloo and its evaluation, section 4 concludes the pape", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We propose IRevalOO, a flexible Object Oriented framework that (i) can be used as-is as a replacement of the widely adopted trec_eval software, and (ii) can be easily extended (or \u201cinstantiated\u201d, in framework terminology) to implement different scenarios of test collection based retrieval evaluation\nInstances of IRevalOO can provide a usable and convenient alternative to the state-of-the-art software commonly used by different initiatives (TREC, NTCIR, CLEF, FIRE, etc.)\nAlso, those instances can be easily adapted to satisfy future customization needs of researchers, as: implementing and experimenting with new metrics, even based on new notions of relevance; using different formats for system output and \u201cqrels\u201d; and in general visualizing, comparing, and managing retrieval evaluation result"},
{"doc": "algorithmic processes that are opaque to users (and researchers) are having increasing influence on users\u2019 access to information\n in the case of search, these processes also influence users\u2019 view of the information landscape and highly ranked results can shape public opinion [6, 11]\nthis is compounded by the trust users show in the results of search engines; often seen as objective and truthful [14].\nmore recently, studies have been undertaken to understand the impact of users\u2019 beliefs and unconscious biases on their search interactions and experiences [11, 18]\nthis paper continues this line of investigation, focusing on user bias within image search that can manifest itself through the decisions people make about the items they select (e.g., perceived quality and relevance), the queries they pose, etc.\nin particular, we seek to understand how user biases in the form of sexism, an area receiving less attention, shape peoples\u2019 views of image search and can reinforce gender stereotype", "labels": "1\n1\n1\n1\n1\n1\n1", "summaries": "There is growing evidence that search engines produce results that are socially biased, reinforcing a view of the world that aligns with prevalent social stereotypes\n One means to promote greater transparency of search algorithms - which are typically complex and proprietary - is to raise user awareness of biased result sets\nHowever, to date, little is known concerning how users perceive bias in search results, and the degree to which their perceptions differ and/or might be predicted based on user attributes\nOne particular area of search that has recently gained attention, and forms the focus of this study, is image retrieval and gender bias\nWe conduct a controlled experiment via crowdsourcing using participants recruited from three countries to measure the extent to which workers perceive a given image results set to be subjective or objective\nDemographic information about the workers, along with measures of sexism, are gathered and analysed to investigate whether (gender) biases in the image search results can be detected\nAmongst other findings, the results confirm that sexist people are less likely to detect and report gender biases in image search result"},
{"doc": "in social image platforms like flickr and instagram, users may annotate an image with tags as well as add comments related to multiple aspects of an image\nin particular, more than 90% of images in nuswide dataset has received at least one comment [2]\na subset of these comments may serve as a potential source of important information about the image\nhowever, these comments are often riddled with noise and irrelevant chatter, making it hard for any automated technique to correlate them with the visual content or context of an image\nconsider figure 1 depicting an image from flickr along with original comments and tags in figures 1(a) and 1(b), respectively\nwe can make the following key observations.\n(a) only a subset of the tags (e.g., fisheye, breakdancing) is interesting to a typical user as highlighted by the comments\ntags such as d200 and 2006 are not brought up in discussions, indicating that they are of little interest to viewers\nfurthermore, some of the comments are not relevant to the visual content of the image (e.g., \u201cfrom the samples you can see the scientific answer is pretty darned wide\u201d).\n(b) based on the discussion in comments, the visual concepts that capture users\u2019 attention include the visual effect (e.g., fisheye) demonstrated in the photo, the scene captured by the photo (e.g., breakdancing), and the emotional effect arise from viewing the image (e.g., funny)\ngiven such disparate collections of tags and comments, how can we identify and rank them according to their relevance to the visual content of the image\n we believe that the answer to this question benefits several applications in social image search, particularly in building superior image ranking model and search result snippet generation\nconsequently, in this paper, given an image we leverage on its visual features, user comments, and tags to concurrently rank tags and comments according to their relevance to the visual content of the image\n specifically, we aim to simultaneously answer the following two questions\n(a) which visual concepts (represented by tags) in an image capture most users\u2019 attention and discussion\n(b) which are the representative comments from users\u2019 discussion reflecting these concepts?\nhere, a visual concept refers to a concrete visual object or scene (e.g., cat, beach), a visual effect that perceived by many users (e.g., fisheye, macro), or an emotional effect arise from viewing the image (e.g., funny, scary)\nstate-of-the-art tag ranking method rely on the visual and semantic similarities between tags to deduce the ranking among tags [7, 13]\nin recent times, deep learning techniques have been employed for tag ranking and recommendation [3, 10]\nhowever, all these techniques rank tags without leveraging the rich information hidden in users\u2019 comments\nthe goal of our research is to concurrently rank comments and tags associated with a social image, paving the way to identify most relevant comments and tags associated with an image\nwe present a novel visual signature-based model for jointly ranking tags and comments\nthe model not only incorporates the semantic and visual properties of the tags associated with a social image, but also evaluates the user comments to generate superior quality results\na distinguishing feature of our model is that it is lightweight in nature\nspecifically, it produces superior ranking without leveraging on deep learning (and expensive training process).\nby applying our proposed technique to real-world flickr images, we show its effectiveness and significant improvement of performance over existing methods that rely only on visual and semantic properties of tags and image", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "User-generated comments and tags can reveal important visual concepts associated with an image in Flickr.\nHowever, due to the inherent noisiness of the metadata, not all user tags are necessarily descriptive of the image\nLikewise, comments may contain spam or chatter that are irrelevant to the image\nHence, identifying and ranking relevant tags and comments can boost applications such as tag-based image search, tag recommendation, etc\nIn this paper, we present a lightweight visual signature-based model to concurrently generate ranked lists of comments and tags of a social image based on their joint relevance to the visual features, user comments, and user tags\nThe proposed model is based on sparse reconstruction of the visual content of an image using its tags and comments\nThrough empirical study on Flickr dataset, we demonstrate the effectiveness and superiority of the proposed technique against state-of-the-art tag ranking and refinement technique"},
{"doc": "development of image retrieval, especially of fine-grained image retrieval is more or less impeded by the problem of missing labeled data due to increasing annotation costs\nas zero-shot learning (zsl) realizes image classification of certain classes which have no labeled training samples, it has been drawing much attention in recent years [5]\nbut task of zsl is difficult because it lacks labeled training samples of some classes, called unseen classes, to directly train classifiers\njust given labeled training samples of seen classes, zsl aims to achieve unseen classes recognition by building relationship between unseen classes and seen classes.\nnowadays, there are mainly two popular methods for zsl: probability reasoning based on attribute prediction [4] and feature projection among different embeddings [6]\nthe first method usually predicts attribute probability to calculate class maximum likelihood while the second method mainly bridges different embedding spaces to exploit space projection and feature mapping for zsl\nhowever, existing methods have several flaws.\non the one hand, there is inherent error accumulation in probabilistic reasoning\non the other hand, most embedding methods apply complex deep network to realize space projection, which is widely believed to have little interpretability of projection process and takes a lot of time to train the network\ndifferent from existing two types of methods, we propose a new type of method for zsl: sample construction\nour proposed method, imagination based sample construction (ibsc), is based on human associative cognition process to directly construct samples of unseen classes.\nthus, unseen class recognition can be realized via learning from the constructed samples.\nhuman can visualize unseen objects through referring some already known objects and assembling their visual components based on imagination [7]\na human, who never see a tiger before but has seen some cats yet, can speculate the species at the first sight of a real tiger if he knows the description of tiger or attribute relationship between tiger and cat\nby mimicking human associative cognition process, we construct samples of unseen classes from samples of seen classes in feature space, based on a relationship between feature and attribute\nour proposed method is schematically displayed in figure 1\nas shown in figure 1, each attribute is related to specific dimensions of image feature.\nfor example, samples which don\u2019t have attribute \u201cpaws\u201d are different from samples with \u201cpaws\u201d in certain feature dimensions\nbased on attribute-feature relation, an image feature can be reconstructed from other samples to express different attributes\nif use feature dimension related to \u201cpaws\u201d to replace original feature dimension of samples without \u201cpaws\u201d, the reconstructed samples will have a new characteristic.\ngenerally, it is reasonable to choose seen classes with large similarity to unseen classes as reference basis when constructing target samples of unseen classes\nafter samples are constructed through splicing different samples of seen classes, the constructed samples of higher quality need to be picked out\nhence, we adopt the idea about dissimilarity representation [3] to measure representativeness of the constructed samples\nalthough our method is designed to classify images of unseen classes, no more new classifiers need constructing, as zsl has been simplified into a traditional supervised classification problem where most existing classifiers can be used\nwe experiment on four benchmark datasets.\ncompared with state-of-the-art approaches, comprehensive results demonstrate the superiority of our proposed method\nfurthermore, our work can be viewed as a baseline for future sample construction works for zs", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Zero-shot learning (ZSL) which aims to recognize unseen classes with no labeled training sample, efficiently tackles the problem of missing labeled data in image retrieval\nNowadays there are mainly two types of popular methods for ZSL to recognize images of unseen classes: probabilistic reasoning and feature projection\nDifferent from these existing types of methods, we propose a new method: sample construction to deal with the problem of ZSL\nOur proposed method, called Imagination Based Sample Construction (IBSC), innovatively constructs image samples of target classes in feature space by mimicking human associative cognition process\nBased on an association between attribute and feature, target samples are constructed from different parts of various samples\nFurthermore, dissimilarity representation is employed to select high-quality constructed samples which are used as labeled data to train a specific classifier for those unseen classes\nIn this way, zero-shot learning is turned into a supervised learning problem\nAs far as we know, it is the first work to construct samples for ZSL thus, our work is viewed as a baseline for future sample construction methods\nExperiments on four benchmark datasets show the superiority of our proposed metho"},
{"doc": "a technology-assisted review (tar) aims at locating all relevant documents in a collection (\u201ctotal recall\u201d) while minimizing manual reviewing effort\ntar has been successfully applied in a variety of high-recall tasks such as conducting systematic reviews in evidence-based medicine [8], electronic discovery in the legal proceedings [1], creating test collections for information retrieval (ir) evaluation [10]\no\u2019mara-eves [8] provides a detailed survey of machine learning methods used in tar\nactive learning techniques, which iteratively improve the accuracy of the predictions through interaction with reviewers, achieve state-of-the-art performance\nin particular, cormack and grossman [1, 2] have proposed the baseline model implementation (bmi), a continuous active learning (cal) algorithm, which has been evaluated in a number of high-recall tasks as the best performing algorithm [5, 7]\nbmi identifies an initial set of documents to be reviewed by experts to be used as an initial training set for learning a logistic regression model.\nthe logistic regression algorithm predicts the relevance of the remaining of the documents\na set of top-scored documents is returned to assessors for labeling\nthe labeled documents are added back to the initial training set and the model is being retrained\nwhile cal algorithms have demonstrated their ability to efficiently find relevant documents in a collection [1, 6], recall typically reaches a plateau of 80%-90% after reviewing and labeling 30%-40% of the collection [7]\nfinding the last few relevant documents requires reviewing almost the entire collection\nthe goal of this work is to efficiently retrieve these last few relevant documents\nour hypothesis is that asking direct questions to reviewers will allow an algorithm to discover the missing documents faster than when requesting relevance feedback on documents through continuous active learning.\nhence, we propose a sequential bayesian search [11] based method (sbstar), which locates the missing relevant documents efficiently by directly querying reviewers about significant pieces of information expected to appear, or not, in the relevant documents\nour framework applies cal up to a certain level of effort, in terms of documents reviewed\nthen it switches to sbstar to directly ask questions to reviewers\n sbstar first identifies a pool of questions to be asked.\nin this work we focus on questions about the expected presence of an entity in the missing relevant documents\nhence, entities found in the corpus constitute the pool of available questions\nsbstar then constructs a prior belief over document relevance on the basis of the ranking model trained by cal\nthen, it applies generalized binary search (gbs) over entities to find the entity that dichotomizes the probability mass of document relevance.\nafter each question is being answered by the reviewer a posterior belief is obtained to be used for the selection of the next question\nthe main contribution of this paper is two-fold\n(1) a method to construct a set of questions to be asked to the reviewers in terms of entities contained in the documents of the collection\n (2) a novel interactive method, which directly queries reviewers about the expected presence of an entity in relevant documents, and updates the prior belief on document relevance at every round of interaction\nto the best of our knowledge this is the first work that attempts to ask explicit questions to reviewers for the purpose of achieving total recall that goes beyond document relevance feedback\nthe evaluation results show that our approach can significantly reduce human effort, while achieve high recal", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The goal of a technology-assisted review is to achieve high recall with low human effort\n Continuous active learning algorithms have demonstrated good performance in locating the majority of relevant documents in a collection, however their performance is reaching a plateau when 80%-90% of them has been found\nFinding the last few relevant documents typically requires exhaustively reviewing the collection\nIn this paper,we propose a novel method to identify these last few, but significant, documents efficiently\nOur method makes the hypothesis that entities carry vital information in documents, and that reviewers can answer questions about the presence or absence of an entity in the missing relevance documents.\nBased on this we devise a sequential Bayesian search method that selects the optimal sequence of questions to ask.\nThe experimental results show that our proposed method can greatly improve performance requiring less reviewing effor"},
{"doc": "multivariate time series data are ubiquitous in our everyday life ranging from the prices in stock markets, the traffic flows on highways, the outputs of solar power plants, the temperatures across different cities, just to name a few\nin such applications, users are often interested in the forecasting of the new trends or potential hazardous events based on historical observations on time series signals\nfor instance, a better route plan could be devised based on the predicted traffic jam patterns a few hours ahead, and a larger profit could be made with the forecasting of the near-future stock market\nmultivariate time series forecasting often faces a major research challenge, that is, how to capture and leverage the dynamics dependencies among multiple variables\nspecifically, real-world applications often entail a mixture of short-term and long-term repeating patterns, as shown in figure 1 which plots the hourly occupancy rate of a freeway\napparently, there are two repeating patterns, daily and weekly\nthe former portraits the morning peaks vs. evening peaks, while the latter reflects the workday and weekend patterns\na successful time series forecasting model should be capture both kinds of recurring patterns for accurate predictions\nas another example, consider the task of predicting the output of a solar energy farm based on the measured solar radiation by massive sensors over different locations\nthe long-term patterns reflect the difference between days vs. nights, summer vs. winter, etc., and the shortterm patterns reflect the effects of cloud movements, wind direction changes, etc\nagain, without taking both kinds of recurrent patterns into account, accurate time series forecasting is not possible\nhowever, traditional approaches such as the large body of work in autoregressive methods [2, 12, 22, 33, 36] fall short in this aspect, as most of them do not distinguish the two kinds of patterns nor model their interactions explicitly and dynamically\naddressing such limitations of existing methods in time series forecasting is the main focus of this paper, for which we propose a novel framework that takes advantages of recent developments in deep learning research\ndeep neural networks have been intensively studied in related domains, and made extraordinary impacts on the solutions of a broad range of problems\nthe recurrent neural networks (rnn) models [9], for example, have become most popular in recent natural language processing (nlp) research\ntwo variants of rnn in particular, namely the long short term memory (lstm) [15] and the gated recurrent unit (gru) [6], have significantly improved the state-of-the-art performance in machine translation, speech recognition and other nlp tasks as they can effectively capture the meanings of words based on the long-term and short-term dependencies among them in input documents [1, 14, 19]\nin the field of computer vision, as another example, convolution neural network (cnn) models [19, 21] have shown outstanding performance by successfully extracting local and shift-invariant features (called \"shapelets\" sometimes) at various granularity levels from input images\ndeep neural networks have received an increasing amount of attention in time series analysis\na substantial portion of the previous work has been focusing on time series classification, i.e., the task of automated assignment of class labels to time series input\nfor instance, rnn architectures have been studied for extracting informative patterns from health-care sequential data [5, 23] and classifying the data with respect diagnostic categories\nrnn has been applied to mobile data, for classifying the input sequences with respect to actions or activities [13]\ncnn models have been used in action/activity recognition [13, 20, 32], for the extraction of shift-invariant local patterns from input sequences as the features of classification models\ndeep neural networks have been studied for time series forecasting [8, 27, 34, 37], i.e., the task of using observed time series in the past to predict the unknown time series in a look-ahead horizon \u2013 the larger the horizon, the harder the problem\nefforts in this direction range from the early work using naive rnn models [7] and the hybrid models [16, 35, 36] combining the use of arima [3] and multilayer perceptron (mlp), to the recent combination of vanilla rnn and dynamic boltzmann machines in time series forecasting [8]\nin this paper, we propose a deep learning framework designed for the multivariate time series forecasting, namely long and short term time-series network (lstnet), as illustrated in figure 2\nit leverages the strengths of both the convolutional layer to discover the local dependency patterns among multi-dimensional input variables and the recurrent layer to capture complex long-term dependencies\na novel recurrent structure, namely recurrent-skip, is designed for capturing very long-term dependence patterns and making the optimization easier as it utilizes the periodic property of the input time series signals\nfinally, the lstnet incorporates a traditional autoregressive linear model in parallel to the non-linear neural network part, which makes the non-linear deep learning model more robust for the time series with violate scale changing\nin the experiment on the real world seasonal time series datasets, our model consistently outperforms the traditional linear models and gru recurrent neural network\nthe rest of this paper is organized as follows\nsection 2 outlines the related background, including representative auto-regressive methods and gaussian process models\nsection 3 describe our proposed lstnet\nsection 4 reports the evaluation results of our model in comparison with strong baselines on real-world datasets\nfinally, we conclude our findings in section ", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation\nTemporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail\nIn this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge\nLSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends\nFurthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model\nIn our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods\nAll the data and experiment codes are available onlin"},
{"doc": "recent advances in hin research [7] allow us to embed a hin into a multi-dimensional vector space [6]\nwe present a method that facilitates these embeddings to be used as features in an ltr framework for the problem of contextual suggestion\nin this problem domain, we have contextual queries and user profiles that contain preference rankings of objects of interest\nin our approach, we model users and objects in the same hin and define meta-paths specific to the problem domain, then generate graph embeddings by randomly sampling the hin conditioned on the meta-paths\nonce users and their interests are projected into the resulting embedding space, we derive features that rank the objects of interest according to a contextual query\nmeta-paths enable us to capture human intuition by introducing a set of semantic constrains on the hin\nfor example, if user u1 tags a relevant document d1 using word w, then another user u2 that has used w to tag a different document d1 might also find d1 relevant\nthis information is inherently contained in the graph but hidden\na domain expert can utilize meta-paths to express her knowledge about this latent structure\nthe generation of the statistical representation of the graph is then guided by conditioning the sampling of the graph along nodes that follow the meta-path\nin this work we show how meta-paths can be utilized in an information retrieval setting.\nby representing users and the objects of interest (i.e. documents) in the same vector space we can make use of the distances between objects as features in a ranking function\nthe ranking function is then learned from a small sample of relevance judged documents (in our case the relevance judgments from previous years of the treccs task)\nonce trained, the ranking function can be utilized to re-rank a set of retrieved documents, thereby incorporating the latent information of the hin\nto demonstrate a concrete application of the retrieval framework we have selected the trip recommendation problem of the treccs task [3]\n the treccs dataset provides user profiles composed of a set of objects ranked by relevancy to the user along with the search intent (e.g. planning a trip)\n the goal is to return a ranked list of attractions that might be interesting to the user\nas the objects of interest are represented by text documents, we investigate how document-based nodes can be broken up into fine-grained node types to improve the retrieval performance significantly\nfurther, we experimentally show that we can reduce sparsity in the graph by limiting the number of nodes prior to training, which results in significant performance improvements\nto summarize, the contributions of this paper are to\n(1) define a general ir framework for the ranking of heterogeneous objects based on hin embeddings\n(2) identify node types and graph topology specific to the treccs task.\n(3) specify meta-paths to encode domain knowledge in the embedding space\n(4) show how finegrained modeling of document-based nodes can improve performance\n(5) compare how different feature selection methods can reduce graph size and sparsity, while improving performanc", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion\nOur method represents users, documents and other context-related documents as heterogeneous objects in a HIN\nUsing meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space\nThis allows inferences of user interest on unseen objects based on distance in the embedding space\nThese object distances are then incorporated as features in a well-established learning to rank (LTR) framework.\nWe make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS tas"},
{"doc": "patent retrieval (pr) is the pillar of almost all patent analysis tasks\npr is a challenging task as patents are multi-page, multi-modal, multi-language, semi-structured, and metadata rich documents\non another hand, patent queries can be a complete multi-page patent application\nthese unique features make traditional ir methods used for web or ad hoc search inappropriate or at least of limited applicability to pr [12].\npr methods are either keyword-based [7, 9, 13] or semantic-based [3, 6, 8].\nbecause neither methods has acceptable performance, few interactive methods were proposed to better discriminate relevant vs. irrelevant terms based on user feedback [4]\nin this paper, we present a novel interactive framework for pr based on distributed representations of concepts and entities identified in patents text.\noffline, we jointly learn the embeddings of words, concepts, patent documents, and patent classes in the same semantic space\nwe then use the learned embeddings to generate multiple vector-based representations of the topic patent query and its prior art candidates\ngiven a topic patent, we find its prior art through two steps:\n1) candidate generation through keyword search, favoring recall,\nand 2) candidate reranking through an ensemble of semantic similarities computed from the vector representations, favoring precision\nempirical evaluation of this automated retrieval scheme on the clef-ip 2010 dataset shows its efficacy over keyword search where we get 4.6% improvement in recall@100\nwe also propose an effective query reformulation and term weighting mechanism based on interactive relevance feedback.\nwe model term weighting as a supervised feature selection problem where term weights are assigned based on how good each term is at discriminating the relevant vs. irrelevant candidates obtained from user feedback\n our interaction mechanism is more practical and realistic than the one proposed by golestan far et al. [4]\nwe ask the user to annotate hits in the top n results as relevant/irrelevant, while in [4] the user is restricted to annotate only relevant candidates which might appear very deep in the candidates list\nwe simulate this interactive term weighting mechanism to demonstrate its effectiveness over the best performer in the clef-ip 2010 competition; patatras [5]\n simulation results show that we can outperform patatras with only 1 annotated candidate regardless of whether it is relevant or not\nit is worth mentioning that similar results have been presented in golestan far et al. [4], but with restricting the user to annotate 1 relevant candidate which again might require the user to navigate through several candidate", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "We present a novel interactive framework for patent retrieval leveraging distributed representations of concepts and entities extracted from the patents text\nWe propose a simple and practical interactive relevance feedback mechanism where the user is asked to annotate relevant/irrelevant results from the top n hits\nWe then utilize this feedback for query reformulation and term weighting where weights are assigned based on how good each term is at discriminating the relevant vs. irrelevant candidates\nFirst, we demonstrate the efficacy of the distributed representations on the CLEF-IP 2010 dataset where we achieve significant improvement of 4.6% in recall over the keyword search baseline\nSecond, we simulate interactivity to demonstrate the efficacy of our interactive term weighting mechanism\nSimulation results show that we can achieve significant improvement in recall from one interaction iteration outperforming previous semantic and interactive patent retrieval method"},
{"doc": "among various recommendation techniques, the most successful approach is collaborative filtering (cf) [4]; it recommends items to a user based on previous ratings of other users whose tastes are similar to the target user\n however, this in turn implies that the performance of cf will suffer without a sufficient amount of ratings previously given by users, which is common in reality\nto compensate for the sparsity of the user\u2013item rating data, side information related to users and items, such as user social network [8], user review documents [11\u201314], and item affinity network [9] has been actively leveraged\n in this work, we specifically focus on user review-aware recommendation\nuser reviews are particularly useful for alleviating the sparsity of user ratings, because the reviews not only embody a user\u2019s intention behind the ratings, but also contain conspicuous item properties\nthat is to say, if reviews are fully exploited, we can build recommender systems even with few ratings provided, which naturally alleviates the sparsity of user\u2013item rating data\nto extract meaningful features from review documents, deep learning-based approaches have been recently proposed [1, 13]\nmore specifically, convolutional neural network (cnn)-based recommendation methods have gained attention [3, 11, 14] thanks to the capability of cnns to capture general contextual features from documents\ndeepconn [14] adopts two cnns, where one of them models users through reviews written by the users, while the other models items through reviews written for the items.\nbuilding upon deepconn, seo et al. propose d-attn [11] that further adopts the dual local and global attention mechanism on the cnns, which endow the recommender systems with interpretability regarding the reviews that are used for modeling users and items\ndespite their state-of-the-art performance, they are limited in that users and items are modeled by the reviews consisting of raw words.\nhowever, each user has different tendency in writing a review and thus words contain an inherent ambiguity, which makes it hard to precisely understand the user\u2019s intent\nas a concrete example, let\u2019s assume that two different users provided reviews that contain the following identical sentence: \u201c... i like the laptop... \u201d\nwhereas a tolerant user would use the word \u201clike\u201d to describe an adequate laptop, a critical user would not use it unless he is completely satisfied with the laptop\n however, the previous reviewaware methods simply aggregate all the associated reviews and feed them to cnns expecting the cnns to automatically extract meaningful features for modeling users and items, which does not suffice for precisely modeling the users and items\nthis phenomenon compounds when users provided only a few reviews, i.e., cold-start [10], which is common in reality\nmoreover, as the existing approaches model each user/item by the concatenation of all the words from every associated review, the size of input for cnns becomes considerably large, which makes the above approaches practically not feasible in the real-world applications\nin this paper, to overcome the above limitations of the existing methods, we propose a novel sentiment guided review-aware recommendation method, called sentirec.\nthe core idea is to leverage the overall sentiments of reviews that are represented as ratings that accompany the reviews.\nin our previous example, if we have a prior knowledge that the tolerant user gave a 3-star rating to the laptop while the critical user gave a 5-star rating, we will be able to more accurately understand the review, which in turn enables us to better model users and items\nour proposed method consists of two steps.\nin the first step, instead of representing a review by the concatenation of its constituent raw words as in the previous methods, we encode each review into a fixed-size review vector that is guided to embody the sentiment information of the review\nmore precisely, we regard a rating that accompanies a review as a summarization of the overall sentiment of a user on an item, and train a cnn that is designed to predict the rating given the review as input, after which a fixed-size vector for the review is obtained by taking the output of the last hidden layer\nthe second step resembles the training process of deepconn and d-attn, but is distinguished in that users/items in sentirec are represented by the concatenation of their associated fixed-size review vectors, rather than raw words\nthe advantages of sentirec compared with the previous methods are\n1) we obtain more accurate representations for reviews by incorporating users\u2019 overall sentiments on items into reviews, which removes the possible ambiguity contained in the reviews.\nthis in turn results in a better understanding of the reviews, and leads to more accurate representations for users and items resulting in an improved recommendation accuracy\n moreover, 2) we drastically reduce the size of the input, which gives us scalability in terms of the training time and the memory usage\nour experiments show that sentirec outperforms the state-of-the-art baselines, while being considerably more efficient\nmoreover, we perform a qualitative evaluation on the review vectors trained by sentirec to ascertain that the overall sentiments are indeed encoded in the vector", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Existing review-aware recommendation methods represent users (or items) through the concatenation of the reviews written by (or for) them, and depend entirely on convolutional neural networks (CNNs) to extract meaningful features for modeling users (or items)\nHowever, understanding reviews based only on the raw words of reviews is challenging because of the inherent ambiguity contained in them originated from the users\u2019 different tendency in writing\n Moreover, it is inefficient in time and memory to model users/items by the concatenation of their associated reviews owing to considerably large inputs to CNNs\nIn this work, we present a scalable reviewaware recommendation method, called SentiRec, that is guided to incorporate the sentiments of reviews when modeling the users and the items\nSentiRec is a two-step approach composed of the first step that includes the encoding of each review into a fixed-size review vector that is trained to embody the sentiment of the review, followed by the second step that generates recommendations based on the vector-encoded reviews\nThrough our experiments, we show that SentiRec not only outperforms the existing reviewaware methods, but also drastically reduces the training time and the memory usage.\nWe also conduct a qualitative evaluation on the vector-encoded reviews trained by SentiRec to demonstrate that the overall sentiments are indeed encoded therei"},
{"doc": "community-acquired pneumonia (cap) [13] refers to the lungs of patients infected when they are not in hospital\nit has long been a major cause of morbidity and death, especially for children\nas reported by the studies [12, 15], pneumonia is one of the top ranked diseases responsible for the deaths of children both in usa and china\ncuring cap largely requires an early administration of appropriate antibiotics [9]\nunfortunately, the issue of the abuse of antibiotics is very prevalent, especially in developing countries such as china [7], which seriously endangers human health\nalleviating the above issue needs an accurate detection of pathogenic microorganism [13].\npathogenic microorganism is a family of microorganisms which will cause human diseases\nif the pathogenic microorganism of cap can be precisely identified, clinicians are able to prescribe optimal antibiotics\nconventional gold-standard detection methods are mainly etiology based, including culture based assays, polymerase-chain-reaction (pcr), etc\nhowever, many of them need specialized equipment and reagents, and are labor and time intensive [4, 17], which limit their application only in major hospitals.\nthus, there is an urgent need to develop intelligent and cost-effective methodologies to detect pathogenic microorganism of cap using data which is easier to be acquired\nrecent progress in wide collection of electronic health records (ehrs) [8] applies the methodologies from artificial intelligence community to cap\nhowever, existing studies in this regard are somewhat limited and mainly aim at 1) predicting whether suspected patients have pneumonia [16] or 2) further judging the risk of patients with pneumonia [3]\nmost of them have ignored to investigate the power of patient easy-to-acquire data from ehrs for automatically detecting pathogenic microorganism of cap.\nin fact, it plays a great role in treating cap children\nin this paper, we formulate a new problem of utilizing pneumonia patients multiple medical features from ehrs to identify their pathogenic microorganisms\nto our best knowledge, none of previous studies has investigated this problem\nthe studied features include time-varying body temperature and some carefully selected clinical measurements which are easy to be acquired, such as white blood cell count from routine blood test (see table 1 for details)\nconsequently, the central challenge is how to effectively fuse the above multiple types of features and construct an effective model for the problem\nto address the challenge, we develop a patient attention based recurrent neural network (pa-rnn), which is capable of modeling sequential body temperatures and fusing multiple types of patient features\nto be specific, pa-rnn first exploits the power of recurrent neural network (rnn) to obtain a sequence of body temperature representations for different time steps\nmeantime it constructs patient basic features which are carefully selected from ehrs\nafterwards, inspired by attention mechanism [1], pa-rnn provides a patient feature based attention to determine the importance of each time-varying temperature representation and further gains an integrated representation for a whole body temperature sequence\nfinally, the model fuses the integrated representation with the representation of patient basic features for pathogenic microorganism detection\nin a nutshell, the major novelty of pa-rnn is that most previous studies which utilize rnn to model ehrs [2, 5, 10, 11, 14] focus on predicting targets at the next time step based on current hidden states of rnn\nhowever, we obtain an integrated representation of body temperatures sequence through a novel patient feature based attention computation to all hidden states of rnn.\nwe conduct comprehensive experiments on a real world dataset from a major hospital in china, indicating the benefit of fusing multiple types of features from ehrs for the studied problem, and demonstrating the effectiveness of pa-rnn over several alternative method", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Community-acquired pneumonia (CAP) is a major death cause for children, requiring an early administration of appropriate antibiotics to cure it\nTo achieve this, accurate detection of pathogenic microorganism is crucial, especially for reducing the abuse of antibiotics\nConventional gold standard detection methods are mainly etiology based, incurring high cost and labor intensity\nAlthough recently electronic health records (EHRs) become prevalent and widely used, their power for automatically determining pathogenic microorganism has not been investigated\nIn this paper, we formulate a new problem for automatically detecting pathogenic microorganism of CAP by considering patient biomedical features from EHRs, including time-varying body temperatures and common laboratory measurements\nWe further develop a Patient Attention based Recurrent Neural Network (PA-RNN) model to fuse different patient features for detection\nWe conduct experiments on a real dataset, demonstrating utilizing electronic health records yields promising performance and PA-RNN outperforms several alternative"},
{"doc": "in recent years, recommender systems have played a significant part in the multimedia field\nhowever, within most of these web services, the number of users and the number of images/videos are dramatically growing, making the multimedia recommendation more challenging than ever before\nthe dominating web multimedia content requires modern recommender systems, in particular, those based on collaborative filtering (cf), to sift through massive multimedia content for users in a highly dynamic environment\ncollaborative filtering methods group people with similar interests and make recommendations on this basis.\nin the context of multimedia recommendation, item indicates different kinds of multimedia content\nmost cf methods rely on items star ratings, which provide explicit feedback [5, 9]\nhowever, when used in the multimedia field, traditional cf methods have two shortcomings.\nfirst, cf methods failed to focus on the multimedia content itself, which is the most important factor when users choose images or videos\nas content information of items is available, content-aware methods have been introduced\nsuch incorporation of content information usually leads to better recommendation performance [1, 10, 12].\nsecond, explicit ratings are not always available in many applications\nmore often, interaction data such as \"like\" of photos, or \"view\" of movies are more convenient to collect\nsuch data are based on implicit feedback\nto combine the multimedia content with the cf method and make the best use of implicit feedback, we propose a novel contentaware multimedia recommendation framework with graph autoencoder (graphcar)\nwe use two graph convolutional networks as the encoder to model latent factor of users and items, respectively\nafter that, we generate preference scores by using the inner product of two latent factor vectors\nwe evaluate graphcar extensively on two real-world datasets that represent a spectrum of different media: the amazon movies and tvs dataset and the vine dataset, with the former providing images features and the latter providing videos features\nthrough these experiments, we observe that graphcar is superior to competing methods of the best configuration, ranging from cf-based methods to content-based methods\nin summary, the main contributions of our work are\nwe propose a novel content-aware multimedia recommendation model with graph autoencoder (graphcar) to employ graph autoencoder in cf with implicit feedback\nto combine user-item interaction with user attributes and the multimedia content, we introduce two graph convolutional networks, both of which are neural networks that can be seamlessly incorporated into any neighborhood models with efficient end-to-end sgd training\nexperiments on two real datasets show that graphcar significantly outperforms state-of-the-art techniques of both cf and content-based method", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Precisely recommending relevant multimedia items from massive candidates to a large number of users is an indispensable yet difficult task on many platforms\nA promising way is to project users and items into a latent space and recommend items via the inner product of latent factor vectors\nHowever, previous studies paid little attention to the multimedia content itself and couldn\u2019t make the best use of preference data like implicit feedback.\nTo fill this gap, we propose a Content-aware Multimedia Recommendation Model with Graph Autoencoder (GraphCAR), combining informative multimedia content with user-item interaction.\nSpecifically, user-item interaction, user attributes and multimedia contents (e.g., images, videos, audios, etc.) are taken as input of the autoencoder to generate the item preference scores for each user\nThrough extensive experiments on two real-world multimedia Web services: Amazon and Vine, we show that GraphCAR significantly outperforms state-of-the-art techniques of both collaborative filtering and content-based method"},
{"doc": " recently, several deep learning models for information retrieval have been proposed [2, 4, 6, 8]\nthese models have demonstrated their potential to improve the effectiveness in ad-hoc search\nin general, a deep neural model is constructed to represent the content of the document and the query [4, 8], and/or their interactions or matching scores [2, 6].\nthe utilization of deep neural models is motivated by their ability to make high level and more abstract matching between the document and the query, thereby alleviating the vocabulary mismatch problem\nwe observe, however, that these models only use one level of final representation or matching score for any document-query pair\nfor example, in [8], several layers of convolutions are used to create more and more abstract representations for the document and the query, and the matching score only relies on the last layer of representation\nconvolution is an operation that aggregates lower-level features to produce more abstract features\na matching score at the highest level tends to reflect a conceptual matching\nin reality, user\u2019s queries may be of different nature\nsome queries such as \u201cron howard\u201d (a query in clueweb) asking for information about a celebrity would require a low level lexical matching rather than conceptual matching.\n we call them lexical queries\n on the other hand, a query like \u201clymphoma in dogs\u201d is intended to find document about corresponding concept(s), therefore a high level conceptual matching is preferred\nthese queries are called conceptual queries\nthese examples clearly show the need for matching document and query at different levels of abstraction.\ninspired by this intuition, in this paper we propose a multi-level abstraction convolutional model (macm), which integrates document-query matching at different levels of abstraction\nthis model is expected to have a better capability of coping with different types of user queries\nalthough neural ir models can focus either on document and query representation or on interactions between them, guo et al. [2] showed that the latter is more effective than the former\nbased on this observation, our model is built on document-query interactions rather than representations.\na critical problem in building deep neural models for ir is the requirement of a large amount of labeled training data, which is often unavailable\nthe idea of weak supervision by a traditional ir model is proposed recently [1] and shown to be effective\ninspired by this work, we employ the bm25 retrieval model [7] for weak supervision - the ranked documents retrieved by bm25 are used to train our deep neural model\nwe will see that this strategy is able to train our deep neural model, leading to superior effectiveness to bm25\nthe main contribution of our paper lies in a new neural model capable of coping with different types of queries by matching them with documents at different levels of abstraction\nthis idea can be easily adopted in other deep neural models, whether they are based on representations or interactions, use cnn or rnn\nour experiments on clueweb confirm that our approach can result in superior retrieval effectivenes", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "Recent neural models for IR have produced good retrieval effectiveness compared with traditional models\nYet all of them assume that a single matching function should be used for all queries.\n In practice, user\u2019s queries may be of various nature which might require different levels of matching, from low level word matching to high level conceptual matching\nTo cope with this problem, we propose a multi-level abstraction convolutional model (MACM) that generates and aggregates several levels of matching scores\nWeak supervision is used to address the problem of large training data\nExperimental results demonstrated the effectiveness of our proposed MACM mode"},
{"doc": "most of information retrieval (ir) contributions follow a standard structure: analysis of the state of the art, description of the approach, and empirical evaluation over a certain data set\nthe large amount of available annotated collections allows to improve models by trial and error\nhowever, this methodology does not match with the standard scientific procedures: hypothesis statement, definition of an experiment guided by the specific hypothesis, and result analysis\nas a consequence, the ir community tends to produce solutions to a greater extent than knowledge\nthe slow progress in creating new knowledge in ir is at least partly because it is not easy to import scientific methodologies from other areas such as physics or human sciences into ir; unlike in other engineering areas, the unpredictability of human behavior makes it difficult to find general laws that describe precise phenomena\n(note that in this paper we focus on effectiveness rather than efficiency or scalability in which the user is not involved.)\nregarding social and psychological researches, the need for effectiveness in systems makes futile the production of general principles\nthis situation makes us wonder whether the current practices in ir research are on the \u201cright\" track toward discovery of new knowledge about ir.\nthe more general question here is:\nwhat are the best methodologies (if any) that researchers should follow to optimally advance the knowledge in ir research\n to address this question, in this paper, we first try to quantify the current methodological trends in ir research and categorize existing ir methodologies along two dimensions: (1) empirical vs. theoretical, and (2) top-down vs. bottom-up\n we then identify six desirable properties and anaylze these four types of methodologies accordingly.\nthe analysis indicates that none of the methodologies can satisfy all the desirable properties but they are complementary to each other\nfor example, theoretical methodologies give theoretical foundations, interpretability, and robustness across new scenarios, while empirical methodologies provide evidence about the relative effectiveness of approaches in particular realistic scenarios, as well as providing statistical significance when comparing systems to each other\nfurthermore, we categorized the 167 full papers published in the sigir 2016 and 2017 as well as ictir 2017 conferences into the proposed four categories, i.e., empirical bottom-up, empirical top-down, theoretical bottom-up and theoretical bottom-down, and found that, to certain extent, empirical bottom-up methods are the most dominating methodology in the ir field, indicating a strong bias toward empirical rather than theoretical work\n motivated by the analysis results, we propose a general methodology for ir research that aims to leverage the strengths of both theoretical and empirical methodologie", "labels": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1", "summaries": "The unpredictability of user behavior and the need for effectiveness make it difficult to define a suitable research methodology for Information Retrieval (IR).\nIn order to tackle this challenge, we categorize existing IR methodologies along two dimensions: (1) empirical vs. theoretical, and (2) top-down vs. bottom-up\nThe strengths and drawbacks of the resulting categories are characterized according to 6 desirable aspects\nThe analysis suggests that different methodologies are complementary and therefore, equally necessary.\nThe categorization of the 167 full papers published in the last SIGIR (2016 and 2017) and ICTIR (2017) conferences suggest that most of existing work is empirical bottom-up, suggesting lack of some desirable aspects\nWith the hope of improving IR research practice, we propose a general methodology for IR that integrates the strengths of existing research method"}]
