{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "changed-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the models module  \n"
     ]
    }
   ],
   "source": [
    "print(\"this is the models module  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "consolidated-weekend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4918, -0.8075, -3.1827, -0.7470]],\n",
      "\n",
      "        [[-0.6181, -0.9139, -0.2974,  0.3185]],\n",
      "\n",
      "        [[ 0.6534,  1.3140,  0.5316,  1.4086]],\n",
      "\n",
      "        [[-0.5043,  1.8300, -0.3793, -2.1729]],\n",
      "\n",
      "        [[-1.3261, -0.2732, -0.0319, -0.5234]],\n",
      "\n",
      "        [[ 0.3076, -1.0698, -0.6218,  0.6029]],\n",
      "\n",
      "        [[-1.8304,  0.3810, -0.0749, -2.1429]],\n",
      "\n",
      "        [[ 0.4508, -0.1929,  0.0575, -0.6249]],\n",
      "\n",
      "        [[ 0.7716, -0.4401,  0.3739,  0.7881]],\n",
      "\n",
      "        [[ 0.8114, -0.2754,  0.1713, -0.2009]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the query features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*query \\\\\n",
    "            attn_scores = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            attn_out = attn * context\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the query\n",
    "    Inputs: query, context\n",
    "        - **query** (batch, query_len, dimensions): tensor containing the query features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: query, attn\n",
    "        - **query** (batch, query_len, dimensions): tensor containing the attended query features from the decoder.\n",
    "        - **attn** (batch, query_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "    \n",
    "    \"\"\"\n",
    "        - query   (batch, query_len, dimensions): tensor containing the query features from the decoder.\n",
    "        - context (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    \"\"\"\n",
    "    def forward(self, query, context):\n",
    "        batch_size = query.size(0)\n",
    "        dim = query.size(2)\n",
    "        in_len = context.size(1)\n",
    "        # (batch, query_len, dim) * (batch, in_len, dim) -> (batch, query_len, in_len)\n",
    "        attn = torch.bmm(query, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn_scores = F.softmax(attn.view(-1, in_len),dim=1).view(batch_size, -1, in_len)\n",
    "\n",
    "        # (batch, query_len, in_len) * (batch, in_len, dim) -> (batch, query_len, dim)\n",
    "        attn_out = torch.bmm(attn_scores, context)\n",
    "\n",
    "        return attn_out, attn_scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(1)\n",
    "    attention = Attention()\n",
    "    context = Variable(torch.randn(10, 20, 4))\n",
    "    query = Variable(torch.randn(10, 1, 4))\n",
    "    query, attn = attention(query, context)\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "streaming-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class BasicModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(BasicModule,self).__init__()\n",
    "        self.args = args\n",
    "        self.model_name = str(type(self))\n",
    "\n",
    "    def pad_doc(self,words_out,doc_lens):\n",
    "        pad_dim = words_out.size(1)\n",
    "        max_doc_len = max(doc_lens)\n",
    "        sent_input = []\n",
    "        start = 0\n",
    "        for doc_len in doc_lens:\n",
    "            stop = start + doc_len\n",
    "            valid = words_out[start:stop]                                       # (doc_len,2*H)\n",
    "            start = stop\n",
    "            if doc_len == max_doc_len:\n",
    "                sent_input.append(valid.unsqueeze(0))\n",
    "            else:\n",
    "                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim))\n",
    "                if self.args.device is not None:\n",
    "                    pad = pad.cuda()\n",
    "                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))          # (1,max_len,2*H)\n",
    "        sent_input = torch.cat(sent_input,dim=0)                                # (B,max_len,2*H)\n",
    "        return sent_input\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint = {'model':self.state_dict(), 'args': self.args}\n",
    "        best_path = '%s%s_seed_%d.pt' % (self.args.save_dir,self.model_name,self.args.seed)\n",
    "        torch.save(checkpoint,best_path)\n",
    "\n",
    "        return best_path\n",
    "\n",
    "    def load(self, best_path):\n",
    "        if self.args.device is not None:\n",
    "            data = torch.load(best_path)['model']\n",
    "        else:\n",
    "            data = torch.load(best_path, map_location=lambda storage, loc: storage)['model']\n",
    "        self.load_state_dict(data)\n",
    "        if self.args.device is not None:\n",
    "            return self.cuda()\n",
    "        else:\n",
    "            return self\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bored-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import BasicModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN_RNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(CNN_RNN,self).__init__(args)\n",
    "        self.model_name = 'CNN_RNN'\n",
    "        self.args = args\n",
    "        \n",
    "        Ks = args.kernel_sizes\n",
    "        Ci = args.embed_dim\n",
    "        Co = args.kernel_num\n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.convs = nn.ModuleList([ nn.Sequential(\n",
    "                                            nn.Conv1d(Ci,Co,K),\n",
    "                                            nn.BatchNorm1d(Co),\n",
    "                                            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                                            nn.Conv1d(Co,Co,K),\n",
    "                                            nn.BatchNorm1d(Co),\n",
    "                                            nn.LeakyReLU(inplace=True)\n",
    "                                     )\n",
    "                                    for K in Ks])\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = Co * len(Ks),\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(2*H,2*H),\n",
    "                nn.BatchNorm1d(2*H),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "\n",
    "    def max_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.max_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def avg_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.avg_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def forward(self,x,doc_lens):\n",
    "        sent_lens = torch.sum(torch.sign(x),dim=1).data \n",
    "        H = self.args.hidden_size\n",
    "        x = self.embed(x)                                                       # (N,L,D)\n",
    "        # word level GRU\n",
    "        x = [conv(x.permute(0,2,1)) for conv in self.convs]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x,1)\n",
    "        # make sent features(pad with zeros)\n",
    "        x = self.pad_doc(x,doc_lens)\n",
    "\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
    "        docs = self.fc(docs)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = docs[index].unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "republican-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#coding:utf8\n",
    "#from .BasicModule import BasicModule\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from .Attention import Attention\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class AttnRNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(AttnRNN,self).__init__(args)\n",
    "        self.model_name = 'AttnRNN'\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.attn = Attention()\n",
    "        self.word_query = nn.Parameter(torch.randn(1,1,2*H))\n",
    "        self.sent_query = nn.Parameter(torch.randn(1,1,2*H))\n",
    "\n",
    "        self.word_RNN = nn.GRU(\n",
    "                        input_size = D,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = 2*H,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "               \n",
    "        self.fc = nn.Linear(2*H,2*H)\n",
    "\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "    def forward(self,x,doc_lens):\n",
    "        N = x.size(0)\n",
    "        L = x.size(1)\n",
    "        B = len(doc_lens)\n",
    "        H = self.args.hidden_size\n",
    "        word_mask = torch.ones_like(x) - torch.sign(x)\n",
    "        word_mask = word_mask.data.type(torch.cuda.ByteTensor).view(N,1,L)\n",
    "        \n",
    "        x = self.embed(x)                                # (N,L,D)\n",
    "        x,_ = self.word_RNN(x)\n",
    "        \n",
    "        # attention\n",
    "        query = self.word_query.expand(N,-1,-1).contiguous()\n",
    "        self.attn.set_mask(word_mask)\n",
    "        word_out = self.attn(query,x)[0].squeeze(1)      # (N,2*H)\n",
    "\n",
    "        x = self.pad_doc(word_out,doc_lens)\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
    "        max_doc_len = max(doc_lens)\n",
    "        mask = torch.ones(B,max_doc_len)\n",
    "        for i in range(B):\n",
    "            for j in range(doc_lens[i]):\n",
    "                mask[i][j] = 0\n",
    "        sent_mask = mask.type(torch.cuda.ByteTensor).view(B,1,max_doc_len)\n",
    "        \n",
    "        # attention\n",
    "        query = self.sent_query.expand(B,-1,-1).contiguous()\n",
    "        self.attn.set_mask(sent_mask)\n",
    "        docs = self.attn(query,x)[0].squeeze(1)      # (B,2*H)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                #print position,F.sigmoid(abs_p + rel_p)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-holder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
