{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forced-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "module_path = os.path.abspath(os.path.join('C:/Users/user/metin özütleme/hphaos summarunner/models'))\n",
    "\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "close-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class BasicModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        super(BasicModule,self).__init__()\n",
    "        self.args = args\n",
    "        print(self.args.device)\n",
    "        print()\n",
    "        self.model_name = str(type(self))\n",
    "\n",
    "    def pad_doc(self,words_out,doc_lens):\n",
    "        pad_dim = words_out.size(1)\n",
    "        max_doc_len = max(doc_lens)\n",
    "        sent_input = []\n",
    "        start = 0\n",
    "        for doc_len in doc_lens:\n",
    "            stop = start + doc_len\n",
    "            valid = words_out[start:stop]                                       # (doc_len,2*H)\n",
    "            start = stop\n",
    "            if doc_len == max_doc_len:\n",
    "                sent_input.append(valid.unsqueeze(0))\n",
    "            else:\n",
    "                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim))\n",
    "                if self.args.device is not None:\n",
    "                    pad = pad.cuda()\n",
    "                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))          # (1,max_len,2*H)\n",
    "        sent_input = torch.cat(sent_input,dim=0)                                # (B,max_len,2*H)\n",
    "        return sent_input\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint = {'model':self.state_dict(), 'args': self.args}\n",
    "        best_path = '%s%s_seed_%d.pt' % (self.args.save_dir,self.model_name,self.args.seed)\n",
    "        torch.save(checkpoint,best_path)\n",
    "\n",
    "        return best_path\n",
    "\n",
    "    def load(self, best_path):\n",
    "        if self.args.device is not None:\n",
    "            print(self.args.device)\n",
    "            data = torch.load(best_path)['model']\n",
    "        else:\n",
    "            data = torch.load(best_path, map_location=lambda storage, loc: storage)['model']\n",
    "        self.load_state_dict(data)\n",
    "        if self.args.device is not None:\n",
    "            return self.cuda()\n",
    "        else:\n",
    "            return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "announced-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicModule import BasicModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class CNN_RNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(CNN_RNN,self).__init__(args)\n",
    "        self.model_name = 'CNN_RNN'\n",
    "        self.args = args\n",
    "        \n",
    "        Ks = args.kernel_sizes\n",
    "        Ci = args.embed_dim\n",
    "        Co = args.kernel_num\n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.convs = nn.ModuleList([ nn.Sequential(\n",
    "                                            nn.Conv1d(Ci,Co,K),\n",
    "                                            nn.BatchNorm1d(Co),\n",
    "                                            nn.LeakyReLU(inplace=True),\n",
    "\n",
    "                                            nn.Conv1d(Co,Co,K),\n",
    "                                            nn.BatchNorm1d(Co),\n",
    "                                            nn.LeakyReLU(inplace=True)\n",
    "                                     )\n",
    "                                    for K in Ks])\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = Co * len(Ks),\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(2*H,2*H),\n",
    "                nn.BatchNorm1d(2*H),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "\n",
    "    def max_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.max_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def avg_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.avg_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def forward(self,x,doc_lens):\n",
    "        sent_lens = torch.sum(torch.sign(x),dim=1).data \n",
    "        H = self.args.hidden_size\n",
    "        x = self.embed(x)                                                       # (N,L,D)\n",
    "        # word level GRU\n",
    "        x = [conv(x.permute(0,2,1)) for conv in self.convs]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x,1)\n",
    "        # make sent features(pad with zeros)\n",
    "        x = self.pad_doc(x,doc_lens)\n",
    "\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
    "        docs = self.fc(docs)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = docs[index].unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "substantial-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4918, -0.8075, -3.1827, -0.7470]],\n",
      "\n",
      "        [[-0.6181, -0.9139, -0.2974,  0.3185]],\n",
      "\n",
      "        [[ 0.6534,  1.3140,  0.5316,  1.4086]],\n",
      "\n",
      "        [[-0.5043,  1.8300, -0.3793, -2.1729]],\n",
      "\n",
      "        [[-1.3261, -0.2732, -0.0319, -0.5234]],\n",
      "\n",
      "        [[ 0.3076, -1.0698, -0.6218,  0.6029]],\n",
      "\n",
      "        [[-1.8304,  0.3810, -0.0749, -2.1429]],\n",
      "\n",
      "        [[ 0.4508, -0.1929,  0.0575, -0.6249]],\n",
      "\n",
      "        [[ 0.7716, -0.4401,  0.3739,  0.7881]],\n",
      "\n",
      "        [[ 0.8114, -0.2754,  0.1713, -0.2009]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    r\"\"\"\n",
    "    Applies an attention mechanism on the query features from the decoder.\n",
    "    .. math::\n",
    "            \\begin{array}{ll}\n",
    "            x = context*query \\\\\n",
    "            attn_scores = exp(x_i) / sum_j exp(x_j) \\\\\n",
    "            attn_out = attn * context\n",
    "            \\end{array}\n",
    "    Args:\n",
    "        dim(int): The number of expected features in the query\n",
    "    Inputs: query, context\n",
    "        - **query** (batch, query_len, dimensions): tensor containing the query features from the decoder.\n",
    "        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    Outputs: query, attn\n",
    "        - **query** (batch, query_len, dimensions): tensor containing the attended query features from the decoder.\n",
    "        - **attn** (batch, query_len, input_len): tensor containing attention weights.\n",
    "    Attributes:\n",
    "        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.mask = None\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"\n",
    "        Sets indices to be masked\n",
    "        Args:\n",
    "            mask (torch.Tensor): tensor containing indices to be masked\n",
    "        \"\"\"\n",
    "        self.mask = mask\n",
    "    \n",
    "    \"\"\"\n",
    "        - query   (batch, query_len, dimensions): tensor containing the query features from the decoder.\n",
    "        - context (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n",
    "    \"\"\"\n",
    "    def forward(self, query, context):\n",
    "        batch_size = query.size(0)\n",
    "        dim = query.size(2)\n",
    "        in_len = context.size(1)\n",
    "        # (batch, query_len, dim) * (batch, in_len, dim) -> (batch, query_len, in_len)\n",
    "        attn = torch.bmm(query, context.transpose(1, 2))\n",
    "        if self.mask is not None:\n",
    "            attn.data.masked_fill_(self.mask, -float('inf'))\n",
    "        attn_scores = F.softmax(attn.view(-1, in_len),dim=1).view(batch_size, -1, in_len)\n",
    "\n",
    "        # (batch, query_len, in_len) * (batch, in_len, dim) -> (batch, query_len, dim)\n",
    "        attn_out = torch.bmm(attn_scores, context)\n",
    "\n",
    "        return attn_out, attn_scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(1)\n",
    "    attention = Attention()\n",
    "    context = Variable(torch.randn(10, 20, 4))\n",
    "    query = Variable(torch.randn(10, 1, 4))\n",
    "    query, attn = attention(query, context)\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "democratic-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#coding:utf8\n",
    "from BasicModule import BasicModule\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from .Attention import Attention\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class AttnRNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(AttnRNN,self).__init__(args)\n",
    "        self.model_name = 'AttnRNN'\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.attn = Attention()\n",
    "        self.word_query = nn.Parameter(torch.randn(1,1,2*H))\n",
    "        self.sent_query = nn.Parameter(torch.randn(1,1,2*H))\n",
    "\n",
    "        self.word_RNN = nn.GRU(\n",
    "                        input_size = D,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = 2*H,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "               \n",
    "        self.fc = nn.Linear(2*H,2*H)\n",
    "\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "    def forward(self,x,doc_lens):\n",
    "        N = x.size(0)\n",
    "        L = x.size(1)\n",
    "        B = len(doc_lens)\n",
    "        H = self.args.hidden_size\n",
    "        word_mask = torch.ones_like(x) - torch.sign(x)\n",
    "        word_mask = word_mask.data.type(torch.cuda.ByteTensor).view(N,1,L)\n",
    "        \n",
    "        x = self.embed(x)                                # (N,L,D)\n",
    "        x,_ = self.word_RNN(x)\n",
    "        \n",
    "        # attention\n",
    "        query = self.word_query.expand(N,-1,-1).contiguous()\n",
    "        self.attn.set_mask(word_mask)\n",
    "        word_out = self.attn(query,x)[0].squeeze(1)      # (N,2*H)\n",
    "\n",
    "        x = self.pad_doc(word_out,doc_lens)\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
    "        max_doc_len = max(doc_lens)\n",
    "        mask = torch.ones(B,max_doc_len)\n",
    "        for i in range(B):\n",
    "            for j in range(doc_lens[i]):\n",
    "                mask[i][j] = 0\n",
    "        sent_mask = mask.type(torch.cuda.ByteTensor).view(B,1,max_doc_len)\n",
    "        \n",
    "        # attention\n",
    "        query = self.sent_query.expand(B,-1,-1).contiguous()\n",
    "        self.attn.set_mask(sent_mask)\n",
    "        docs = self.attn(query,x)[0].squeeze(1)      # (B,2*H)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                #print position,F.sigmoid(abs_p + rel_p)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "velvet-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self,embed,word2id):\n",
    "        self.embed = embed\n",
    "        self.word2id = word2id\n",
    "        self.id2word = {v:k for k,v in word2id.items()}\n",
    "        assert len(self.word2id) == len(self.id2word)\n",
    "        self.PAD_IDX = 0\n",
    "        self.UNK_IDX = 1\n",
    "        self.PAD_TOKEN = 'PAD_TOKEN'\n",
    "        self.UNK_TOKEN = 'UNK_TOKEN'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(word2id)\n",
    "\n",
    "    def i2w(self,idx):\n",
    "        return self.id2word[idx]\n",
    "    def w2i(self,w):\n",
    "        if w in self.word2id:\n",
    "            return self.word2id[w]\n",
    "        else:\n",
    "            return self.UNK_IDX\n",
    "    \n",
    "    def make_features(self,batch,sent_trunc=25,doc_trunc=50,split_token='\\n'):\n",
    "        sents_list,targets,doc_lens = [],[],[]\n",
    "        # trunc document\n",
    "        for doc,label in zip(batch['doc'],batch['labels']):\n",
    "            sents = doc.split(split_token)\n",
    "            labels = label.split(split_token)\n",
    "            labels = [int(l) for l in labels]\n",
    "            max_sent_num = min(doc_trunc,len(sents))##doküman 50 den az cümle içerebilir.\n",
    "            sents = sents[:max_sent_num]\n",
    "            labels = labels[:max_sent_num]\n",
    "            sents_list += sents\n",
    "            targets += labels\n",
    "            doc_lens.append(len(sents)) ##doküman uzunluğu maksimum cümle kadar içerir\n",
    "        # trunc or pad sent\n",
    "        max_sent_len = 0\n",
    "        batch_sents = []\n",
    "        for sent in sents_list:\n",
    "            words = sent.split()\n",
    "            if len(words) > sent_trunc:##cümle içindeki maksimum kelime sayısı\n",
    "                words = words[:sent_trunc]\n",
    "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "            batch_sents.append(words)\n",
    "        \n",
    "        features = []\n",
    "        for sent in batch_sents:\n",
    "            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len-len(sent))]\n",
    "            features.append(feature)\n",
    "        \n",
    "        features = torch.LongTensor(features)    \n",
    "        targets = torch.LongTensor(targets)\n",
    "        summaries = batch['summaries']\n",
    "\n",
    "        return features,targets,summaries,doc_lens\n",
    "\n",
    "    def make_predict_features(self, batch, sent_trunc=100, doc_trunc=75, split_token='. '):\n",
    "        sents_list, doc_lens = [],[]\n",
    "        for doc in batch:\n",
    "            sents = doc.split(split_token)\n",
    "            max_sent_num = min(doc_trunc,len(sents))\n",
    "            sents = sents[:max_sent_num]\n",
    "            sents_list += sents\n",
    "            doc_lens.append(len(sents))\n",
    "        # trunc or pad sent\n",
    "        max_sent_len = 0\n",
    "        batch_sents = []\n",
    "        for sent in sents_list:\n",
    "            words = sent.split()\n",
    "            if len(words) > sent_trunc:\n",
    "                words = words[:sent_trunc]\n",
    "            max_sent_len = len(words) if len(words) > max_sent_len else max_sent_len\n",
    "            batch_sents.append(words)\n",
    "\n",
    "        features = []\n",
    "        for sent in batch_sents:\n",
    "            feature = [self.w2i(w) for w in sent] + [self.PAD_IDX for _ in range(max_sent_len-len(sent))]\n",
    "            features.append(feature)\n",
    "\n",
    "        features = torch.LongTensor(features)\n",
    "\n",
    "        return features, doc_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continental-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "#from Vocab import Vocab\n",
    "import numpy as np\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, examples):\n",
    "        super(Dataset,self).__init__()\n",
    "        # data: {'sents':xxxx,'labels':'xxxx', 'summaries':[1,0]}\n",
    "        self.examples = examples \n",
    "        self.training = True\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "        return self\n",
    "    def test(self):\n",
    "        self.training = True\n",
    "        return self\n",
    "    def shuffle(self,words):\n",
    "        np.random.shuffle(words)\n",
    "        return ' '.join(words)\n",
    "    def dropout(self,words,p=0.3):\n",
    "        l = len(words)\n",
    "        drop_index = np.random.choice(l,int(l*p))\n",
    "        keep_words = [words[i] for i in range(l) if i not in drop_index]\n",
    "        return ' '.join(keep_words)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        return ex\n",
    "        #words = ex['sents'].split()\n",
    "        #guess = np.random.random()\n",
    "\n",
    "        #if self.training:\n",
    "        #    if guess > 0.5:\n",
    "        #        sents = self.dropout(words,p=0.3)\n",
    "        #    else:\n",
    "        #        sents = self.shuffle(words)\n",
    "        #else:\n",
    "        #    sents = ex['sents']\n",
    "        #return {'id':ex['id'],'sents':sents,'labels':ex['labels']}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "professional-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicModule import BasicModule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN_RNN(BasicModule):\n",
    "    def __init__(self, args, embed=None):\n",
    "        super(RNN_RNN, self).__init__(args)\n",
    "        self.model_name = 'RNN_RNN'\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num\n",
    "        D = args.embed_dim\n",
    "        H = args.hidden_size\n",
    "        S = args.seg_num\n",
    "        P_V = args.pos_num\n",
    "        P_D = args.pos_dim\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.word_RNN = nn.GRU(\n",
    "                        input_size = D,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = 2*H,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.fc = nn.Linear(2*H,2*H)\n",
    "\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "\n",
    "    def max_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.max_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def avg_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(F.avg_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "    def forward(self,x,doc_lens):\n",
    "        sent_lens = torch.sum(torch.sign(x),dim=1).data \n",
    "        x = self.embed(x)                                                      # (N,L,D)\n",
    "        # word level GRU\n",
    "        H = self.args.hidden_size\n",
    "        x = self.word_RNN(x)[0]                                                 # (N,2*H,L)\n",
    "        #word_out = self.avg_pool1d(x,sent_lens)\n",
    "        word_out = self.max_pool1d(x,sent_lens)\n",
    "        # make sent features(pad with zeros)\n",
    "        x = self.pad_doc(word_out,doc_lens)\n",
    "\n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
    "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            if self.args.device is not None:\n",
    "                s = s.cuda()\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                if self.args.device is not None:\n",
    "                    abs_index = abs_index.cuda()\n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                if self.args.device is not None:\n",
    "                    rel_index = rel_index.cuda()\n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
    "                s = s + torch.mm(prob,h)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigirdata():\n",
    "    \"\"\"\n",
    "    MAIN DATASETS FORMAT WAS LIKE\n",
    "    ----\n",
    "    'doc': 'johnathan isaby , chief executive of the tax payers â\\x80\\x99 alliance , \n",
    "    called the number of communications staff â\\x80\\x98 blatant hypocrisy â\\x80\\x99 taxpayer\n",
    "    money is being used to fund an â\\x80\\x98 army â\\x80\\x99 of spin doctors with more than \n",
    "    3,400 press officers employed by local councils across the uk\\nthe number of communications\n",
    "    employees working for local government is more than two times that working across 20 central government \n",
    "    departments\\nlondon has at least 425 marketing staff and press officers working across its local authorities \n",
    "    , four times more than the entire editorial staff at the evening standard , the times reported\\nnearly 45 councils\n",
    "    employ 20 or more communications staff each , with manchester city council the worst offender for its size\\nit has 77 \n",
    "    individuals working for it in pr and similar areas\\nleeds city council comes second with 47 staff and bristol and sheffield\n",
    "    city come joint third\\nglasgow city council and the kirklees metropolitan borough council in yorkshire each employed 40 pr officers \n",
    "    and individuals in similar fields\\nthe figures were unveiled in a freedom of information act request by press gazette to 435 city \n",
    "    , district and borough councils to which 405 replied with information about their communications staff\\nthe figures were unveiled \n",
    "    in a freedom of information act request by press gazette to 435 city , district and borough councils to which 405 replied with information\n",
    "    about their communications staff\\npress gazette made a similar request to central government over its pr staff during another investigation \n",
    "    last year\\nfigures revealed that the home office had 275 full - time positions in marketing , press relations and similar areas\\nmanchester\n",
    "    city council ( pictured ) was the worst offender for its size\\nit had 77 individuals working for it in pr and similar areas the cabinet office\n",
    "    had 205 pr staff and the department for work and pensions had 184\\njohnathan isaby , the chief executive of the tax payers â\\x80\\x99 alliance \n",
    "    was quoted as saying it was â\\x80\\x98 blatant hypocrisy â\\x80\\x99 for councils to comment so often on the need for necessary saving while\n",
    "    keeping on so many staff for communications\\nhe added that the next central government needed to crack down on the â\\x80\\x98 army of\n",
    "    propagandists â\\x80\\x99 funded by the taxpayer\\nhe said those footing the bill expected their money to go to front line services \n",
    "    not spin doctors .', \n",
    "    'labels': '1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n1\\n1', \n",
    "    'summaries': \"taxpayer money used to pay for an ' army ' of spin doctors in local areas\\nthe number \n",
    "    of communications staff in local government more than 3,400\\ntotal is more than double the number working across central government\\nlondon has 425 members of pr staff working across its local authorities\"} \n",
    "    ----\n",
    "    SO THIS DATA SHOULD HAVE SAME FORMAT. THIS PART READ FILES FROM TXT FILES AND OUTPUTS A FORMAT LIKE ABOVE\n",
    " \n",
    "    \"\"\"\n",
    "    filecount=0\n",
    "    abslinecount=0\n",
    "    dir = \"C:/Users/user/Desktop/sondönem/bitirme/sigir/SIGIR2018_Extracts/SIGIR_Sessions\" \n",
    "    finaldoc=[]\n",
    "    mydict={}\n",
    "    finalstr=\"\"\n",
    "    newlinecount=0\n",
    "    labels=\"1\"\n",
    "    keys=['doc', 'labels','summaries']\n",
    "    for i in keys:\n",
    "        mydict[i] = \"\"\n",
    "    for dirPath, foldersInDir,fileName in os.walk(dir):\n",
    "        if fileName is not []:\n",
    "            for file in fileName:\n",
    "                if file.endswith('t.txt'):\n",
    "                    loc = os.sep.join([dirPath,file])\n",
    "                    abstract=open(loc,encoding=\"utf8\")\n",
    "                    abst=abstract.read()\n",
    "\n",
    "                    lines = abst.split(\"\\n\")\n",
    "                    #print(lines)\n",
    "                    #print(\"--------------------------\")\n",
    "                    m = re.findall('1\">(.+?).</', str(lines))\n",
    "                    if m:\n",
    "                        for t in m:\n",
    "                            abslinecount=abslinecount+1\n",
    "                            t = t.replace(\"',\", \"\")\n",
    "                            t = t.replace(\"'\", \"\")\n",
    "                            finalstr=finalstr+t+\"\\n\"\n",
    "                            mydict['summaries']=finalstr\n",
    "                abssizes.append(abslinecount)\n",
    "                abslinecount=0\n",
    "                finalstr=\"\"\n",
    "                \n",
    "                if file.endswith('o.txt'):\n",
    "                \n",
    "                    #print(\"readin intro\")\n",
    "                    loc = os.sep.join([dirPath,file])\n",
    "                    doc=open(loc,encoding=\"utf8\")\n",
    "                    doc=doc.read()\n",
    "                    #print(doc)\n",
    "                    #print()\n",
    "                    lines = doc.split(\"\\n\")\n",
    "                    #print(lines)\n",
    "                    #print(\"--------------------------\")\n",
    "                    #text = 'gfgfdAAA1234ZZZuijjk'\n",
    "                    #while m!=[]:\n",
    "                    m = re.findall('\"[0-9]\">(.+?).</', str(lines))\n",
    "                    if m:\n",
    "                        for t in m:\n",
    "                            #print(t)\n",
    "                            #labels=labels+\"\"\n",
    "                           \n",
    "                            \n",
    "                            #labels=labels.replace(\" \",\"\")\n",
    "                            newlinecount=newlinecount+1\n",
    "                            t = t.replace(\"',\", \"\").lower()\n",
    "                            t = t.replace(\"'\", \"\")\n",
    "                            finalstr=finalstr+t+\"\\n\"\n",
    "                            labels=labels+\"k\\n\"+\"1\"\n",
    "                            labels=labels.replace(\"k\", \"\")\n",
    "                            mydict['doc']=finalstr\n",
    "                            lb=str(labels)\n",
    "                            mydict['labels']=str(lb)\n",
    "                            \n",
    "                     \n",
    "                    #print(\"-------------\")\n",
    "                    finaldoc.append(dict(mydict))\n",
    "                    #print(finaldoc[filecount])\n",
    "                    #print(\"----------\")\n",
    "                    filecount+=1\n",
    "                    finalstr=\"\"\n",
    "                    labels=\"1\"\n",
    "                    newlinecount=0\n",
    "                    #print(finaldoc)\n",
    "                    \n",
    "            print(\"SIGIR DATA GENERATED\")\n",
    "    return finaldoc\n",
    "\"\"\"\n",
    "FILES LIKE\n",
    "--------\n",
    "{'doc': 'Traditionally, medical doctors and care providers have been the main source of information for patients who suffer \n",
    "from chronic or life-threatening diseases\\n However, with the advent of the Internet and the creation of many online health\n",
    "communities (OHCs), e.g., Everyday Health, Cancer Survivors’ Network, and WebMD, patients use these health communities increasingly \n",
    "as an integral source for finding health-related information [5]\\nOHCs provide an environment for patients, their family members and\n",
    "friends to interact with other participants and share experiences and information (e.g., recommendations and feedback) on issues related \n",
    "to prescribed medicines, side effects, therapeutic processes, mental health, and feelings\\nTable 1 shows examples of posts that contain \n",
    "health-related information shared among patients in an online cancer community\\nThis information is very unique and is often not available\n",
    "elsewhere, e.g., referring to the medication Sertraline, a patient writes: Doctors also say it helps with hot flashes.\\nI don’t know about\n",
    "that since I still get them (see Example 1 in the table)\\nSeveral studies showed that using OHCs to obtain information from people who went\n",
    "through the same or similar experiences (either by direct interactions or sifting through the online posts) brings better feelings and fewer\n",
    "mortality odds to patients [8].\\nThus, the large and growing amounts of user-generated content in OHCs need to be accurately classified for a\n",
    "variety of applications, e.g., designing smart information retrieval systems for content recommendation\\nRecent computational studies in OHCs \n",
    "started to investigate the high level identification of informational posts [1, 19], however, with no emphasis on the unique challenges associated \n",
    "with the detection of the information type, e.g., therapeutic procedures vs. side effects\\nA deep understanding of the text and the writer’s intention\n",
    "is required in order to correctly extract the types of information present in OHCs messages\\nExample 1 in Table 1 refers to therapeutic procedure, whereas \n",
    "Example 2 refers to side effects through various medication (Sertraline and Anzamet, respectively)\\nIn this paper, we propose to analyze messages in OHCs t\n",
    "o extract the information type that they contain, i.e., therapeutic procedures (any medical treatment, activity, or behavior that have a positive impact on\n",
    "patients’ health, precisely, can help prevent, cure or improve a patient’s condition) and side effects (any medical treatment, activity, or behavior that \n",
    "have a negative impact on patients’ health, precisely, a secondary, often undesirable effect of a drug or medical treatment)\\nTo achieve this, we design a computational model\n",
    "that is able to exploit the semantic information from text, and coherently combines high-level (abstract) features with surface-level and lexicon-based\n",
    "features.\\nOur contributions are as follows\\n(1) We propose to extract fine-grained information types from messages posted in OHCs.\\nIdentifying information\n",
    "types provides doctors, health practitioners and OHCs’ moderators with an insightful view of patients’ physical status during various treatments\\nIn addition,\n",
    "it can provide new diagnosed patients with information about what they should expect throughout their treatments and help them in making informed decisions about their\n",
    "disease more effectively [14]\\nTo our knowledge, we are the first to address fine-grained information type extraction in OHCs\\n(2) We design and explore a computational\n",
    "model that can identify messages belonging to therapeutic procedures and side effects with high accuracy\\nOur model is a hybrid neural network combined with lexicon-based \n",
    "features\\n(3) We show empirically that our model significantly outperforms strong baselines and prior works and continues to perform well even in the absence of lexicon-based features\\n', \n",
    "'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', \n",
    "'summaries': 'Online health communities have become a medium for patients to share their personal experiences and interact with peers \n",
    "on topics related to a disease, medication, side effects, and therapeutic processes\\nAnalyzing informational posts in these communities \n",
    "can provide an insightful view about the dominant health issues and can help patients find the information that they need easier\\nIn this\n",
    "paper, we propose a computational model that mines user content in online health communities to detect positive experiences and suggestions \n",
    "on health improvement as well as negative impacts or side effects that cause suffering throughout fighting with a disease\\nSpecifically, \n",
    "we combine high-level, abstract features extracted from a convolutional neural network with lexicon-based features and features extracted\n",
    "from a long short term memory network to capture the semantics in the data\\nWe show that our model, with and without lexicon-based features, outperforms strong baselines\\n'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automatic-lesbian",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'models' from 'C:\\\\Users\\\\user\\\\metin özütleme\\\\hphaos summarunner\\\\models\\\\__init__.py'>\n",
      "0\n",
      "True\n",
      "0 device 0 <torch.cuda.device object at 0x00000218B2119C88> count 1 name GeForce GTX 1650\n",
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import models\n",
    "#import utils\n",
    "import argparse,random,logging,numpy,os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "print(models)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')\n",
    "parser = argparse.ArgumentParser(description='extractive summary')\n",
    "# model\n",
    "abssizes=[]\n",
    "parser.add_argument('-save_dir',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/checkpoints/')\n",
    "parser.add_argument('-embed_dim',type=int,default=100)\n",
    "parser.add_argument('-embed_num',type=int,default=100)\n",
    "parser.add_argument('-pos_dim',type=int,default=50)\n",
    "parser.add_argument('-pos_num',type=int,default=100)\n",
    "parser.add_argument('-seg_num',type=int,default=10)\n",
    "parser.add_argument('-kernel_num',type=int,default=100)\n",
    "parser.add_argument('-kernel_sizes',type=str,default='3,4,5')\n",
    "parser.add_argument('-model',type=str,default='RNN_RNN')\n",
    "parser.add_argument('-hidden_size',type=int,default=200)\n",
    "# train\n",
    "parser.add_argument('-lr',type=float,default=1e-3)\n",
    "parser.add_argument('-batch_size',type=int,default=32)\n",
    "parser.add_argument('-epochs',type=int,default=3)\n",
    "parser.add_argument('-seed',type=int,default=1)\n",
    "parser.add_argument('-train_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/train.json')\n",
    "parser.add_argument('-val_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/val.json')\n",
    "parser.add_argument('-embedding',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/embedding.npz')\n",
    "parser.add_argument('-word2id',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/word2id.json')\n",
    "parser.add_argument('-report_every',type=int,default=1500)\n",
    "parser.add_argument('-seq_trunc',type=int,default=50)\n",
    "parser.add_argument('-max_norm',type=float,default=1.0)\n",
    "# test\n",
    "parser.add_argument('-load_dir',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/checkpoints/CNN_RNN_seed_1.pt')\n",
    "parser.add_argument('-sigirseed',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/sigir/sigircheckpoints/CNN_RNN_seed_1.pt')\n",
    "parser.add_argument('-test_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/test.json')\n",
    "parser.add_argument('-ref',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/outputs/ref')\n",
    "parser.add_argument('-hyp',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/outputs/hyp')\n",
    "parser.add_argument('-filename',type=str,default='x.txt') # TextFile to be summarized\n",
    "parser.add_argument('-topk',type=int,default=3)\n",
    "# device\n",
    "parser.add_argument('-device',type=int,default=0)\n",
    "# option\n",
    "parser.add_argument('-test',action='store_true')\n",
    "parser.add_argument('-debug',action='store_true')\n",
    "parser.add_argument('-predict',action='store_true')\n",
    "args = parser.parse_args()\n",
    "print(args.device)\n",
    "\n",
    "use_gpu = args.device is not None\n",
    "#use_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available() and not use_gpu:\n",
    "    print(\"WARNING: You have a CUDA device, should run with -device 0\")\n",
    "    if torch.cuda.is_available():  \n",
    "        dev = \"cuda:0\" \n",
    "        use_gpu=dev\n",
    "        torch.cuda.set_device(0)\n",
    "    else:  \n",
    "        dev = \"cpu\"  \n",
    "        device = torch.device(dev)  \n",
    "        \n",
    "print(use_gpu)\n",
    "print(torch.cuda.current_device(),\"device 0\",torch.cuda.device(0),\"count\",torch.cuda.device_count(),\"name\", torch.cuda.get_device_name(0) )\n",
    "\n",
    "# set cuda device and seed\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.device)\n",
    "    print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "numpy.random.seed(args.seed) \n",
    "def eval(net,vocab,data_iter,criterion):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    for batch in data_iter:\n",
    "        features,targets,_,doc_lens = vocab.make_features(batch)\n",
    "        features,targets = Variable(features), Variable(targets.float())\n",
    "        if use_gpu:\n",
    "\n",
    "            features = features.cuda()\n",
    "            targets = targets.cuda()\n",
    "        probs = net(features,doc_lens)\n",
    "        loss = criterion(probs,targets)\n",
    "        total_loss += loss.data\n",
    "        batch_num += 1\n",
    "    loss = total_loss / batch_num\n",
    "    net.train()\n",
    "    print(\"done eval\")\n",
    "    return loss\n",
    "\n",
    "def train():\n",
    "    logging.info('Loading vocab,train and val dataset.Wait a second,please')\n",
    "    \n",
    "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
    "    with open(args.word2id,encoding = 'utf8') as f:\n",
    "        word2id = json.load(f)\n",
    "    vocab = Vocab(embed, word2id)\n",
    "\n",
    "    with open(args.train_dir,encoding=\"utf8\") as f:\n",
    "        examples = [json.loads(line) for line in f]\n",
    "        print(examples[0])\n",
    "    train_dataset = Dataset(examples)\n",
    "    with open(args.val_dir,encoding=\"utf8\") as f:\n",
    "        examples = [json.loads(line) for line in f]\n",
    "    val_dataset = Dataset(examples)\n",
    "    # update args\n",
    "    args.embed_num = embed.size(0)\n",
    "    args.embed_dim = embed.size(1)\n",
    "    args.kernel_sizes = [int(ks) for ks in args.kernel_sizes.split(',')]\n",
    "    # build model\n",
    "    net = getattr(models,args.model)(args,embed)######################33232132131232131232130123912931203912093129301293012930129302193012930219\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    # load dataset\n",
    "    train_iter = DataLoader(dataset=train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True)\n",
    "    val_iter = DataLoader(dataset=val_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False)\n",
    "    # loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    # model info\n",
    "    print(net)\n",
    "    params = sum(p.numel() for p in list(net.parameters())) / 1e6\n",
    "    print('#Params: %.1fM' % (params))\n",
    "    \n",
    "    min_loss = float('inf')\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
    "    net.train()\n",
    "    t1 = time() \n",
    "    for epoch in range(1,args.epochs+1):\n",
    "        for i,batch in enumerate(train_iter):\n",
    "            features,targets,_,doc_lens = vocab.make_features(batch)\n",
    "            features,targets = Variable(features), Variable(targets.float())\n",
    "            if use_gpu:\n",
    "                features = features.cuda()\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            probs = net(features,doc_lens)\n",
    "            loss = criterion(probs,targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm(net.parameters(), args.max_norm)\n",
    "            optimizer.step()\n",
    "           \n",
    "            if args.debug:\n",
    "                print('Batch ID:%d Loss:%f' %(i,loss.data[0]))\n",
    "                continue\n",
    "            if i % args.report_every == 0:\n",
    "                cur_loss = eval(net,vocab,val_iter,criterion)\n",
    "                if cur_loss < min_loss:\n",
    "                    min_loss = cur_loss\n",
    "                    best_path = net.save()\n",
    "                logging.info('Epoch: %2d Min_Val_Loss: %f Cur_Val_Loss: %f'\n",
    "                        % (epoch,min_loss,cur_loss))\n",
    "            \n",
    "    t2 = time()\n",
    "    print(\"train ended\")\n",
    "    logging.info('Total Cost:%f h'%((t2-t1)/3600))\n",
    "    \n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"test started\")\n",
    "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
    "    with open(args.word2id,encoding=\"utf8\") as f:\n",
    "        word2id = json.load(f)\n",
    "    vocab = Vocab(embed, word2id)\n",
    "\n",
    "    with open(args.test_dir,encoding=\"utf8\") as f:\n",
    "        examples = [json.loads(line) for line in f]\n",
    "        print(examples[1],\"\\n\",examples[2])\n",
    "  \n",
    "    test_dataset = Dataset(examples)\n",
    "\n",
    "    test_iter = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False)\n",
    "    \n",
    "    if use_gpu:\n",
    "        checkpoint = torch.load(args.load_dir)#-load_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/checkpoint/RNN_RNN_seed_1.pt')\n",
    "    else:\n",
    "        checkpoint = torch.load(args.load_dir, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    # checkpoint['args']['device'] saves the device used as train time\n",
    "    # if at test time, we are using a CPU, we must override device to None\n",
    "    if not use_gpu:\n",
    "        checkpoint['args'].device = None\n",
    "    net = getattr(models,checkpoint['args'].model)(checkpoint['args'])\n",
    "    net.load_state_dict(checkpoint['model'])\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    doc_num = len(test_dataset)\n",
    "    time_cost = 0\n",
    "    file_id = 1\n",
    "    for batch in tqdm(test_iter):\n",
    "        features,_,summaries,doc_lens = vocab.make_features(batch)\n",
    "        t1 = time()\n",
    "        if use_gpu:\n",
    "            probs = net(Variable(features).cuda(), doc_lens)\n",
    "        else:\n",
    "            probs = net(Variable(features), doc_lens)\n",
    "        t2 = time()\n",
    "        time_cost += t2 - t1\n",
    "        start = 0\n",
    "        for doc_id,doc_len in enumerate(doc_lens):\n",
    "            stop = start + doc_len\n",
    "            prob = probs[start:stop]\n",
    "            topk = min(args.topk,doc_len)\n",
    "            #topk=min(abssizes[file_id-1],doc_len)\n",
    "            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n",
    "            topk_indices.sort()\n",
    "            doc = batch['doc'][doc_id].split('\\n')[:doc_len]\n",
    "            hyp = [doc[index] for index in topk_indices]\n",
    "            ref = summaries[doc_id]\n",
    "            with open(os.path.join(args.ref,str(file_id)+'.txt'), 'w',encoding=\"utf8\") as f:\n",
    "                f.write(ref)\n",
    "            with open(os.path.join(args.hyp,str(file_id)+'.txt'), 'w',encoding=\"utf8\") as f:\n",
    "                f.write('\\n'.join(hyp))\n",
    "            start = stop\n",
    "            file_id = file_id + 1\n",
    "    print('Speed: %.2f docs / s' % (doc_num / time_cost))\n",
    "    print(\"test ended\")\n",
    "\n",
    "\n",
    "def predict(examples):\n",
    "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
    "    with open(args.word2id,encoding=\"utf8\") as f:\n",
    "        word2id = json.load(f)\n",
    "    vocab = Vocab(embed, word2id)\n",
    "    pred_dataset = Dataset(examples)\n",
    "\n",
    "    pred_iter = DataLoader(dataset=pred_dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False)\n",
    "    if use_gpu:\n",
    "        checkpoint = torch.load(args.load_dir)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.load_dir, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    # checkpoint['args']['device'] saves the device used as train time\n",
    "    # if at test time, we are using a CPU, we must override device to None\n",
    "    if not use_gpu:\n",
    "        checkpoint['args'].device = None\n",
    "    net = getattr(AttnRNN,checkpoint['args'].model)(checkpoint['args'])\n",
    "    net.load_state_dict(checkpoint['model'])\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    doc_num = len(pred_dataset)\n",
    "    time_cost = 0\n",
    "    file_id = 1\n",
    "    for batch in tqdm(pred_iter):\n",
    "        features, doc_lens = vocab.make_predict_features(batch)\n",
    "        t1 = time()\n",
    "        if use_gpu:\n",
    "            probs = net(Variable(features).cuda(), doc_lens)\n",
    "        else:\n",
    "            probs = net(Variable(features), doc_lens)\n",
    "        t2 = time()\n",
    "        time_cost += t2 - t1\n",
    "        start = 0\n",
    "        for doc_id,doc_len in enumerate(doc_lens):\n",
    "            stop = start + doc_len\n",
    "            prob = probs[start:stop]\n",
    "            topk = min(args.topk,doc_len)\n",
    "            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n",
    "            topk_indices.sort()\n",
    "            doc = batch[doc_id].split('. ')[:doc_len]\n",
    "            hyp = [doc[index] for index in topk_indices]\n",
    "            with open(os.path.join(args.hyp,str(file_id)+'.txt'), 'w') as f:\n",
    "                f.write('. '.join(hyp))\n",
    "            start = stop\n",
    "            file_id = file_id + 1\n",
    "    print('Speed: %.2f docs / s' % (doc_num / time_cost))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "coastal-yesterday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-24 00:00:49,631 [INFO] Loading vocab,train and val dataset.Wait a second,please\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc': \"by daily mail reporter last updated at 11:49 am on 5th october 2011 documents obtained under a federal freedom of information act lawsuit filed by judicial watch detail costs incurred during the first lady 's trip to africa and botswana in june - with the cost of firing up ' air force 2 ' alone amounting to $ 424,142\\nthe white house earlier professed the purpose of the trip was to help ' youth leadership , education , health and wellness ' in africa\\n' air force 2 ' : obama waves as she boards her private plane after a week - long trip to africa in june\\ncosts of the flight are estimated to tally more than $ 424,000 greetings : mrs obama was met by excited children when she arrived to botswana on june 24 warm welcome : mrs obama with daughters sasha , far right , and malia , right , were greeted by traditional dancers as they arrive in gaborone , botswana judicial watch said it based the jet costs on the u.s. department of defense 's published hourly rates for the c-32a aircraft - a specifically configured military version of the boeing 757\\nduring the june 21 - 27 trip , the group accounted for 34.8 flight hours at $ 12,188 per hour\\nthat does not include a tally of local transportation , secret service protection , food for the 21 family and staff members - and the cost of pre-trip preparations contributing to the final amount\\nthose figures have yet to be disclosed\\naccording to the release , the passenger manifests confirm the presence of obama 's daughter 's , malia and sasha on the trip\\nthe two girls are listed as ' senior staff\\n' family safari : mrs obama , joined by her daughters sasha and malia and her mother marian robinson enjoyed a ride through madikwe game reserve during the trip encouraging : mrs obama stands with mamphela ramphele , fifth from right , and high school students after she answered students ' questions at the university of cape town in cape town , africa on june 23 playful : mrs obama ( r ) and daughters sasha ( c ) and malia take turns read to students during a visit to the emthonjeni community center in zandspruit township , johannesburg , africa on june 21 goals : mrs obama said the trip would help ' youth leadership , education , health and wellness ' in southern africa , according to the white house the manifests also list mrs obama 's mother , marian robinson , and niece and nephew , leslie and avery robinson , as well mrs. obama 's make - up and hairstylist ( carl ray and johnny wright )\\nthe expense records also show $ 928.44 was spent for ' bulk food ' purchases on flight\\noverall , during the trip , 192 meals were served for the 21 passengers on board\\nthe ' professed purpose ' of the trip ' was to encourage young people living in the two growing democracies to become involved in national affairs ; and during her scheduled stops in pretoria and cape town , africa and in gaborone , the capital of botswana , the first lady used the opportunity to speak on education , health and wellness issues , ' the report states\\nmemorable : mrs obama also took her daughters to visit nelson mandela , a visit which she called ' surreal ' family : from left , mrs obama 's niece leslie robinson , malia , archbishop desmond tutu , mrs obama , sasha obama and nephew avery robinsona at cape town stadium in africa on june 23 business : mrs obama met botswana 's president lt. gen. seretse khama ian khama during her visit mission expense records and passenger manifests from the air force related to the june 21 - 27 , 2011 trip taken by first lady obama , her family and her staff to africa and botswana , according to watchdog group judicial watch include : the malia also enjoyed a meet up with nelson mandela\\nhowever , the trip also included visits to historical landmarks , and ended with a private family safari at a africa game reserve before the group returned to washington on june 27\\njudicial watch , which investigates and fights government corruption , said in a press release issued today it is investigating the purpose and itinerary of the trip\\non june 28 , 2011 , the group filed a freedom of information act request seeking the mission taskings , transportation records , and passenger manifests for obama 's africa trip\\ndocuments were only provided after judicial watch sued to obtain the documents , according to the release\\n' this trip was as much an opportunity for the obama family to go on a safari as it was a trip to conduct government business , ' said judicial watch president tom fitton\\n' this junket wasted tax dollars and the resources of our overextended military\\nno wonder we had to sue to pry loose this information\", 'labels': '1\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0', 'summaries': \"costs of ' air force 2 ' flight amount to over $ 424,000 , air force manifests indicate\\nin - flight meals for 21 passengers cost over $ 900\\nfirst daughters malia , 13 , and sasha obama , 10 , listed as ' senior staff ' in manifests\"}\n",
      "RNN_RNN(\n",
      "  (abs_pos_embed): Embedding(100, 50)\n",
      "  (rel_pos_embed): Embedding(10, 50)\n",
      "  (embed): Embedding(153824, 100, padding_idx=0)\n",
      "  (word_RNN): GRU(100, 200, batch_first=True, bidirectional=True)\n",
      "  (sent_RNN): GRU(400, 200, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (content): Linear(in_features=400, out_features=1, bias=False)\n",
      "  (salience): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
      "  (novelty): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
      "  (abs_pos): Linear(in_features=50, out_features=1, bias=False)\n",
      "  (rel_pos): Linear(in_features=50, out_features=1, bias=False)\n",
      ")\n",
      "#Params: 17.0M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:155: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-00d7c3c8c71e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#testsigir()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"everything done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c8ad651ac62f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[0mcur_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcur_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[0mmin_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c8ad651ac62f>\u001b[0m in \u001b[0;36meval\u001b[1;34m(net, vocab, data_iter, criterion)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc_lens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\metin özütleme\\hphaos summarunner\\models\\RNN_RNN.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, doc_lens)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;31m# classification layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0msalience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msalience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[0mnovelty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnovelty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mabs_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input1, input2)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbilinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbilinear\u001b[1;34m(input1, input2, weight, bias)\u001b[0m\n\u001b[0;32m   1771\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0mare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1772\u001b[0m     \"\"\"\n\u001b[1;32m-> 1773\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbilinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    train()\n",
    "    test()\n",
    "    #testsigir()\n",
    "    print(\"everything done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-journalism",
   "metadata": {},
   "source": [
    "######SIGIR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "martial-minute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'models' from 'C:\\\\Users\\\\user\\\\metin özütleme\\\\hphaos summarunner\\\\models\\\\__init__.py'>\n",
      "WARNING: You have a CUDA device, should run with -device 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def sigirdata():\n",
    "    \"\"\"\n",
    "    MAIN DATASETS FORMAT WAS LIKE\n",
    "    ----\n",
    "    'doc': 'johnathan isaby , chief executive of the tax payers â\\x80\\x99 alliance , \n",
    "    called the number of communications staff â\\x80\\x98 blatant hypocrisy â\\x80\\x99 taxpayer\n",
    "    money is being used to fund an â\\x80\\x98 army â\\x80\\x99 of spin doctors with more than \n",
    "    3,400 press officers employed by local councils across the uk\\nthe number of communications\n",
    "    employees working for local government is more than two times that working across 20 central government \n",
    "    departments\\nlondon has at least 425 marketing staff and press officers working across its local authorities \n",
    "    , four times more than the entire editorial staff at the evening standard , the times reported\\nnearly 45 councils\n",
    "    employ 20 or more communications staff each , with manchester city council the worst offender for its size\\nit has 77 \n",
    "    individuals working for it in pr and similar areas\\nleeds city council comes second with 47 staff and bristol and sheffield\n",
    "    city come joint third\\nglasgow city council and the kirklees metropolitan borough council in yorkshire each employed 40 pr officers \n",
    "    and individuals in similar fields\\nthe figures were unveiled in a freedom of information act request by press gazette to 435 city \n",
    "    , district and borough councils to which 405 replied with information about their communications staff\\nthe figures were unveiled \n",
    "    in a freedom of information act request by press gazette to 435 city , district and borough councils to which 405 replied with information\n",
    "    about their communications staff\\npress gazette made a similar request to central government over its pr staff during another investigation \n",
    "    last year\\nfigures revealed that the home office had 275 full - time positions in marketing , press relations and similar areas\\nmanchester\n",
    "    city council ( pictured ) was the worst offender for its size\\nit had 77 individuals working for it in pr and similar areas the cabinet office\n",
    "    had 205 pr staff and the department for work and pensions had 184\\njohnathan isaby , the chief executive of the tax payers â\\x80\\x99 alliance \n",
    "    was quoted as saying it was â\\x80\\x98 blatant hypocrisy â\\x80\\x99 for councils to comment so often on the need for necessary saving while\n",
    "    keeping on so many staff for communications\\nhe added that the next central government needed to crack down on the â\\x80\\x98 army of\n",
    "    propagandists â\\x80\\x99 funded by the taxpayer\\nhe said those footing the bill expected their money to go to front line services \n",
    "    not spin doctors .', \n",
    "    'labels': '1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n1\\n1', \n",
    "    'summaries': \"taxpayer money used to pay for an ' army ' of spin doctors in local areas\\nthe number \n",
    "    of communications staff in local government more than 3,400\\ntotal is more than double the number working across central government\\nlondon has 425 members of pr staff working across its local authorities\"} \n",
    "    ----\n",
    "    SO THIS DATA SHOULD HAVE SAME FORMAT. THIS PART READ FILES FROM TXT FILES AND OUTPUTS A FORMAT LIKE ABOVE\n",
    " \n",
    "    \"\"\"\n",
    "    filecount=0\n",
    "    abslinecount=0\n",
    "    dir = \"C:/Users/user/Desktop/sondönem/bitirme/sigir/SIGIR2018_Extracts/SIGIR_Sessions\" \n",
    "    finaldoc=[]\n",
    "    mydict={}\n",
    "    finalstr=\"\"\n",
    "    newlinecount=0\n",
    "    labels=\"1\"\n",
    "    keys=['doc', 'labels','summaries']\n",
    "    for i in keys:\n",
    "        mydict[i] = \"\"\n",
    "    for dirPath, foldersInDir,fileName in os.walk(dir):\n",
    "        if fileName is not []:\n",
    "            for file in fileName:\n",
    "                if file.endswith('t.txt'):\n",
    "                    loc = os.sep.join([dirPath,file])\n",
    "                    abstract=open(loc,encoding=\"utf8\")\n",
    "                    abst=abstract.read()\n",
    "\n",
    "                    lines = abst.split(\"\\n\")\n",
    "                    #print(lines)\n",
    "                    #print(\"--------------------------\")\n",
    "                    m = re.findall('1\">(.+?).</', str(lines))\n",
    "                    if m:\n",
    "                        for t in m:\n",
    "                            abslinecount=abslinecount+1\n",
    "                            t = t.replace(\"',\", \"\")\n",
    "                            t = t.replace(\"'\", \"\")\n",
    "                            finalstr=finalstr+t+\"\\n\"\n",
    "                            mydict['summaries']=finalstr\n",
    "                abssizes.append(abslinecount)\n",
    "                abslinecount=0\n",
    "                finalstr=\"\"\n",
    "                \n",
    "                if file.endswith('o.txt'):\n",
    "                \n",
    "                    #print(\"readin intro\")\n",
    "                    loc = os.sep.join([dirPath,file])\n",
    "                    doc=open(loc,encoding=\"utf8\")\n",
    "                    doc=doc.read()\n",
    "                    #print(doc)\n",
    "                    #print()\n",
    "                    lines = doc.split(\"\\n\")\n",
    "                    #print(lines)\n",
    "                    #print(\"--------------------------\")\n",
    "                    #text = 'gfgfdAAA1234ZZZuijjk'\n",
    "                    #while m!=[]:\n",
    "                    m = re.findall('\"[0-9]\">(.+?).</', str(lines))\n",
    "                    if m:\n",
    "                        for t in m:\n",
    "                            #print(t)\n",
    "                            #labels=labels+\"\"\n",
    "                           \n",
    "                            \n",
    "                            #labels=labels.replace(\" \",\"\")\n",
    "                            newlinecount=newlinecount+1\n",
    "                            t = t.replace(\"',\", \"\").lower()\n",
    "                            t = t.replace(\"'\", \"\")\n",
    "                            finalstr=finalstr+t+\"\\n\"\n",
    "                            labels=labels+\"k\\n\"+\"1\"\n",
    "                            labels=labels.replace(\"k\", \"\")\n",
    "                            mydict['doc']=finalstr\n",
    "                            lb=str(labels)\n",
    "                            mydict['labels']=str(lb)\n",
    "                            \n",
    "                     \n",
    "                    #print(\"-------------\")\n",
    "                    finaldoc.append(dict(mydict))\n",
    "                    #print(finaldoc[filecount])\n",
    "                    #print(\"----------\")\n",
    "                    filecount+=1\n",
    "                    finalstr=\"\"\n",
    "                    labels=\"1\"\n",
    "                    newlinecount=0\n",
    "                    #print(finaldoc)\n",
    "                    \n",
    "    print(\"SIGIR DATA GENERATED\")\n",
    "    return finaldoc\n",
    "\"\"\"\n",
    "FILES LIKE\n",
    "--------\n",
    "{'doc': 'Traditionally, medical doctors and care providers have been the main source of information for patients who suffer \n",
    "from chronic or life-threatening diseases\\n However, with the advent of the Internet and the creation of many online health\n",
    "communities (OHCs), e.g., Everyday Health, Cancer Survivors’ Network, and WebMD, patients use these health communities increasingly \n",
    "as an integral source for finding health-related information [5]\\nOHCs provide an environment for patients, their family members and\n",
    "friends to interact with other participants and share experiences and information (e.g., recommendations and feedback) on issues related \n",
    "to prescribed medicines, side effects, therapeutic processes, mental health, and feelings\\nTable 1 shows examples of posts that contain \n",
    "health-related information shared among patients in an online cancer community\\nThis information is very unique and is often not available\n",
    "elsewhere, e.g., referring to the medication Sertraline, a patient writes: Doctors also say it helps with hot flashes.\\nI don’t know about\n",
    "that since I still get them (see Example 1 in the table)\\nSeveral studies showed that using OHCs to obtain information from people who went\n",
    "through the same or similar experiences (either by direct interactions or sifting through the online posts) brings better feelings and fewer\n",
    "mortality odds to patients [8].\\nThus, the large and growing amounts of user-generated content in OHCs need to be accurately classified for a\n",
    "variety of applications, e.g., designing smart information retrieval systems for content recommendation\\nRecent computational studies in OHCs \n",
    "started to investigate the high level identification of informational posts [1, 19], however, with no emphasis on the unique challenges associated \n",
    "with the detection of the information type, e.g., therapeutic procedures vs. side effects\\nA deep understanding of the text and the writer’s intention\n",
    "is required in order to correctly extract the types of information present in OHCs messages\\nExample 1 in Table 1 refers to therapeutic procedure, whereas \n",
    "Example 2 refers to side effects through various medication (Sertraline and Anzamet, respectively)\\nIn this paper, we propose to analyze messages in OHCs t\n",
    "o extract the information type that they contain, i.e., therapeutic procedures (any medical treatment, activity, or behavior that have a positive impact on\n",
    "patients’ health, precisely, can help prevent, cure or improve a patient’s condition) and side effects (any medical treatment, activity, or behavior that \n",
    "have a negative impact on patients’ health, precisely, a secondary, often undesirable effect of a drug or medical treatment)\\nTo achieve this, we design a computational model\n",
    "that is able to exploit the semantic information from text, and coherently combines high-level (abstract) features with surface-level and lexicon-based\n",
    "features.\\nOur contributions are as follows\\n(1) We propose to extract fine-grained information types from messages posted in OHCs.\\nIdentifying information\n",
    "types provides doctors, health practitioners and OHCs’ moderators with an insightful view of patients’ physical status during various treatments\\nIn addition,\n",
    "it can provide new diagnosed patients with information about what they should expect throughout their treatments and help them in making informed decisions about their\n",
    "disease more effectively [14]\\nTo our knowledge, we are the first to address fine-grained information type extraction in OHCs\\n(2) We design and explore a computational\n",
    "model that can identify messages belonging to therapeutic procedures and side effects with high accuracy\\nOur model is a hybrid neural network combined with lexicon-based \n",
    "features\\n(3) We show empirically that our model significantly outperforms strong baselines and prior works and continues to perform well even in the absence of lexicon-based features\\n', \n",
    "'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', \n",
    "'summaries': 'Online health communities have become a medium for patients to share their personal experiences and interact with peers \n",
    "on topics related to a disease, medication, side effects, and therapeutic processes\\nAnalyzing informational posts in these communities \n",
    "can provide an insightful view about the dominant health issues and can help patients find the information that they need easier\\nIn this\n",
    "paper, we propose a computational model that mines user content in online health communities to detect positive experiences and suggestions \n",
    "on health improvement as well as negative impacts or side effects that cause suffering throughout fighting with a disease\\nSpecifically, \n",
    "we combine high-level, abstract features extracted from a convolutional neural network with lexicon-based features and features extracted\n",
    "from a long short term memory network to capture the semantics in the data\\nWe show that our model, with and without lexicon-based features, outperforms strong baselines\\n'}\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "use_gpu = args.device is not None\n",
    "#use_gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available() and not use_gpu:\n",
    "    print(\"WARNING: You have a CUDA device, should run with -device 0\")\n",
    "    if torch.cuda.is_available():  \n",
    "        dev = \"cuda:0\" \n",
    "        use_gpu=dev\n",
    "        torch.cuda.set_device(0)\n",
    "    else:  \n",
    "        dev = \"cpu\"  \n",
    "        device = torch.device(dev)  \n",
    "         \n",
    "print(use_gpu)\n",
    "print(torch.cuda.current_device(),\"device 0\",torch.cuda.device(0),\"count\",torch.cuda.device_count(),\"name\", torch.cuda.get_device_name(0) )\n",
    "\n",
    "# set cuda device and seed\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.device)\n",
    "    print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "    \"\"\"\n",
    "import json\n",
    "import models\n",
    "#import utils\n",
    "import argparse,random,logging,numpy,os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "print(models)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [INFO] %(message)s')\n",
    "parser = argparse.ArgumentParser(description='extractive summary')\n",
    "# model\n",
    "abssizes=[]\n",
    "parser.add_argument('-save_dir',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/checkpoints/')\n",
    "parser.add_argument('-embed_dim',type=int,default=100)\n",
    "parser.add_argument('-embed_num',type=int,default=100)\n",
    "parser.add_argument('-pos_dim',type=int,default=50)\n",
    "parser.add_argument('-pos_num',type=int,default=100)\n",
    "parser.add_argument('-seg_num',type=int,default=10)\n",
    "parser.add_argument('-kernel_num',type=int,default=100)\n",
    "parser.add_argument('-kernel_sizes',type=str,default='3,4,5')\n",
    "parser.add_argument('-model',type=str,default='AttnRNN')\n",
    "parser.add_argument('-hidden_size',type=int,default=200)\n",
    "# train\n",
    "parser.add_argument('-lr',type=float,default=1e-3)\n",
    "parser.add_argument('-batch_size',type=int,default=32)\n",
    "parser.add_argument('-epochs',type=int,default=3)\n",
    "parser.add_argument('-seed',type=int,default=1)\n",
    "parser.add_argument('-train_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/train.json')\n",
    "parser.add_argument('-val_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/val.json')\n",
    "parser.add_argument('-embedding',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/embedding.npz')\n",
    "parser.add_argument('-word2id',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/word2id.json')\n",
    "parser.add_argument('-report_every',type=int,default=1500)\n",
    "parser.add_argument('-seq_trunc',type=int,default=50)\n",
    "parser.add_argument('-max_norm',type=float,default=1.0)\n",
    "# test\n",
    "parser.add_argument('-load_dir',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/checkpoints/CNN_RNN_seed_1.pt')\n",
    "parser.add_argument('-sigirseed',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/sigir/sigircheckpoints/CNN_RNN_seed_1.pt')\n",
    "parser.add_argument('-test_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/data/test.json')\n",
    "parser.add_argument('-ref',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/outputs/ref')\n",
    "parser.add_argument('-hyp',type=str,default='C:/Users/user/Desktop/sondönem/bitirme/outputs/hyp')\n",
    "parser.add_argument('-filename',type=str,default='x.txt') # TextFile to be summarized\n",
    "parser.add_argument('-topk',type=int,default=3)\n",
    "# device\n",
    "#parser.add_argument('-device',type=int,default=0)\n",
    "parser.add_argument('-device',type=int)\n",
    "# option\n",
    "parser.add_argument('-test',action='store_true')\n",
    "parser.add_argument('-debug',action='store_true')\n",
    "parser.add_argument('-predict',action='store_true')\n",
    "args = parser.parse_args()\n",
    "\"\"\"\n",
    "#dev = \"cpu\"  \n",
    "#device = torch.device(dev)\n",
    "#torch.cuda.set_device(args.device)\n",
    "use_gpu=False\n",
    "#device = torch.device(\"cpu\")\n",
    "#torch.cuda.set_device(args.device)\n",
    "#print(torch.cuda.current_device(),\"device 0\",torch.cuda.device(0),\"count\",torch.cuda.device_count(),\"name\", torch.cuda.get_device_name(0) )\n",
    "\"\"\"\n",
    "use_gpu = args.device is not None\n",
    "\n",
    "if torch.cuda.is_available() and not use_gpu:\n",
    "    print(\"WARNING: You have a CUDA device, should run with -device 0\")\n",
    "\n",
    "# set cuda device and seed\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(args.device)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "numpy.random.seed(args.seed) \n",
    "    \n",
    "def eval(net,vocab,data_iter,criterion):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    for batch in data_iter:\n",
    "        features,targets,_,doc_lens = vocab.make_features(batch)\n",
    "        features,targets = Variable(features), Variable(targets.float())\n",
    "        if use_gpu:\n",
    "            features = features.cuda()\n",
    "            targets = targets.cuda()\n",
    "        probs = net(features,doc_lens)\n",
    "        loss = criterion(probs,targets)\n",
    "        total_loss += loss.data[0]\n",
    "        batch_num += 1\n",
    "    loss = total_loss / batch_num\n",
    "    net.train()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def trainsigir(data):\n",
    "    logging.info('Loading vocab,train and val dataset.Wait a second,please')\n",
    "    \n",
    "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
    "    with open(args.word2id,encoding = 'utf8') as f:\n",
    "        word2id = json.load(f)\n",
    "    vocab = Vocab(embed, word2id)\n",
    "\n",
    "    #with open(args.train_dir,encoding=\"utf8\") as f:\n",
    "        #examples = [json.loads(line) for line in f]\n",
    "    examples=data[:-10]\n",
    "    print(examples[0])\n",
    "    train_dataset = Dataset(examples)\n",
    "    #with open(args.val_dir,encoding=\"utf8\") as f:\n",
    "        #examples = [json.loads(line) for line in f]\n",
    "    examples=data[-10:]\n",
    "    val_dataset = Dataset(examples)\n",
    "    # update args\n",
    "    args.embed_num = embed.size(0)\n",
    "    args.embed_dim = embed.size(1)\n",
    "    args.kernel_sizes = ['3,4,5']\n",
    "    # build model\n",
    "    net = getattr(models,args.model)(args,embed)######################33232132131232131232130123912931203912093129301293012930129302193012930219\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    # load dataset\n",
    "    train_iter = DataLoader(dataset=train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True)\n",
    "    val_iter = DataLoader(dataset=val_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=False)\n",
    "    # loss function\n",
    "    criterion = nn.BCELoss()\n",
    "    # model info\n",
    "    print(net)\n",
    "    params = sum(p.numel() for p in list(net.parameters())) / 1e6\n",
    "    print('#Params: %.1fM' % (params))\n",
    "    \n",
    "    min_loss = float('inf')\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=args.lr)\n",
    "    net.train()\n",
    "    t1 = time() \n",
    "    for epoch in range(1,args.epochs+1):\n",
    "        for i,batch in enumerate(train_iter):\n",
    "            features,targets,_,doc_lens = vocab.make_features(batch)\n",
    "            features,targets = Variable(features), Variable(targets.float())\n",
    "            if use_gpu:\n",
    "                features = features.cuda()\n",
    "                targets = targets.cuda()\n",
    "            probs = net(features,doc_lens)\n",
    "            loss = criterion(probs,targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm(net.parameters(), args.max_norm)\n",
    "            optimizer.step()\n",
    "           \n",
    "            if args.debug:\n",
    "                print('Batch ID:%d Loss:%f' %(i,loss.data[0]))\n",
    "                continue\n",
    "            if i % args.report_every == 0:\n",
    "                cur_loss = eval(net,vocab,val_iter,criterion)\n",
    "                if cur_loss < min_loss:\n",
    "                    min_loss = cur_loss\n",
    "                    best_path = net.save()\n",
    "                logging.info('Epoch: %2d Min_Val_Loss: %f Cur_Val_Loss: %f'\n",
    "                        % (epoch,min_loss,cur_loss))\n",
    "            \n",
    "    t2 = time()\n",
    "    print(\"train ended\")\n",
    "    logging.info('Total Cost:%f h'%((t2-t1)/3600))\n",
    "    \n",
    "\n",
    "\n",
    "def testsigir(data):\n",
    "    print(\"test started\")\n",
    "    embed = torch.Tensor(np.load(args.embedding)['embedding'])\n",
    "    with open(args.word2id,encoding=\"utf8\") as f:\n",
    "        word2id = json.load(f)\n",
    "    vocab = Vocab(embed, word2id)\n",
    "\n",
    "    examples=data\n",
    "    print(examples[0])\n",
    "    test_dataset = Dataset(examples)\n",
    "\n",
    "    test_iter = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False)\n",
    "    \n",
    "    if use_gpu:\n",
    "        checkpoint = torch.load(args.sigirseed)#-load_dir',type=str,default='C:/Users/user/metin özütleme/hphaos summarunner/checkpoint/RNN_RNN_seed_1.pt')\n",
    "        print(\"\\nin there\\n\")\n",
    "    else:\n",
    "        checkpoint = torch.load(args.sigirseed, map_location=lambda storage, loc: storage)\n",
    "        print(\"not in there\\n\\n\")\n",
    "\n",
    "    # checkpoint['args']['device'] saves the device used as train time\n",
    "    # if at test time, we are using a CPU, we must override device to None\n",
    "    if not use_gpu:\n",
    "        checkpoint['args'].device = None\n",
    "    net = getattr(models,checkpoint['args'].model)(checkpoint['args'])\n",
    "    net.load_state_dict(checkpoint['model'])\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    doc_num = len(test_dataset)\n",
    "    time_cost = 0\n",
    "    file_id = 1\n",
    "    for batch in tqdm(test_iter):\n",
    "        features,_,summaries,doc_lens = vocab.make_features(batch)\n",
    "        t1 = time()\n",
    "        if use_gpu:\n",
    "            probs = net(Variable(features).cuda(), doc_lens)\n",
    "        else:\n",
    "            probs = net(Variable(features), doc_lens)\n",
    "        t2 = time()\n",
    "        time_cost += t2 - t1\n",
    "        start = 0\n",
    "        for doc_id,doc_len in enumerate(doc_lens):\n",
    "            stop = start + doc_len\n",
    "            prob = probs[start:stop]\n",
    "            #topk = min(args.topk,doc_len)\n",
    "            topk=min(abssizes[file_id-1],doc_len)\n",
    "            topk_indices = prob.topk(topk)[1].cpu().data.numpy()\n",
    "            topk_indices.sort()\n",
    "            doc = batch['doc'][doc_id].split('\\n')[:doc_len]\n",
    "            hyp = [doc[index] for index in topk_indices]\n",
    "            ref = summaries[doc_id]\n",
    "            with open(os.path.join('C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirref',str(file_id)+'.txt'), 'w',encoding=\"utf8\") as f:\n",
    "                f.write('C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirref')\n",
    "            with open(os.path.join('C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirhyp',str(file_id)+'.txt'), 'w',encoding=\"utf8\") as f:\n",
    "                f.write('\\n'.join('C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirhyp'))\n",
    "            start = stop\n",
    "            file_id = file_id + 1\n",
    "    print('Speed: %.2f docs / s' % (doc_num / time_cost))\n",
    "    print(\"test ended\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-maintenance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entire-contract",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Rouge155.py, line 335)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\user\\anaconda3\\envs\\sigir\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3343\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-4-d712d274f3c6>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from pyrouge import Rouge155\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\anaconda3\\envs\\sigir\\lib\\site-packages\\pyrouge\\__init__.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from pyrouge.Rouge155 import Rouge155\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\user\\anaconda3\\envs\\sigir\\lib\\site-packages\\pyrouge\\Rouge155.py\"\u001b[1;36m, line \u001b[1;32m335\u001b[0m\n\u001b[1;33m    self.log.info(\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from pyrouge import Rouge155\n",
    "def remove_broken_files():\n",
    "    error_id = []\n",
    "    for f in os.listdir('C:/Users/user/Desktop/sondönem/bitirme/outputs/ref'):\n",
    "        try:\n",
    "            open('ref/' + f).read()\n",
    "        except:\n",
    "            error_id.append(f)\n",
    "    for f in os.listdir('C:/Users/user/Desktop/sondönem/bitirme/outputs/hyp'):\n",
    "        try:\n",
    "            open('hyp/' + f).read()\n",
    "        except:\n",
    "            error_id.append(f)\n",
    "    error_set = set(error_id)\n",
    "    for f in error_set:\n",
    "        #os.remove('ref/' + f)\n",
    "        os.remove('hyp/' + f)\n",
    "\n",
    "def rouge():\n",
    "    r = Rouge155()\n",
    "    r.home_dir = '.'\n",
    "    r.system_dir = 'C:/Users/user/Desktop/sondönem/bitirme/outputs/hyp'\n",
    "    r.model_dir =  'C:/Users/user/Desktop/sondönem/bitirme/outputs/ref'\n",
    "    \n",
    "    r.system_filename_pattern = '(\\d+).txt'\n",
    "    r.model_filename_pattern = '#ID#.txt'\n",
    "\n",
    "    command = '-e C:/ROUGE-1.5.5/data -a -c 95 -m -n 2 -b 75'\n",
    "    output = r.convert_and_eva<luate(rouge_args=command)\n",
    "    print(\"we done\")\n",
    "    print(output)\n",
    "def rougesigir():\n",
    "    r = Rouge155()\n",
    "    r.home_dir = '.'\n",
    "    r.system_dir = 'C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirhyp'\n",
    "    r.model_dir =  'C:/Users/user/Desktop/sondönem/bitirme/sigir/sigirref'\n",
    "    \n",
    "    r.system_filename_pattern = '(\\d+).txt'\n",
    "    r.model_filename_pattern = '#ID#.txt'\n",
    "\n",
    "    command = '-e C:/ROUGE-1.5.5/data -a -c 95 -m -n 2 -b 75'\n",
    "    output = r.convert_and_evaluate(rouge_args=command)\n",
    "    print(\"we done\")\n",
    "    print(output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #remove_broken_files()\n",
    "    #rouge()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "processed-transaction",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rougesigir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2c9adbcd548d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrougesigir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rougesigir' is not defined"
     ]
    }
   ],
   "source": [
    "rougesigir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efficient-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-01 00:53:10,459 [INFO] Loading vocab,train and val dataset.Wait a second,please\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGIR DATA GENERATED\n",
      "{'doc': 'traditionally, medical doctors and care providers have been the main source of information for patients who suffer from chronic or life-threatening diseases\\n however, with the advent of the internet and the creation of many online health communities (ohcs), e.g., everyday health, cancer survivors’ network, and webmd, patients use these health communities increasingly as an integral source for finding health-related information [5]\\nohcs provide an environment for patients, their family members and friends to interact with other participants and share experiences and information (e.g., recommendations and feedback) on issues related to prescribed medicines, side effects, therapeutic processes, mental health, and feelings\\ntable 1 shows examples of posts that contain health-related information shared among patients in an online cancer community\\nthis information is very unique and is often not available elsewhere, e.g., referring to the medication sertraline, a patient writes: doctors also say it helps with hot flashes.\\ni don’t know about that since i still get them (see example 1 in the table)\\nseveral studies showed that using ohcs to obtain information from people who went through the same or similar experiences (either by direct interactions or sifting through the online posts) brings better feelings and fewer mortality odds to patients [8].\\nthus, the large and growing amounts of user-generated content in ohcs need to be accurately classified for a variety of applications, e.g., designing smart information retrieval systems for content recommendation\\nrecent computational studies in ohcs started to investigate the high level identification of informational posts [1, 19], however, with no emphasis on the unique challenges associated with the detection of the information type, e.g., therapeutic procedures vs. side effects\\na deep understanding of the text and the writer’s intention is required in order to correctly extract the types of information present in ohcs messages\\nexample 1 in table 1 refers to therapeutic procedure, whereas example 2 refers to side effects through various medication (sertraline and anzamet, respectively)\\nin this paper, we propose to analyze messages in ohcs to extract the information type that they contain, i.e., therapeutic procedures (any medical treatment, activity, or behavior that have a positive impact on patients’ health, precisely, can help prevent, cure or improve a patient’s condition) and side effects (any medical treatment, activity, or behavior that have a negative impact on patients’ health, precisely, a secondary, often undesirable effect of a drug or medical treatment)\\nto achieve this, we design a computational model that is able to exploit the semantic information from text, and coherently combines high-level (abstract) features with surface-level and lexicon-based features.\\nour contributions are as follows\\n(1) we propose to extract fine-grained information types from messages posted in ohcs.\\nidentifying information types provides doctors, health practitioners and ohcs’ moderators with an insightful view of patients’ physical status during various treatments\\nin addition, it can provide new diagnosed patients with information about what they should expect throughout their treatments and help them in making informed decisions about their disease more effectively [14]\\nto our knowledge, we are the first to address fine-grained information type extraction in ohcs\\n(2) we design and explore a computational model that can identify messages belonging to therapeutic procedures and side effects with high accuracy\\nour model is a hybrid neural network combined with lexicon-based features\\n(3) we show empirically that our model significantly outperforms strong baselines and prior works and continues to perform well even in the absence of lexicon-based features\\n', 'labels': '1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1', 'summaries': 'Online health communities have become a medium for patients to share their personal experiences and interact with peers on topics related to a disease, medication, side effects, and therapeutic processes\\nAnalyzing informational posts in these communities can provide an insightful view about the dominant health issues and can help patients find the information that they need easier\\nIn this paper, we propose a computational model that mines user content in online health communities to detect positive experiences and suggestions on health improvement as well as negative impacts or side effects that cause suffering throughout fighting with a disease\\nSpecifically, we combine high-level, abstract features extracted from a convolutional neural network with lexicon-based features and features extracted from a long short term memory network to capture the semantics in the data\\nWe show that our model, with and without lexicon-based features, outperforms strong baselines\\n'}\n",
      "dsadksadlas\n",
      "AttnRNN(\n",
      "  (abs_pos_embed): Embedding(100, 50)\n",
      "  (rel_pos_embed): Embedding(10, 50)\n",
      "  (embed): Embedding(153824, 100, padding_idx=0)\n",
      "  (attn): Attention()\n",
      "  (word_RNN): GRU(100, 200, batch_first=True, bidirectional=True)\n",
      "  (sent_RNN): GRU(400, 200, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (content): Linear(in_features=400, out_features=1, bias=False)\n",
      "  (salience): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
      "  (novelty): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
      "  (abs_pos): Linear(in_features=50, out_features=1, bias=False)\n",
      "  (rel_pos): Linear(in_features=50, out_features=1, bias=False)\n",
      ")\n",
      "#Params: 17.0M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-3cf664b5e0d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainsigir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-95049b44801e>\u001b[0m in \u001b[0;36mtrainsigir\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc_lens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\metin özütleme\\hphaos summarunner\\models\\AttnRNN.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, doc_lens)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mword_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m                                \u001b[1;31m# (N,L,D)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_RNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m         return F.embedding(\n\u001b[0;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1916\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "data=sigirdata()\n",
    "x=data[:-10]\n",
    "y=data[-10:]\n",
    "trainsigir(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "#print(len(sigirdata()))\n",
    "kfold = KFold(5, True, 1)\n",
    "# enumerate splits\n",
    "counter=0\n",
    "trainx=[]\n",
    "testx=[]\n",
    "data=sigirdata()\n",
    "for train, test in kfold.split(data):\n",
    "    for t in range (0,len(train)):\n",
    "        val=data[train[t]]\n",
    "        trainx.append(val)\n",
    "    for i in range (0,len(test)):\n",
    "        vall=data[test[i]]\n",
    "        testx.append(val)\n",
    "    print(len(testx),\"sended to test\")\n",
    "    print(len(trainx),\"sended to train\")\n",
    "    #print(type(trianx[]))\n",
    "    trainsigir(trainx)\n",
    "    #testsigir(testx)\n",
    "    #rougesigir()\n",
    "    #trainx=[]\n",
    "    #testx=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainx[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"doc\": \"by daily mail reporter last updated at 11:49 am on 5th october 2011 documents obtained under a federal freedom of information act lawsuit filed by judicial watch detail costs incurred during the first lady\n",
    " 's trip to africa and botswana in june - with the cost of firing up ' air force 2 ' alone amounting to $ 424,142\\nthe white house earlier professed the purpose of the trip was to help ' youth leadership , education , \n",
    " health and wellness ' in africa\\n' air force 2 ' : obama waves as she boards her private plane after a week - long trip to africa in june\\ncosts of the flight are estimated to tally more than $ 424,000 greetings :\n",
    " mrs obama was met by excited children when she arrived to botswana on june 24 warm welcome : mrs obama with daughters sasha , far right , and malia , right , were greeted by traditional dancers as they arrive in gaborone \n",
    " , botswana judicial watch said it based the jet costs on the u.s. department of defense 's published hourly rates for the c-32a aircraft - a specifically configured military version of the boeing 757\\nduring the june 21 -\n",
    " 27 trip , the group accounted for 34.8 flight hours at $ 12,188 per hour\\nthat does not include a tally of local transportation , secret service protection , food for the 21 family and staff members - and the cost of pre\n",
    " -trip preparations contributing to the final amount\\nthose figures have yet to be disclosed\\naccording to the release , the passenger manifests confirm the presence of obama 's daughter 's , malia and sasha on the trip\\nthe\n",
    " two girls are listed as ' senior staff\\n' family safari : mrs obama , joined by her daughters sasha and malia and her mother marian robinson enjoyed a ride through madikwe game reserve during the trip encouraging : mrs obama\n",
    " stands with mamphela ramphele , fifth from right , and high school students after she answered students ' questions at the university of cape town in cape town , africa on june 23 playful : mrs obama ( r ) and daughters sasha\n",
    " ( c ) and malia take turns read to students during a visit to the emthonjeni community center in zandspruit township , johannesburg , africa on june 21 goals : mrs obama said the trip would help ' youth leadership , education\n",
    " , health and wellness ' in southern africa , according to the white house the manifests also list mrs obama 's mother , marian robinson , and niece and nephew ,\n",
    " leslie and avery robinson , as well mrs. obama 's make - up and hairstylist ( carl ray and johnny wright )\\nthe expense records also show $ 928.44 was spent for ' bulk food '\n",
    " purchases on flight\\noverall , during the trip , 192 meals were served for the 21 passengers on board\\nthe ' professed purpose ' of the trip ' was to encourage young peopl\n",
    " e living in the two growing democracies to become involved in national affairs ; and during her scheduled stops in pretoria and cape town , africa and in gaborone , the capital of\n",
    " botswana , the first lady used the opportunity to speak on education , health and wellness issues , ' the report states\\nmemorable : mrs obama also took her daughters to visit nelson \n",
    " mandela , a visit which she called ' surreal ' family : from left , mrs obama 's niece leslie robinson , malia , archbishop desmond tutu , mrs obama , sasha obama and nephew avery\n",
    " robinsona at cape town stadium in africa on june 23 business : mrs obama met botswana 's president lt. gen. seretse khama ian khama during her visit mission expense records and passenger\n",
    " manifests from the air force related to the june 21 - 27 , 2011 trip taken by first lady obama , her family and her staff to africa and botswana , according to watchdog group judicial watch include\n",
    " : the malia also enjoyed a meet up with nelson mandela\\nhowever , the trip also included visits to historical landmarks , and ended with a private family safari at a africa game reserve before the\n",
    " group returned to washington on june 27\\njudicial watch , which investigates and fights government corruption , said in a press release issued today it is investigating the purpose and itinerary of\n",
    " the trip\\non june 28 , 2011 , the group filed a freedom of information act request seeking the mission taskings , transportation records , and passenger manifests for obama 's africa trip\\ndocuments\n",
    " were only provided after judicial watch sued to obtain the documents , according to the release\\n'\n",
    " this trip was as much an opportunity for the obama family to go on a safari as it was a trip to conduct government business \n",
    " , ' said judicial watch president tom fitton\\n' this junket wasted tax dollars and the resources of our overextended military\\nno wonder we had to sue to pry loose this information\", \n",
    " \"labels\": \"1\\n0\\n1\\n1\\n0\\n1\\n0\\n0\\n1\\n1\\n0\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\", \n",
    " \"summaries\": \"costs of ' air force 2 ' flight amount to over $ 424,000 , air force manifests indicate\\nin - flight meals for 21 passengers cost over $ 900\\nfirst daughters malia , 13 , and sasha obama , 10 , listed as ' senior staff ' in manifests\"}\n",
    "{\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
